{"meta":{"title":"ZL Asica的博客","subtitle":"Blog","description":"ZL Asica的小站，分享各种技术知识与个人的技术记录。个人技术记录及小技巧分享、生活小分享、小知识，喜欢的话欢迎点进来康康(*≧∪≦)","author":"ZL Asica","url":"https://www.zl-asica.com","root":"/"},"pages":[{"title":"关于","date":"2024-10-15T06:00:00.000Z","updated":"2024-12-02T16:04:42.808Z","comments":true,"path":"about.html","permalink":"https://www.zl-asica.com/about.html","excerpt":"","text":"自我介绍这个页面是关于我的一个自我介绍页面坐标Evanston, IL，Northwestern University 硕士在读。项目为MSCS，方向是医学+HCI+ML。关于更多我的学术信息请访问我的学术网站 白鲸字幕组创始人&#x2F;组长社交平台联系方式全在左边，需要的可以联系我哦交换友链请点击左侧我的朋友查看详情平常会编写一些小的程序、上学、写代码、写网站、剪视频之类的。 创建此博客的意图创建此博客的意图是为了记录一些我想到的小点子、在编程的时候遇到一些问题时的解决过程及方案以及一些视频剪辑的小技巧和资源分享(GitHub、YouTube以及B站链接均在网站首页)。 关于此博客此博客基于Hexo 搭建，部署在GitHub Pages，并且采用 Markdown 语法来写文章。想要访问我的Wordpress站点可以点击这里。 给予我鼓励各位大佬打赏一下我吧ヾ(&#x3D;･ω･&#x3D;)o，目前支持支付宝、微信、和paypal国际版哦 PayPal国际站的赞助可以点击此处跳转！ 支付宝 微信 博客开启了RSS，需要的可以关注一下。"},{"title":"文章归档","date":"2024-12-02T16:04:42.808Z","updated":"2024-12-02T16:04:42.808Z","comments":true,"path":"archive.html","permalink":"https://www.zl-asica.com/archive.html","excerpt":"","text":""},{"title":"朋友","date":"2019-11-13T06:00:00.000Z","updated":"2024-12-02T16:04:42.808Z","comments":true,"path":"friends.html","permalink":"https://www.zl-asica.com/friends.html","excerpt":"","text":"我的项目 友情链接 下面全是大佬!!!同样也都是朋友～～～ 添加友链可以直接在下面评论区，使用如下格式留言，方便我复制粘贴。只要我在你的页面中看到了我的链接且你的链接没有问题，我会第一时间添加！ 123456&#123; &quot;title&quot;: &quot;ZL Asica&quot;, &quot;link&quot;: &quot;https://www.zl-asica.com/&quot;, &quot;img&quot;: &quot;https://cdn.v2ex.com/gravatar/cba8b28739dd6225f6fe961762bdb0b71b858d68c83d946a37cee3b0e0daece5?size=512&quot;, &quot;des&quot;: &quot;不要哭，不要笑，不要恨，要理解。&quot;&#125; 需要更新头像或者链接请留言，有时间会处理的。若没有及时处理请给我发邮件催一下，邮箱是zl at zla dot moe。"},{"title":"","date":"2024-12-02T16:04:42.808Z","updated":"2024-12-02T16:04:42.808Z","comments":true,"path":"custom.css","permalink":"https://www.zl-asica.com/custom.css","excerpt":"","text":":root{--gutter:25px;--radius:13px;--color-primary:#f6a8b8;--color2:#5bcefa;--color3:#131210;--color4:#33d57a;--color5:#ff761e;--color6:#1a98ff;--color7:#9090ff;--color-primary-bg:rgba(246, 168, 184, 0.15);--color2-bg:rgba(91, 206, 250, 0.149);--color3-bg:rgba(255, 185, 0, 0.15);--color4-bg:rgba(51, 213, 122, 0.15);--color5-bg:rgba(255, 118, 30, 0.15);--color6-bg:rgba(26, 152, 255, 0.15);--color7-bg:rgba(144, 144, 255, 0.15);--color-shadow:rgba(161, 177, 204, 0.4)}"}],"posts":[{"title":"Python数据结构-List","slug":"python-data_structure-list","date":"2024-10-25T00:00:00.000Z","updated":"2024-12-02T16:04:42.808Z","comments":true,"path":"2024/python-data-structure-list/","permalink":"https://www.zl-asica.com/2024/python-data-structure-list/","excerpt":"","text":"Python","categories":[{"name":"代码","slug":"代码","permalink":"https://www.zl-asica.com/categories/%E4%BB%A3%E7%A0%81/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://www.zl-asica.com/tags/Python/"}]},{"title":"2023年度总结","slug":"2023-annual-summary","date":"2023-12-31T15:59:59.000Z","updated":"2024-12-02T16:04:42.808Z","comments":true,"path":"2023/2023-annual-summary/","permalink":"https://www.zl-asica.com/2023/2023-annual-summary/","excerpt":"本文概括了我在2023年的生活、学习和娱乐活动，以及我对2024年的期望和计划。在娱乐方面，我分享了自己观看的电影、番剧和纪录片，阅读的小说，以及在游戏和电子产品上的体验。在日常生活方面，我记录了自己的饮食习惯、住宿经历和各地的旅行体验。学术上，我分享了自己的成绩、参与的研究项目和开发的应用程序。此外，我还为新的一年制定了目标和计划，展望未来，期待在新的一年中取得更多成就。(ChatGPT 4.0生成)","text":"本文概括了我在2023年的生活、学习和娱乐活动，以及我对2024年的期望和计划。在娱乐方面，我分享了自己观看的电影、番剧和纪录片，阅读的小说，以及在游戏和电子产品上的体验。在日常生活方面，我记录了自己的饮食习惯、住宿经历和各地的旅行体验。学术上，我分享了自己的成绩、参与的研究项目和开发的应用程序。此外，我还为新的一年制定了目标和计划，展望未来，期待在新的一年中取得更多成就。(ChatGPT 4.0生成) 娱乐电影本年度一共去电影院看过5部电影，在家看过的数目应该比这更多一些。因为美国院线上**流浪地球2的时候没时间看，后来在家里自己补了一下，在五六月还补了部分宫崎骏的电影（龙猫、魔女宅急便**）。 流浪地球2 (The Wandering Earth II) - 4.5&#x2F;5.0 xxxx - 03&#x2F;2023 - 0美元 这电影我是真的喜欢，不管从剧情还是特效，我个人都很满意。唯一的问题就是你不要考虑太多科学角度的问题，比如什么太空电梯这种明显花费多余可回收火箭方案的方案为什么会被使用之类的。只要不考虑过多科学角度问题就非常好看。 铃芽之旅 (Suzume) - 4.0&#x2F;5.0 AMC - IMAX - 04&#x2F;13&#x2F;2023 - 20.29美元(约145人民币) 铃芽之旅在北美上线时间较晚，所以是4月份才看到，这已经是首映当天了。新海诚新作大家评价还是有些差别的，我个人感觉还是蛮不错的，比天气之子稍弱一些（天气之子4.5&#x2F;5.0）。看的是IMAX版日语+英文字幕，这个英文字幕稍微有点影响体验，翻译风格和中国翻译风格不大一样。尤其是关门那段英文直接直译，远远不及国内的诗词翻译。 超级玛利欧兄弟电影版 (The Super Mario Bros. Movie) - 3.5&#x2F;5.0 AMC - Dolby Vision - 04&#x2F;25&#x2F;2023 - 12.25美元(约86人民币) 因为我个人并不是玛利欧的资深老玩家，主机&#x2F;掌机游戏玩的比较少也是这两年才开始接触Switch的。玩过的相关作品只有**超级马力欧：奥德赛和路易基鬼屋3**这两部，而且也都没有通关。我个人看的还是比较开心的，并没有什么观看门槛。 变形金刚：万兽崛起 (Transformers: Rise of the Beasts) - 2.5&#x2F;5.0 AMC - Dolby Vision - 06&#x2F;13&#x2F;2023 - 13美元(约92人民币) 我的评级很简单，这啥玩意？说实话我能给这个评分也是全看在特效和情怀了，剧情方面我不是很好评价。太老套了这剧情。人物里也就Mirage给我的感觉会比较深刻，蛮可爱的角色。 奥本海默 (Oppenheimer) - 4.0&#x2F;5.0 中国大陆 - 星铁IMAX - IMAX - 09&#x2F;16&#x2F;2023 - 约6.48美元(45.9人民币) 造原子弹，怎么说呢，愿世界和平。诺兰拍的电影，整体观感肯定没得说，我只说一下剧情感受。剧情方面充满了美国的人情世故的感觉，给我一种窒息的感觉，不管自己的技术水平多么强大，在政治面前只是一个小丑，非常无力。科研过程中就是会出现自我肯定和否定的交织，而这种交织在政治家看来就是可以利用的点。而且如果真的创造出一个可能毁灭全人类也有可能帮助全人类的东西，到底应该怎么选择，是继续还是停下？对于我这样平平无奇的人来说思考这些当然过多，但是对于顶尖的科学家来说他们不就是需要去面临这种困境。 旺卡 (Wonka) - 4.0&#x2F;5.0 AMC - IMAX - 12&#x2F;26&#x2F;2023 - 12美元(约85人民币) 在年底看了这部旺卡，这绝对是全年龄都可以找到自己观看点的电影。我小时候非常喜欢看罗达尔的书，他写的基本所有书我都看过好几遍，我个人是非常喜欢的。这部电影更多是巧克力工厂的前传，讲得更多的是巧克力工厂是怎么来的，非常梦幻。注意看的时候不要考虑科不科学的问题，要不然就会很容易出戏。 番剧来到番剧部分，我本年度看过的番剧和往年一样数目过多。说实话我自己都数不清楚我看了多少（统计出来103部），我会主要挑几部影响深刻的来分享和推荐一下。而且因为B站目前的看番环境，我绝大部分看的番剧都是在其他网站看的，这里特别感谢一下各位专注于番剧的日语字幕组，我自己也有字幕组所以很清楚大家所付出的努力和辛勤劳动，感谢大家！我会按照四个季度和补番5个部分来解析，只会列举部分我能够予以评价印象比较深刻的番剧，没有提到的可能是因为我没有追完或者不是很符合我的类型我没办法作出评价。 2023年1月新番本季度我看过的番剧有16部（共62部定档，观看25.8%）：久保同学不放过我、网购技能开启异世界美食之旅、转生公主与天才千金的魔法革命、冰属性男子与酷酷女同事、因为太怕痛就全点防御力了 第2期、间谍教室、别当哥哥了、变成狗后被喜欢的人捡走了、冰剑的魔术师将要统一世界、傲娇反派千金莉洁洛特与实况主远藤同学及解说员小林同学、进化之实踏上胜利的人生 第2期、关于邻家的天使大人不知不觉把我惯成了废人这件事、尼尔自动人形 Ver1.1a、为了养老金去异界存八万金、虚构推理 第2期、JOJO的奇妙冒险 石之海 Part.3、极主夫道 第2期。 神作 别当哥哥了！ (お兄ちゃんはおしまい！) - 5.0&#x2F;5.0 这还用解释吗，这已经不是普通的番剧了，这是艺术品，请大家都留存一份BD珍藏。从第一集到最后一集，每一帧画面都透露着“我有钱”仨字，这已经溢出屏幕的经费。请多来一些这样的优质艺术品内容。题材方面涉及到性转，国内审核制度对这个方面近几年过于敏感无法过审，我是很不能接受的。哦你卖是我认为本年度最佳之一。 推荐 久保同学不放过我 - 4.0&#x2F;5.0 纯爱，类似高木同学。 变成狗后被喜欢的人捡走了 （?） - 4.0&#x2F;5.0 这玩意我没法予以评价。 虚构推理 第2期 - 4.0&#x2F;5.0 推荐看完第一季再看，纯爱，非常喜欢 JOJO的奇妙冒险 石之海 Part.3 - 3.5&#x2F;5.0 这个无需解释。 极主夫道 第2期 - 3.5&#x2F;5.0 很有趣，看过就明白了，黑道大佬洗白变成家庭主夫。 还不错 网购技能开启异世界美食之旅 - 3.5&#x2F;5.0 主要是做的饭太香了我无法抵抗，而且这个丝滑小连招也太丝滑了。 冰属性男子与酷酷女同事 - 3.5&#x2F;5.0 剧情风格我很喜欢，办公室纯爱叠加异能。 答辩 间谍教室 - 1.0&#x2F;5.0 这纯纯答辩，我强忍着看的，这都啥玩意呀。剧情完全没法看，作画还是蛮不错的。 为了养老金去异界存八万金 - 1.0&#x2F;5.0 逻辑何在？整体的逻辑就离谱，属于是把观众的智商按在地上摩擦。同样作画时蛮不错的 2023年4月新番本季度我看过的番剧有30部（共54部定档，观看55.6%）：熊熊勇闯异世界 第2期、异世界里得到了开挂般能力的我现实世界中也举世无双、绊之Allele、带着智能手机闯荡异世界 第2期、公爵的契约未婚妻、放学后失眠的你、亡骸游戏、跃动青春、THE MARGINAL SERVICE、为美好的世界献上爆焰、无神世界中的神明活动、我推的孩子、石纪元 第3期、百合是我的工作、勇者死了、国王排名 勇气的宝箱、可爱过头大危机、总之就是非常可爱 第2期、小鸟之翼 高尔夫少女 第2期、女神的露天咖啡厅、江户前的废柴精灵、天国大魔境、地狱乐、和山田进行Lv999的恋爱、我心里危险的东西、异世界一击无双姐姐、邻人似银河、第二次被异世界召唤、转生贵族的异世界冒险录、我家的英雄、进击的巨人 最终季 完结篇 前篇。 神作 跃动青春 (スキップとローファー) - 4.5&#x2F;5.0 这个标题的翻译微偏向于意译，直译的话是跳跃和乐福鞋（Skip and Loafer），英文就是这样翻译的。整体的剧情也是高中生活交织着青春恋爱，虽然和国内普高的生活有些不同，但是高一的部分时光和初中还是可以做到类似的情况的，是比较容易产生共鸣的类别和剧情了已经算。这部番剧的内容展现的非常全面，并不是展现主角最完美的一面给你，也会给你展现他们真实的所有方面。这就是我们在高中阶段所经历的真正的经历，从青涩的青春期和直来直去向成年人的人情世故转变，变得更加柔和。这部番剧中的生活和节奏正就是我们所期望的真正高中生活的写照，除了学习和考试，我们也希望有自己的课后生活，不管是和同学一起出去看电影还是吃饭唱K，这都是我们心目中最理想里的生活。学生生活带给我们的深刻记忆不仅仅是紧张的学习生活和考试，生活中的点点滴滴、和朋友的相处更是我们生活中的重要部分。 我推的孩子 (推しの子) - 4.5&#x2F;5.0 首先，OP已经血洗TikTok了，也屠了日语歌的榜，确实非常好听。其次，本片又名，爸爸去哪了。整体经费爆炸，尤其第一集非常炸裂，内容和制作都非常完美。动画工坊牛逼！虽然后面有几集比较平淡，但是整体内容并没有什么问题。在这部番中也探讨到了非常重大的网络暴力问题，这已经是非常多我认为的神作里不断提到的点了（比如青春猪头少年不会遇到兔女郎学姐），这是一个非常严重的问题。主要的内容聚集在日本的偶像圈&#x2F;娱乐圈的一些问题，揭露了很多的问题，可能会让一些人不是很满意。 天国大魔境 (天国大魔境) - 5.0&#x2F;5.0 首先是末世生存番和类似于梦幻岛的机构剧情，在开头就把吸引拉满了让人能够持续看下去。中途透露了姐姐是性转，在机构中也点出了消灭性别概念的理念，在当前的社会下我觉得是一个非常值得深入研究的问题，给大家从出生就贴标签的逻辑是什么？这种逻辑是否应该适用于所有场景？同时制作也非常优良，经费拉满。内容中包含部分猎奇内容，害怕的不要看会引起不适。 我心里危险的东西 - 4.0&#x2F;5.0 纯爱恋爱番，对比放学后失眠的你受众面更大，大太多了。开头不是非常吸引人但是慢慢看下去节奏感非常好，作画也非常好。 推荐 放学后失眠的你 - 4.0&#x2F;5.0 其实我是想给它神作的，不管从作画角度还是剧情角度我都非常喜欢。只是因为这个季度的神作有点多，一对比这个反而没那么高了。我想给神作完全是因为我个人比较喜欢这个类别，客观来看确实在推荐这个级别。主要为摄影和天文交叉的纯爱番。男女主的失眠相遇开始走到一个同好会，再到之后的一起拍照和露营，整体的风格非常好，这才是青春恋爱该有的样子，或者说理想的样子。主线的天文和摄影相对小众一些，节奏较为缓慢不适合所有人，所以可能有些人不是很喜欢这个风格。 为美好的世界献上爆焰 - 3.5&#x2F;5.0 阿克西斯教团万岁！慧慧的角度的番。就算没有看过为美好的世界献上祝福的朋友们也可以直接看，阿库娅只在最后才出现。作画方面有一些小问题所以扣了0.5。 石纪元 第3期 - 4.0&#x2F;5.0 主要是我一直都在追这个，我个人是非常喜欢这个风格的科普番的，内容和制作都非常好，不是单纯的儿童科普，也有对抗与打斗。 地狱乐 - 4.0&#x2F;5.0 我很纠结这个番要不要推荐说实话。我可以用这个标题来总结这部番的剧情方面内容，主打一个又地狱又乐。部分内容稍微有点猎奇，害怕的朋友不要看，确实会引发不适。MAPPA的作画真的是非常不错，非常好，剧情拖后腿。 和山田进行Lv999的恋爱 - 4.0&#x2F;5.0 虽然女主一开始很恋爱脑被渣男渣了，但是女主还是情商很高。比较常规的发糖番，网游只是一个媒介而已。 我家的英雄 - 3.5&#x2F;5.0 被作画和经费拖累的剧情，叛逆期女儿要被黑道大哥迫害之前，老爸把黑道大哥直接杀掉然后抹清证据逃亡的故事。剧情非常不错，作画非常拉跨和崩坏。 还不错 无神世界中的神明活动 - 3.0&#x2F;5.0 这个我必须放个图，我真崩不住，这番的经费是10块钱吗。就，从剧情和内容来说我是推荐的，但是这个作画简直就是依托答辩，内容非常的炸裂，你永远想不到下一秒它能整出来什么花活，这也是为啥我给了这么高的评分还放在还不错里（说实话我甚至想放到推荐但是这个作画确实有点emmmm，总体来说和其他的推荐没法比）。 总之就是非常可爱 第2期 - 3.5&#x2F;5.0 续作，看的时候躺在床上当蛆就可以了，注意看完第一季再看这个观感会好一些。 小鸟之翼 高尔夫少女 第2期 - 3.5&#x2F;5.0 轻百，但我觉得更多的是棋逢对手的友情。角色之间的关系处理我个人觉得还是非常不错的。可以当作一个正常经费充足，剧情不错的番来看。 江户前的废柴精灵 - 3.5&#x2F;5.0 非常慢节奏的日常番，后期内容稍尬。 单纯想谈谈 熊熊勇闯异世界 第2期 - 3.0&#x2F;5.0 续作，因为第一季对我来说就属于平平无奇的异世界类，所以这续作我也没法推荐，属于你要是喜欢女主和整体画风的话可以一看的水平。 亡骸游戏 - 2.0&#x2F;5.0 别看我给了这么低的评分，但是我还追完了就离谱，看的还蛮仔细的。但一样的问题龙傲天穿越，不知道在干啥，逻辑也很奇怪。想放到答辩里，但是感觉本季度答辩也有点多，这个还没那么答辩就放到这里了。 边缘服务 (THE MARGINAL SERVICE) - 2.5&#x2F;5.0 首先，剧情有点奇怪，制作和画风还可以，但是总体非常尬。这是为数不多所有字幕组都弃坑第一时间更新的番。说实话我一开始看的还可以，但是越看越感觉奇怪，非常无力吐槽了可以说。 百合是我的工作 - 3.0&#x2F;5.0 本身是轻百，关系方面处理的并不好。尤其是双女主的这个关系处理，感觉有点用力过猛。画风非常不错。 国王排名 勇气的宝箱 - 3.0&#x2F;5.0 另外一个角度，主要我比较喜欢波吉这个角色，还是感觉总体不错的。 女神的露天咖啡厅 - 3.0&#x2F;5.0 福利番，不要在意其他的方面了。 邻人似银河 - 3.5&#x2F;5.0 我个人非常喜欢这部番带来的感觉，纯爱番非常的慢节奏。把男女主之间的关系逻辑及两个家庭之间的关系梳理得非常好。慢慢看是非常好的番，很多发糖和磕点。 进击的巨人 最终季 完结篇 前篇 - 3.5&#x2F;5.0 我个人不是很好评价这个名字贼长贼难记的最终季。巨人十年，代表的是我们这代追番人的十年，剧情比较奇怪，所有人都在忏悔感觉，更多的是给我们自己的一个纪念，代表我们的十年。 答辩 异世界里得到了开挂般能力的我现实世界中也举世无双 - 2.0&#x2F;5.0 经费拉满，MAPPA作画顶级。但是这个剧情真的我无力吐槽，死肥宅变帅哥，女神投怀送抱。2023年来还有这样的厕纸。如果喜欢龙傲天的可以看。 绊之Allele - 0.5&#x2F;5.0 ？？？这就是我的评价。你在开玩笑吗爱酱都休眠了，还在这消费呢？ 带着智能手机闯荡异世界 第2期 - 2.0&#x2F;5.0 从第一季开始就是厕纸，这第二季当然还是。说实话我都不能理解这是怎么出的第二季。 勇者死了 - 2.0&#x2F;5.0 ?上来我就没绷住，就离谱这啥剧情，根本看不下去。 第二次被异世界召唤 - 2.0&#x2F;5.0 第二次？so what？习惯了？图啥？异世界厕纸。 转生贵族的异世界冒险录 - 2.5&#x2F;5.0 异世界厕纸，龙傲天，制作还可以，愿意看的可以看。 2023年7月新番本季度我看过的番剧有24部（共53部定档，观看44.4%）：甜点转生、满怀美梦的少年是现实主义者、喜欢的人忘记戴眼镜了、转生成自动贩卖机的我今天也在迷宫徘徊、我的幸福婚姻、不死少女 杀人笑剧、咒术回战 第2季、BanG Dream! It’s MyGO!!!!!、圣者无双、间谍教室 第2季、租借女友 第3季、AI电子基因、能干的猫今天也忧郁、莱莎的炼金工房、AYAKA、公司里的小小前辈、其实我乃最强？、虽然等级只有1级但固有技能是最强的、庙不可言、黑暗集会、僵尸100：在成为僵尸前要做的100件事、无职转生 第2季、死神少爷与黑女仆 第2季、英雄教室。 神作 BanG Dream! It’s MyGO!!!!! - 4.0&#x2F;5.0 邦邦人，邦邦魂！单纯因为这个季度实在有点拉，能把邦邦推上来，本来是我们小众人的快乐。 推荐 我的幸福婚姻 - 4.0&#x2F;5.0 我个人很喜欢，比较霸总宠妻的感觉，纯爱，蛮女性向的一个番。男主情商很高，女主因为被原生家庭PUA的心理已经有问题了不能像正常人思考。 不死少女 杀人笑剧 - 3.5&#x2F;5.0 很无厘头的开局，但是之后的剧情还可以，除了部分地方比较奇怪，比如吸血鬼爸爸知道自己的儿子是杀人犯直接把儿子给活活打死。剧情节奏把控不错，不快不慢。作画优良。 租借女友 第3季 - 3.5&#x2F;5.0 一直追的可以继续看，主打一个舔狗男主。 能干的猫今天也忧郁 - 3.5&#x2F;4.0 一开始的运镜太奇怪了，和无人机拍摄一样的感觉，非常奇怪。主题很好家里又一支不用花钱还给当保姆的猫。很日常的日常番。画风和K类似。 僵尸100：在成为僵尸前要做的100件事 - 4.0&#x2F;5.0 开头有丑化女性的问题。稍微有点爽番感觉，适合社畜看。高分是因为作画非常强。 死神少爷与黑女仆 第2季 - 4.0&#x2F;5.0 直接开始在床上像蛆一样扭就好了，发糖，甜。 还不错 转生成自动贩卖机的我今天也在迷宫徘徊 - 3.0&#x2F;5.0 日本人的XP终于向奇怪的方向发展了，剧情方面还是蛮不错的，很新颖蛮有意思。 莱莎的炼金工房 - 3.0&#x2F;5.0 游戏宣传类的番，剧情比较一般，主要是福利吧，更多是看腿。 公司里的小小前辈 - 3.0&#x2F;5.0 P9的画风不错，剧情比较套路。 黑暗集会 - 3.0&#x2F;5.0 恐怖动漫领域，稍微有点小恐怖。 无职转生 第2季 - 3.0&#x2F;5.0 抖M男主，节奏有点慢这一季。 单纯想谈谈 甜点转生 - 2.5&#x2F;5.0 不是很好评价这个东西，本来想看甜点师和制作的，好家伙这基本上没有呀。又是龙傲天异世界转生。 满怀美梦的少年是现实主义者 - 2.5&#x2F;5.0 一开始男主是舔狗，女主不理人，突然某天不舔了，女主反而来了。emmmm就酱。 圣者无双 - 2.5&#x2F;5.0 还不错的异世界厕纸，男主人设艰苦奋斗。 AI电子基因 - 3.0&#x2F;5.0 没法推荐的原因是这个剧情比较奇怪，想要谈论AI&#x2F;机器人与人类之间的关系，但是用力很奇怪，剧情也很奇怪。关注点是好的。 AYAKA - 3.0&#x2F;5.0 剧情还可以，就是逻辑稍微奇怪点，画风不错，真是AYAKA呀。 庙不可言 - 3.0&#x2F;5.0 福利番。画风还可以。 答辩 间谍教室 第2期 - 2.5&#x2F;5.0 没想到能出第二季，虽然比第一季好，但是第一季实在是不行，影响到我对第二季的评价了。 其实我乃最强？-2.5&#x2F;5.0 异世界厕纸，男主数值太高测不出来被国王王后抛弃，被边境贵族收养，之后各种英藏实力和“无意”暴露实力。 英雄教室 - 2.5&#x2F;5.0 这玩意，有点脑溢血，男主情商过低，不停在收后宫感觉。 2023年10月新番本季度我看过的番剧有26部（共76部定档，观看35.5%）：米奇与达利、鸭乃桥论的禁忌推理、我推是反派大小姐、亡骸游戏Part.2、星灵感应、圣女魔力无所不能 第2季、BULLBUSTER、捡到被退婚大小姐的我教会她做坏坏的事、16bit的感动、想当冒险者前往都市的女儿升到了S级、大小姐和看门犬、石纪元 第3季 Part.2、葬送的芙莉莲、经验丰富的你和经验为零的我交往的故事、不死不幸、女友成堆 第2季、家里蹲吸血姬的苦闷、间谍过家家 第2季、猪肝倒是热热再吃啊、我们的雨色协议、药屋少女的呢喃、香格里拉·开拓异境～粪作猎手挑战神作～、OVERTAKE、MF Ghost极速车魂、暴食的巴萨卡、超超超超超喜欢你的100个女朋友、进击的巨人 最终季 完结篇 后篇。 神作 葬送的芙莉莲 (葬送のフリーレン) - 5.0&#x2F;5.0 真的需要解释吗，大家看番是为了什么，不就是为了它吗。除了op有点不大匹配，内容完全挑不出来瑕疵。剧情顶级、制作顶级、配乐顶级。主线芙莉莲在漫长的岁月中看到了人类生命虽短暂但是有自己的意义。人类奋斗一生即使在当时全世界都知道但是随着时间的推移还是会被人类忘却的事实。寿命和生存的意义带给了我们深刻的思考。芙莉莲最喜欢的花的魔法，也同时是老师最喜欢的魔法，也是辛美尔最喜欢的魔法。 间谍过家家 第2季 (SPY×FAMILY Season 2) - 4.5&#x2F;5.0 哇库哇库，可爱是无敌的。 推荐 米奇与达利 - 3.5&#x2F;5.0 双男主和领养的父母的故事。揭露了孤儿的一些问题，原生家庭的缺失给孩子从小造成无法磨灭的心理问题非常严重。我个人看的稍微有点窒息感和无比的心疼，所以评分给不到太高。 鸭乃桥论的禁忌推理 - 3.5&#x2F;5.0 侦探推理番，从这个角度来说非常好看。但是有一个问题就是异能比较难理解逻辑上的联系和定位。 我推是反派大小姐 - 3.5&#x2F;5.0 转生乙女游戏，题材挺常见，轻百，女主抖M，攻略抖S。剧情非常好的地方就是打直球，没那么多乱七八糟的套路，看着很舒服。 16bit的感动 - 3.5&#x2F;5.0 实际上有点纠结是推荐还是还不错，女主有些时候不理智，而且外星人的加入有点奇怪。说实话就是在融合各种元素到一起这部番。 石纪元 第3季 Part.2 - 4.0&#x2F;5.0 依旧推荐，理由和Part 1一样。 不死不幸 - 4.0&#x2F;5.0 打斗场面非常好，剧情也非常不错，不拖沓不龙傲天。有点你的英雄学院的感觉。 药屋少女的呢喃 - 3.5&#x2F;5.0 日本人眼中的中国后宫。细节和中文方面有一种给意大利人吃菠萝披萨的感觉，就，这是中国后宫吗，你家中文是这么发音的。抛开这部分不谈，剧情安排比较有趣（当然也有漏洞，感觉女主来后宫断案来了），没有什么后宫的勾心斗角。网飞做的，画风制作都很棒。 香格里拉·开拓异境～粪作猎手挑战神作～ - 3.5&#x2F;5.0 MMORPG类型的番，内容剧情制作都很不错。题材没什么新意但是无过。 超超超超超喜欢你的100个女朋友 - 3.5&#x2F;5.0 虽然是后宫，但是这个设定过于逆天，这玩意100个后宫还不党争的可还行。 还不错 星灵感应 - 3.0&#x2F;5.0 芳文社轻百日长题材，因为开头有点小尬我没大看下去，之后有机会补一下应该会好不少。芳文社不会让大家在萌豚失望的。 捡到被退婚大小姐的我教会她做坏坏的事 - 3.0&#x2F;5.0 稍微有点无厘头。画风很不错，设定很有意思。男主属于霸道总裁，非常不错。有发糖。 经验丰富的你和经验为零的我交往的故事 - 3.0&#x2F;5.0 女主的行为稍微有点不是很好评价说实话，也没乱搞，在交往期间也很专一，但还是交往很多人而且没有什么度可言。 单纯想谈谈 亡骸游戏Part.2 - 2.0&#x2F;5.0 续作，但是这次我看不大下去了，所以这个甚至不如上半段。 圣女魔力无所不能 第2季 - 2.5&#x2F;5.0 续作，第一季的时候女主被召唤过来因为被认为不是圣女就被晾在一边，幸亏有好心人接受让女主有了待的地方。第二季节奏更慢，有点emmm。 进击的巨人 最终季 完结篇 后篇 - 1.0&#x2F;5.0 为啥不是答辩呢，因为再怎么样也是给10年的情怀画上了句号，虽然看完感觉被喂了一口答辩但是还得吃下去的感觉。 答辩 想当冒险者前往都市的女儿升到了S级 - 2.0&#x2F;5.0 厕纸，随便看看可以。没什么实际性内容。 大小姐和看门犬 - 1.0&#x2F;5.0 ？啥玩意这是？ 猪肝倒是热热再吃啊 - 2.0&#x2F;5.0 母猪的产后护理。 我们的雨色协议 - 0.0&#x2F;5.0 我看番这么多年，第一次对一部番没有一滴好感。 暴食的巴萨卡 - 2.0&#x2F;5.0 开头还有点意思，越往后越没意思。 其他补番这里只谈一下我能想得起来的7部，莉可丽丝、孤独摇滚！、灵能百分百I、灵能百分百II、灵能百分百III、摇曳露营 第一季、摇曳露营 第二季。虽然但是，这七部都是神作。 莉可丽丝 (リコリス・リコイル) - 4.5&#x2F;5.0 07&#x2F;2023 (2022) 双女主轻百。召集孤儿训练成杀手，暗杀犯罪的人，让整个社会表面上犯罪率为0。有人想要恢复正常的社会而把Lycoris捅出来，让大家攻击他们。后期的槽点有点大，尤其是女主心脏不行了给它换心脏那段，女主死活不用实弹不管是自己要死还是其他人要死都不用实弹，我不理解。 孤独摇滚！ (ぼっち・ざ・ろっく!) - 4.5&#x2F;5.0 05&#x2F;2023 (2022) 社恐小孤独，非常戳中社恐人的内心。 灵能百分百 (モブサイコ100) 第I&#x2F;II&#x2F;III季- 4.5&#x2F;5.0 06&#x2F;2023 (2016&#x2F;2019&#x2F;2022) 灵幻师傅太善良了，专职中二病20年，一个凡人能在能力者面前侃侃而谈毫不畏惧。路人作为地表最强在灵幻师傅的帮助下保住了自己的青春。 摇曳露营 ( ゆるキャン) 第1&#x2F;2季 - 4.5&#x2F;5.0 08&#x2F;2023 (2018&#x2F;2021) 芳文社yyds。 纪录片及剧纪录片和剧我看的不多，很少，今年主要看过俩，一个是孤独的美食家，一个是中国救护。中国救护更多的是拓展我对于国内医疗现状及常见紧急疾病的认知。 小说有一说一今年实际上就看过一本小说，会说话的肘子的《第一序列》，同时它的番我也看完了。第一序列这本小说主要讲的是末世背景下实验体和主角的大逃亡，虽然是很常见的题材但是意外的好看。大概在后段30%的地方开始就稍显无趣了，属于主要地图刷完了没得刷开辟一块新地图跑到欧洲搞巫师去了。但是肘子说的这段话还是比较不错的。 废土之上，人类文明得以苟延残喘。一座座壁垒拔地而起，秩序却不断崩坏。有人说，当灾难降临时，精神意志才是人类面对危险的第一序列武器。有人说，不要让时代的悲哀，成为你的悲哀。有人说，我要让我的悲哀，成为这个时代的悲哀。这次是一个新的故事。浩劫余生，终见光明。 这本书我综合评分能够给到4.0&#x2F;5.0，因为结尾处理不好而扣1分。 游戏今年主要玩的游戏也不多，今年确实稍微忙一些娱乐的时间也减少了，我这里就把我今年玩过的所有游戏都简单说一下吧。 群星 (Stellaris) - 4.0&#x2F;5.0 一个让自己成为第四天灾、宇宙甲级战犯的游戏。我开始玩的版本是3.6，现在最新版本应该是3.10。群星属于RTS类游戏，和我从小玩的红警2属于一个类别。发展经济、科研、政策，然后点科技树、造战舰、占地盘，到宣称、开战、附庸、灭国。属于长线的游戏，也非常的吞噬时间，玩这个游戏要小心时间会消失（P社游戏特点）。 守望先锋 2 (Overwatch 2) - 4.0&#x2F;5.0 暴雪这个样子我们玩家也没办法，守望还是依旧好玩的，5人对战给到了T和主奶更大的压力，也要求C做更多的击杀而非单纯输出。现在的版本只要T或者主奶掉了就得赶紧往回拉，要不然就要团灭。 城市：天际线2 (Cities Skylines II) - 4.5&#x2F;5.0 这游戏，属于把城市天际线1和simcity5结合起来了一部分，再改进了一部分。有了更大更灵活的地图，更简易的平地和修路，建筑物扩展等等。也全面改进了经济系统。虽然目前来说优化是答辩我3070Ti玩4K中画质都稍微有点卡，但画质和整体游玩效果是确实没得说。希望后期能够改进建筑物扩展以后建筑物的部分不能移动或拆除的这种问题，包括80m的桥墩太窄了现在高架都没法拉。 节奏大师 - 4.0&#x2F;5.0 这个东西，属于情怀加分了，11月7日回归。我目前侥幸玩到了宗师段位，闯关打到了400出头。属于音游里比较入门难度的游戏（10级歌除外）。 电子产品今年在电子产品方面的消费确实非常少（往年除了更新换代也确实不多），这里我就提四样我今年买的电子产品来简单聊聊。 盯盯拍Z50行车记录仪 (DDPAI Z50 Dashcam) - 3.0&#x2F;5.0 02&#x2F;14&#x2F;2023 - 145.44美元(约1030人民币) 为啥突然买个行车记录仪呢？原因很简单，我在美国农历大年初一开车出门的路上被一个皮卡恶意追尾，当时他说的比唱的都好听一口一个抱歉、一口一个对不起。我回去报了保险以后他跟保险公司说是我故意倒车撞到他？我在那等红绿灯停的好好的，看他停的离我太近往前挪，他也跟着我往前就直接撞上来了。所以买了这个行车记录仪。 说实话安装难度稍微有点，要开机用辅助线校准贴到辅助贴上，走线也要自己弄，最后我用的是点烟器取电稳定性目前来看还可以。后置摄像头画质有一些拉，前置在白天还可以，晚上噪点有一些多。 APP非常脑溢血，海外版是单独的APP和国内不一样，连接上相机以后会在手机相册里创建居多DDPAI相簿，就很脑溢血。综合下来考虑价格给了3.0。 AirTag - 4.0&#x2F;5.0 05&#x2F;11&#x2F;2023 - 4个91.96美元(约650人民币) - 单价23美元(约163人民币) 其实这个东西不用我多做什么介绍大家都清楚，这次买主要是因为要出去旅游和去香港带两个月，为了保证行李的安全性。在行李箱和包里都放了一个，准确度还是可以的，目前用了半年没有低电量警告，考虑到价格也就给了一个4分。 NOCO Genius 1汽车蓄电池维持器 - 4.5&#x2F;5.0 06&#x2F;06&#x2F;2023 - 27.4美元(约194人民币) 这个东西可能大家不是很清楚，因为我需要把车停在美国三个月一动不动，长时间不动车蓄电池会跑空电。这个东西可以直接接到车的蓄电池上，检测蓄电池的剩余电量，在电量过低的时候充电，满电的时候就停下来。保护电池让电池不会没电，也不会过冲。剩下了叫Jumpstart的功夫（虽然保险公司保险但是很麻烦）。 小米米家血压计BPX1 - 4.0&#x2F;5.0 07&#x2F;05&#x2F;2023 - 约30.76美元(217.73人民币) 这个东西我买的原因很简单，就是它的这个创新设计的绑带。传统的绑带需要把手从中间穿过去套到上臂，它这种设计可以直接夹在上臂然后粘上，省了一个步骤。准确度也是非常不错的，属于居家必备的小家电。唯一的问题就是虽然能够连接米家APP，但是不能自动同步到Apple Health，如果能同步的话将是绝杀。如果家里有米家网关给老人用很好，每次老人测完血压会自动通过网关上传并且给你发推送。 应用今年我想要提到的应用只有一个，虽然它接近年底才出来，但我觉得还是非常有意思和方便的一个应用，那就是iOS内置的《手记》。这个APP和其他日记不一样的点就是安全和自动化，它会在你出去旅游一天回到酒店里给你推送你今天旅游的景点和也许你想要记录的内容的照片，结合地点为你带来一些思路。它也会在你没出去玩没点子写日记的时候给你提供一些好的点子方便你写日记。 二次元周边今年没买什么新的手办。但是去日本的时候买了蛮多的小摆件、立牌和啪唧，都放在国内家里了。 生活衣说实话今年主要是体重掉了接近20公斤，之前的衣服基本上都穿不上了，趁着去日本和回国的时候买了一些优衣库和GU家的衣服（便宜）。 食美国是美食荒漠的事实大家都知道，虽然比英国肯定是好不少，但问题在于有吃的我吃不起呀…所以问题在我，不是他们贵，是我穷。下面就是我本年在美国、日本、香港、深圳等地方吃到的我觉得还能拿上台面给大家分享一下的美食，包含粤菜、美式快餐、日料、意大利菜、甜品、和CUHK的食堂。当然最后也有我自己今年部分自己做的菜，在美国大多数时候自己做正经菜还是煎牛排或者烤东西这种比较快和简单的菜。 首先是甜品和快餐们，把他们放到一起单纯为了节省空间。左边一列是甜品，右侧两列最上面两个是香港的麦当劳早餐、美国KFC的限定炸鸡汉堡、美国Dominos的披萨（脆底）、狗蛋的汉堡店、In-N-Out（我的食堂）我最常吃的Protein Double Double、拉斯维加斯吃到的龙虾卷、拉斯维加斯喝到的16种全球可乐。 然后是港中文CUHK的食堂，我主要吃的就是和声书院楼下的（因为不出门，太麻烦了）。一顿饭价格在40-60港币不等，在学校两个月已经把所有能点的都点过一遍了。 接下来是我在美国、香港、深圳吃到的粤菜们。最上面是早茶（最爱糯米鸡、虾饺、豉汁排骨）、接下来是烧腊们（最爱烧鹅）、然后是云吞面、牛肚面、肉骨茶。 然后是我们的意大利菜，披萨、肉丸、意面、risotto（意大利烩饭）、和一个奇怪的可可加咖啡腌制过的煎牛排 接下来是日料们，上面两排是寿司、接下来是寿喜烧、烤肉、鳗鱼饭、日式早餐、拉面、沾面。 最后是泰餐，最多的还是泰式咖喱了。 最后就是我自己做的一部分饭了。今年做了两次肘子、一次龙虾、无数次牛排、鸡翅、鱼等。 住今年一共离境美国大概3个半月，除了那段时间其他时间和去拉斯维加斯玩的几天，其他时候我都在自己租的房子里面待着。房租一个月是1200美金，2b2b condo中的1b1b，已经住了一年多了。暑假的时候恰好有隔壁实验室的好姐妹需要就转租给她了两个半月多。 6月初放假后在去香港的路上顺路去了东京玩。住了银座名铁穆瑟酒店4晚，一个人约150美元（1078人民币）。说实话这家是稍微有点小，大概能打开3个28寸的行李箱房间就没有落脚的地方了，但这么小的房间居然一应俱全，佩服。最后一晚是用AMEX Hilton Aspire送的Free Night换的Conrad Tokyo，一分钱没花，钻卡运气不错去的早给升到了海景套，当晚现金价格是大概870美元（约6150人民币）。这家虽然设施比较老，但总体住的还是蛮舒服的，外加上钻卡的行政酒廊也还可以。下面是东京康莱德海景套房的内饰和行政酒廊的情况。 来到香港以后住了两个月港中文的和声书院宿舍，8000港币两个月，双人间单独的床、桌子、柜子，有空调，不计电费。每层共用浴室和卫生间，没有LGBTQ宿舍，只有binary sex。 去拉斯维加斯期间住的是Hilton旗下挂Double Tree牌子的Tropicana，主要为了用掉Aspire送的250刀resort credit。定的普通房三晚一个人141.3，钻卡在美国不好使，就给升到了高级房，好在给了个最边上的房间还大一些有一些风景。早餐不用想一个人就给13刀credit没有免费早餐。总体体验还可以，给小费home keeper也没拿。 行今年我自己的车一共开了6547 miles（10,520 km），消耗燃油232 gallon（879 L），平均油耗27.66 MPG（8.54 L&#x2F;100KM）。一共油费大概是1300美元（约9200人民币），约每加仑5.6美元（10.5人民币&#x2F;升）。 在日本玩的时候都靠地铁和电车，没打过车（要命地贵），大概一共花了35美元（248人民币）6天的地铁和电车。在香港除了从机场到宿舍和宿舍到机场行李太多只能打车（单程340港币-红色出租车），其他时候都靠地铁（也贼贵），坐地铁可能花了四百多港币吧（八达通充值比较复杂不好记账）。在深圳玩的时候基本上靠打车和地铁，国内打车太便宜了。 今年一共飞了6段，全都是现金票买的（积分换不是很划算）。洛杉矶到东京羽田坐的delta，北美航司没什么期待，LAX也没有PPS的休息室。东京羽田到香港坐的香港快运，廉航能到地方就行，一直在购物，羽田我去的时候没有PPS的休息室。香港回到青岛坐东航买的留学生票，上海虹桥转机，行李过多。香港机场chase的休息室吃的还不错，有炖牛肉和云吞面，还有一些常规冷盘。这个迪士尼联名飞机还不算难看，挺好。到上海因为香港雷暴雨晚点，给免费改签了最近的一班飞机，给了第一排的位置，一个人占仨座还是蛮宽敞的。上海的休息室吃的就很常规了，一些中式炒菜和面条。东航从上海到青岛这么短还给了个搭配蛮不错的简餐，挺好。自从胶东机场建好我就从这飞到美国，两年了终于第一次在这里落。 从青岛回洛杉矶的时候买的新航学生票。青岛国际出发的休息室餐食蛮不错，早餐有面、炒菜、面包、各种饮料和零食。前段青岛到新加坡是酷航，给免费饮料和餐食（没提前定就只有vegan），新加坡这个饮料分级蛮好玩，常温的可乐不好喝（就算可口也不好使）。在新加坡停留一晚第二天飞洛杉矶，樟宜的休息室也非常不错，我们亚洲机场的休息室是真的蛮好吃的种类还多。飞洛杉矶中途在成田经停一个小时，提前网上订好了免税商品赶紧去付钱拿着东西走人，休息室顺道看了一眼感觉蛮小但是吃的挺全。新航的餐食还是蛮好吃的，餐后甜点是梦龙。 信用卡今年毕竟进了524本来想安分守己一些，但实在是忍不住，一共开了4张卡，如果想要refer link的可以在下面评论区问我要或者直接给我发邮件，我就不直接放在这里了。2月开了AMEX Hilton Aspire秒批CL 8K、4月CSP说可以破524直接来了一张秒批CL10K、5月看到Bilt有开卡5X正好有大额消费就开了也是秒批CL 2K、最后一张是10月份AMEX Hilton系列改版第一天开的无年费（pending打电话秒批CL 240K）。 其他从20年开始3年了，我终于阳了一次，被人传染的，上课的时候后面有人一直咳嗽。正好有一整个周末加上周五周一。前两天高烧39度以上外加心率一直在120+，退烧药都退不下来，人直接废了，第三天开始嗓子微痛开始退烧，四五天彻底转阴。 生产收入今年的收入组成仍然主要为学校兼职收入大约10K多，部分杂工，额外的就是信用卡薅的羊毛（积分不折现），以及前不久capital one的refer薅了500刀。 大概的信用卡点数收入，共约3,400美元（23,800人民币）：UR点数收入-100K（约1,600美元），MR点数收入-60K（约720美元），Hilton点数收入-270K（约1,080美元） 债务今年还了一大部分车贷，目前还剩约4000多美元，目标还是本科毕业前还完全部车贷，目前看来是可以完成的。 学业今年的所有学校课程成绩全都是A和A+，总体来说非常不错，稳住了GPA的稳定增长。今年同时面临研究生申请，目前已经提交了UCLA、USC、UCSD、Purdue、NWU五所学校的申请，接下来还有一堆学校等着提交。 科研今年参与了三个科研项目，两个本校和一个港中文暑研。未来的发展和研究方向也基本确定了，目前来看我会专注于ML尤其是DL在Health Care和Public Health方面的算法开发，着重于可以运用到消费领域和临床的相关算法和模型。 项目今年做的项目不算少，有前端也有后端，甚至还做了个iOS的app。同时也重构了我的zl-saica.com和zla.pub的博客后端及服务器续费。zla.icu也是今年更新的新域名，老的bmmw.net作为短网址的功能也全都迁移到了icu站。app站也用于了项目展示，下一步会做为我的个人在线简历使用。 2024愿景最大的愿望当然就是被最好的MS项目录取，开始我的MS生活。肯定会换一个新的城市生活，希望新的城市和新家能够符合我的期待吧。如果能在毕业后的暑假有一个比较好的衔接research项目就会更好了。另外的愿望，我最亲密的朋友们当然很清楚了，开启全新的我希望的生活！外加身体健康。 OKRO1: 健康管理 KR1: 早睡早起23:30-7:30。 KR2: 维持现有饮食结构，高蛋白质低脂肪。 KR3: 增加运动，至少每周1-2次（最好2次吧…），每次15min以上。 KR2: 体重维持在115-125斤范围内，体脂率维持在15%以内。 O2: 内容创作 KR1: 博客每月至少更新1篇新文章，全年至少20篇文章。 KR2: 手记每周至少2-3篇，旅游至少1天一篇。 O3: 学术目标 KR1: 至少发1篇文章。 KR2: 完成Honor Program毕业要求。 KR3: MS第一个学期维持GPA在3.7以上。 KR4: 在MS第一个学期找到做research的组。 O4: 财务管理 KR1: 继续维持记账的习惯。 KR2: 阅读3本关于财务规划和投资的书籍。 KR3: 每月花销按照预算，在6月还完车贷 KR4: 还完车贷后每月存200美元，其中100到标普500定投、100到saving account。 O5: 个人成长 KR1: 继续完成至少10个Coursera或者类似的在线课程。 KR2: 学习日语，完成50音图和duolingo的课程。 KR3: 阅读至少3本和专业领域不相关的专业书籍。 🥳2024新年快乐！","categories":[{"name":"随想","slug":"随想","permalink":"https://www.zl-asica.com/categories/%E9%9A%8F%E6%83%B3/"}],"tags":[{"name":"Summary","slug":"Summary","permalink":"https://www.zl-asica.com/tags/Summary/"}]},{"title":"Deep Learning深度学习-学习笔记","slug":"deep-learning-notes","date":"2023-11-17T18:18:00.000Z","updated":"2024-12-02T16:04:42.808Z","comments":true,"path":"2023/deep-learning-notes/","permalink":"https://www.zl-asica.com/2023/deep-learning-notes/","excerpt":"This notes’ content are all based on https://www.coursera.org/specializations/deep-learning","text":"This notes’ content are all based on https://www.coursera.org/specializations/deep-learning Latex may have some issues when displaying. 1. Neural Networks and Deep Learning1.1 Introduction to Deep Learning1.1.1 Supervised Learning with Deep Learning Structured Data: Charts. Unstructured Data: Audio, Image, Text. 1.1.2 Scale drives deep learning progress The larger the amount of data, the better the performance of the larger neural network compare to smaller one or supervised learning. Sigmoid change to ReLU will make gradient descent much more faster. Since the gradient will not go to 0 really fast. 1.2 Basics of Neural Network Programming1.2.1 Binary Classification Input: X∈RnxX \\in R^{nx} Output: 0, 1 1.2.2 Logistic Regression Given xx, want y^=P(y=1∣x)\\hat{y} = P(y=1|x) Input: x∈Rnxx \\in R^{n_x} Parameters: w∈Rnx,b∈Rw \\in R^{n_x}, b \\in R Output y^=σ(wTx+b)\\hat{y} = \\sigma(w^Tx + b) σ(z)=11+e−z\\sigma(z)=\\dfrac{1}{1+e^{-z}} If zz large, σ(z)≈11+0≈1\\sigma(z)\\approx\\dfrac{1}{1+0}\\approx1 If zz large negative number, σ(z)≈11+Bignum≈0\\sigma(z)\\approx\\dfrac{1}{1+Bignum}\\approx0 Loss (error) function: y^=σ(wTx+b)\\hat{y} = \\sigma(w^Tx + b), where σ(z)=11+e−z\\sigma(z)=\\dfrac{1}{1+e^{-z}} z(i)=wTx(i)+bz^{(i)}=w^Tx^{(i)}+b Want y(i)≈y^(i)y^{(i)} \\approx \\hat{y}^{(i)} L(y,y^)=−[ylog⁡(y^)+(1−y)log⁡(1−y^)]L(y, \\hat{y}) = -[y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})] If y=1:L(y^,y)=−log⁡y^y=1: L(\\hat{y}, y)=-\\log{\\hat{y}} &lt;- want log⁡y^\\log{\\hat{y}} as large as possible, want y^\\hat{y} large If y=0:L(y^,y)=−log⁡(1−y^)y=0: L(\\hat{y}, y)=-\\log{(1-\\hat{y})} &lt;- want log⁡(1−y^)\\log{(1-\\hat{y})} as large as possible, want y^\\hat{y} small Cost function J(w,b)=1m∑∗i=1mL(y^(i),y(i))=−1m∑∗i=1mL[y(i)log⁡(y^(i))+(1−y(i))log⁡(1−y^(i))]J(w, b)=\\dfrac{1}{m}\\sum\\limits*{i=1}^{m}L(\\hat{y}^{(i)},y^{(i)})=-\\dfrac{1}{m}\\sum\\limits*{i=1}^{m}L[y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)})] 1.2.3 Gradient Descent Repeat w:=w−αdJ(w)dww:=w-\\alpha\\dfrac{dJ(w)}{dw}; b:=b−α∂J(w,b)∂bb:=b-\\alpha\\dfrac{\\partial J(w,b)}{\\partial b} α\\alpha: Learning rate Right side of minimum, dJ(w)dw&gt;0\\dfrac{dJ(w)}{dw} &gt; 0; Left side of minimum, dJ(w)dw&lt;0\\dfrac{dJ(w)}{dw} &lt; 0 Logistic Regression Gradient Descent x1,x2,w1,w2,bx_1,x_2,w_1,w_2,b z=w1x1+w2x2+bz=w_1x_1+w_2x_2+b -->a=σ(z)a=\\sigma(z) -->L=(a,y)L=(a,y) da=dL(a,y)da=−ya+1−y1−ada=\\dfrac{dL(a,y)}{da}=-\\dfrac{y}{a}+\\dfrac{1-y}{1-a} dL(y,a)da=dda(−ylog⁡(a)−(1−y)log⁡(1−a))\\dfrac{dL(y,a)}{da} = \\dfrac{d}{da}(-y\\log(a) - (1-y)\\log(1-a)) dda(−ylog⁡(a))=−ya\\dfrac{d}{da} (-y\\log(a)) = -\\dfrac{y}{a} dda(−(1−y)log⁡(1−a))=−1−y1−a×(−1)=1−y1−a\\dfrac{d}{da} (-(1-y)\\log(1-a)) = -\\dfrac{1-y}{1-a} \\times (-1) = \\dfrac{1-y}{1-a} =−ya+1−y1−a=−ya−y−11−a=-\\dfrac{y}{a} + \\dfrac{1-y}{1-a} = -\\dfrac{y}{a} - \\dfrac{y-1}{1-a} dz=dLdz=dL(a,y)dz=a−ydz=\\dfrac{dL}{dz}=\\dfrac{dL(a,y)}{dz}=a-y =dLda⋅dadz=\\dfrac{dL}{da}\\cdot\\dfrac{da}{dz} (dadz=a(1−a)\\dfrac{da}{dz}=a(1-a)) dLdw1=&quot;dw1&quot;=x1⋅dz\\dfrac{dL}{dw_1}=&quot;dw_1&quot;=x_1\\cdot dz dLdw2=&quot;dw2&quot;=x2⋅dz\\dfrac{dL}{dw_2}=&quot;dw_2&quot;=x_2\\cdot dz db=dzdb=dz Gradient Descent on mm examples J(w,b)=1m∑_i=1mL(a(i),y(i))J(w, b)=\\dfrac{1}{m}\\sum\\limits\\_{i=1}^{m}L(a^{(i)},y^{(i)}) ∂∂w∗1J(w,b)=1m∑∗i=1m∂∂w1L(a(i),y(i))\\dfrac{\\partial}{\\partial w*1}J(w,b)=\\dfrac{1}{m}\\sum\\limits*{i=1}^{m}\\dfrac{\\partial}{\\partial w_1}L(a^{(i)},y^{(i)}) J=0;dw1=0;dw2=0;db=0J=0;dw_1=0;dw_2=0;db=0 for i=1i=1 to mm z(i)=wTx(i)+bz^{(i)}=w^Tx^{(i)}+b a(i)=σ(z(i))a^{(i)}=\\sigma (z^{(i)}) J+=−[y(i)loga(i)+(1−y(i))log(1−a(i))]J+=-[y^{(i)}loga^{(i)}+(1-y^{(i)})log(1-a^{(i)})] dz(i)=a(i)−y(i)dz^{(i)}=a^{(i)}-y^{(i)} dw1+=x1(i)dz(i)dw_1+=x_1^{(i)}dz^{(i)} (for n = 2) dw2+=x2(i)dz(i)dw_2+=x_2^{(i)}dz^{(i)} (for n = 2) db+=dz(i)db+=dz^{(i)} J/=m;dw1/=m;dw2/=m;db/=mJ/=m;dw_1/=m;dw_2/=m;db/=m dw1=∂J∂w1;dw2=∂J∂w2dw_1=\\dfrac{\\partial J}{\\partial w_1}; dw_2=\\dfrac{\\partial J}{\\partial w_2} w1:=w1−αdw1w_1:=w_1-\\alpha dw_1 w2:=w2−αdw2w_2:=w_2-\\alpha dw_2 b:=b−αdbb:=b-\\alpha db 1.2.4 Computational Graph J(a,b,c)=3(a+bc)J(a,b,c)=3(a+bc) u=bcu=bc v=a+uv=a+u J=3vJ=3v Left to right computation Derivatives with a Computation Graph dJdv=3\\dfrac{dJ}{dv}=3 dJda=3\\dfrac{dJ}{da}=3 dvda=1\\dfrac{dv}{da}=1 Chain Rule: dJda=dJdv⋅dvda\\dfrac{dJ}{da}=\\dfrac{dJ}{dv}\\cdot\\dfrac{dv}{da} dJdu=3;dudb=2;dJdb=6\\dfrac{dJ}{du}=3; \\dfrac{du}{db}=2; \\dfrac{dJ}{db}=6 dudc=3;dJdc=9\\dfrac{du}{dc}=3; \\dfrac{dJ}{dc}=9 1.2.5 Vectorization avoid explicit for-loops. J=0;dw=np.zeros((nx,1));db=0J=0;dw=np.zeros((n_x,1));db=0 for i=1i=1 to mm z(i)=wTx(i)+bz^{(i)}=w^Tx^{(i)}+b a(i)=σ(z(i))a^{(i)}=\\sigma (z^{(i)}) J+=−[y(i)loga(i)+(1−y(i))log(1−a(i))]J+=-[y^{(i)}loga^{(i)}+(1-y^{(i)})log(1-a^{(i)})] dz(i)=a(i)−y(i)dz^{(i)}=a^{(i)}-y^{(i)} dw+=x(i)dz(i)dw+=x^{(i)}dz^{(i)} db+=dz(i)db+=dz^{(i)} J/=m;dw/=m;db/=mJ/=m;dw/=m;db/=m Z=np.dot(w.T,x)+bZ=np.dot(w.T,x)+b ; b(1,1)-->Broodcasting Vectorization Logistic Regression dz(1)=a(1)−y(1);dz(2)=a(2)−y(2)...dz^{(1)}=a^{(1)}-y^{(1)}; dz^{(2)}=a^{(2)}-y^{(2)}... dz=[dz(1),dz(2),...,dz(m)]dz=[dz^{(1)}, dz^{(2)},...,dz^{(m)}] 1×m1\\times m A=[a(1),a(2),...,a(m)]A=[a^{(1)}, a^{(2)}, ..., a^{(m)}] Y=[y(1),y(2),...,y(m)]Y=[y^{(1)}, y^{(2)}, ..., y^{(m)}] dz=A−Y=[a(1)−y(1),a(2)−y(2),...]dz=A-Y=[a^{(1)}-y^{(1)}, a^{(2)}-y^{(2)}, ...] Get rid of dbdb and dwdw in for loop db=1m∑_i=1mdz(i)=1mnp.sum(dz)db=\\dfrac{1}{m}\\sum\\limits\\_{i=1}^{m}dz^{(i)}=\\dfrac{1}{m} np.sum(dz) dw=1m⋅X⋅dzT=1m[x(1)...][dz(1)...]=1m⋅[x(1)dz(1)+...+x(m)dz(m)]dw=\\dfrac{1}{m}\\cdot X\\cdot dz^T=\\dfrac{1}{m}[x^{(1)}...][dz^{(1)}...]=\\dfrac{1}{m}\\cdot[x^{(1)}dz^{(1)}+...+x^{(m)}dz^{(m)}] n×1n\\times 1 New Form of Logistic Regression Z=wtX+b=np.dot(w.T,X)+bZ=w^tX+b=np.dot(w.T, X)+b A=σ(Z)A=\\sigma (Z) dz=A−Ydz=A-Y dw=1m⋅X⋅dZTdw=\\dfrac{1}{m}\\cdot X \\cdot dZ^T db=1mnp.sum(dz)db=\\dfrac{1}{m}np.sum(dz) w:=w−αdww:=w-\\alpha dw b:=b−αdbb:=b-\\alpha db Broadcasting(same as bsxfun in Matlab&#x2F;Octave) (m,n)(m,n)+-\\*/(1,n)(1,n)->(m,n)(m,n) 1->m will be all the same number. (m,n)(m,n)+-\\*/(m,1)(m,1)->(m,n)(m,n) 1->n will be all the same number Don’t use a=np.random.randn(5)a = np.random.randn(5) a.shape=(5,)a.shape = (5,) “rank 1 array” Use a=np.random.randn(5,1)a = np.random.randn(5,1) or a=np.random.randn(1,5)a = np.random.randn(1,5) Check by assert(a.shape==(5,1))assert(a.shape == (5,1)) Fix rank 1 array by a=a.reshape((5,1))a = a.reshape((5,1)) Logistic Regression Cost Function Lost p(y∣x)=y^y(1−y^)(1−y)p(y|x)=\\hat{y}^y(1-\\hat{y})^{(1-y)} If y=1y=1: p(y∣x)=y^p(y|x)=\\hat{y} If y=0y=0: p(y∣x)=(1−y^)p(y|x)=(1-\\hat{y}) log⁡p(y∣x)=log⁡y^y(1−y^)(1−y)=ylog⁡y^+(1−y)log⁡(1−y^)=−L(y^,y)\\log p(y|x)=\\log \\hat{y}^y(1-\\hat{y})^{(1-y)}=y\\log \\hat{y}+(1-y)\\log(1-\\hat{y})=-L(\\hat{y},y) Cost log⁡p(labels in training set)=log⁡Π_i=1mp(y(i),x(i))\\log p(labels\\space in\\space training\\space set)=\\log \\Pi\\_{i=1}^{m}p(y^{(i)},x^{(i)}) log⁡p(labels in training set)=∑∗i=1mlog⁡p(y(i),x(i))=−∑∗i=1mL(y^(i),y(i))\\log p(labels\\space in\\space training\\space set)=\\sum\\limits*{i=1}^m\\log p(y^{(i)},x^{(i)})=-\\sum\\limits*{i=1}^mL(\\hat{y}^{(i)},y^{(i)}) Use maximum likelihood estimation(MLE) Cost(minmize): J(w,b)=1m∑_i=1mL(y^(i),y(i))J(w,b)=\\dfrac{1}{m}\\sum\\limits\\_{i=1}^mL(\\hat{y}^{(i)},y^{(i)}) 1.3 Shallow Neural Networks1.3.1 Neural Network Representation Input layer, hidden layer, output layer a[0]=xa^{[0]}=x -> a[1]=[[a1[1],a2[1],a3[1],a4[1]]]a^{[1]}=[[a^{[1]}_1,a^{[1]}_2,a^{[1]}_3,a^{[1]}_4]] -> a[2]a^{[2]} Layers count by # of hidden layer+# of output layer. x1,x2,x3x_1,x_2,x_3 -> 4 hidden nodes4\\space hidden\\space nodes -> Output layerOutput\\space layer First hidden node: z[1]_1=w[1]T_1+b[1]_1,a[1]_1=σ(z[1]_1)z^{[1]}\\_1=w^{[1]T}\\_1+b^{[1]}\\_1, a^{[1]}\\_1=\\sigma(z^{[1]}\\_1) Seconde hidden node: z[1]_2=w[1]T_2+b[1]_2,a[1]_2=σ(z[1]_2)z^{[1]}\\_2=w^{[1]T}\\_2+b^{[1]}\\_2, a^{[1]}\\_2=\\sigma(z^{[1]}\\_2) Third hidden node: z[1]_3=w[1]T_3+b[1]_3,a[1]_3=σ(z[1]_3)z^{[1]}\\_3=w^{[1]T}\\_3+b^{[1]}\\_3, a^{[1]}\\_3=\\sigma(z^{[1]}\\_3) Forth hidden node: z[1]_4=w[1]T_4+b[1]_4,a[1]_4=σ(z[1]_4)z^{[1]}\\_4=w^{[1]T}\\_4+b^{[1]}\\_4, a^{[1]}\\_4=\\sigma(z^{[1]}\\_4) Vectorization w[1]=[−w[1]T_1−−w[1]T_2−−w[1]T_3−−w[1]T_4−](4,3)matrixw^{[1]}=\\begin{gathered}\\begin{bmatrix}-w^{[1]T}\\_1- \\\\ -w^{[1]T}\\_2- \\\\ -w^{[1]T}\\_3- \\\\ -w^{[1]T}\\_4- \\end{bmatrix}\\end{gathered} (4,3)matrix z[1]=[−w[1]T_1−−w[1]T_2−−w[1]T_3−−w[1]T_4−]⋅[x1x2x3]+[b[1]_1b[1]_2b[1]_3b[1]_4]=[w[1]T_1⋅x+b[1]_1w[1]T_2⋅x+b[1]_2w[1]T_3⋅x++b[1]_3w[1]T_4⋅x+b[1]_4]=[z[1]_1z[1]_2z[1]_3z[1]_4]z^{[1]}=\\begin{gathered}\\begin{bmatrix}-w^{[1]T}\\_1- \\\\ -w^{[1]T}\\_2- \\\\ -w^{[1]T}\\_3- \\\\ -w^{[1]T}\\_4- \\end{bmatrix}\\end{gathered}\\cdot \\begin{gathered}\\begin{bmatrix}x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\end{gathered} + \\begin{gathered}\\begin{bmatrix}b^{[1]}\\_1 \\\\ b^{[1]}\\_2 \\\\b^{[1]}\\_3 \\\\ b^{[1]}\\_4 \\end{bmatrix}\\end{gathered} =\\begin{gathered}\\begin{bmatrix}w^{[1]T}\\_1\\cdot x+b^{[1]}\\_1 \\\\ w^{[1]T}\\_2\\cdot x+b^{[1]}\\_2 \\\\ w^{[1]T}\\_3\\cdot x++b^{[1]}\\_3 \\\\ w^{[1]T}\\_4\\cdot x+b^{[1]}\\_4 \\end{bmatrix}\\end{gathered}=\\begin{gathered}\\begin{bmatrix}z^{[1]}\\_1 \\\\ z^{[1]}\\_2 \\\\z^{[1]}\\_3 \\\\ z^{[1]}\\_4 \\end{bmatrix}\\end{gathered} a[1]=[a[1]_1a[1]_2a[1]_3a[1]_4]=σ(z[1])a^{[1]}=\\begin{gathered}\\begin{bmatrix}a^{[1]}\\_1 \\\\ a^{[1]}\\_2 \\\\a^{[1]}\\_3 \\\\ a^{[1]}\\_4 \\end{bmatrix}\\end{gathered}=\\sigma(z^{[1]}) z[2]=W[2]⋅a[1]+b[2]z^{[2]}=W^{[2]}\\cdot a^{[1]}+b^{[2]} (1,1),(1,4),(4,1),(1,1)(1, 1),(1, 4),(4, 1),(1, 1) a[2]=σ(z[2])a^{[2]}=\\sigma(z^{[2]}) (1,1),(1,1)(1,1),(1,1) a[2](i)a^{[2](i)}: layer 22; example ii for i&#x3D;1 to m: z[1](i)=W[1]⋅x(i)+b[1]z^{[1](i)}=W^{[1]}\\cdot x(i)+b^{[1]} a[1](i)=σ(z[1](i))a^{[1](i)}=\\sigma(z^{[1](i)}) z[2](i)=W[2]⋅a[1](i)+b[2]z^{[2](i)}=W^{[2]}\\cdot a^{[1](i)}+b^{[2]} a[2](i)=σ(z[2](i))a^{[2](i)}=\\sigma(z^{[2](i)}) Vectorizing of the above for loop X=[∣∣∣∣x(1),x(2),...,x(m)∣∣∣∣](nx,m)matrixX=\\begin{gathered}\\begin{bmatrix}| &amp; | &amp; | &amp; | \\\\ x^{(1)}, &amp; x^{(2)}, &amp; ..., &amp; x^{(m)} \\\\ | &amp; | &amp; | &amp; |\\end{bmatrix}\\end{gathered} (n_x,m)matrix n is different hidden units Z[1]=W[1]⋅X+b[1]Z^{[1]}=W^{[1]}\\cdot X+b^{[1]} A[1]=σ(Z[1])A^{[1]}=\\sigma(Z^{[1]}) Z[2]=W[2]⋅A[1]+b[2]Z^{[2]}=W^{[2]}\\cdot A^{[1]}+b^{[2]} A[2]=σ(Z[2])A^{[2]}=\\sigma(Z^{[2]}) hrizontally: training examples; vertically: hidden units 1.3.2 Activation Functions g[i]g^{[i]}: activation function of layer ii Sigmoid: a=11+e[−z]a=\\dfrac{1}{1+e^{[-z]}} Tanh: a=ez−e[−z]ez+e[−z]a=\\dfrac{e^z-e^{[-z]}}{e^z+e^{[-z]}} ReLU: a=max(0,z)a=max(0,z) Leaky ReLu: a=max(0.01z,z)a=max(0.01z, z) Rules to choose activation function Output is between {0, 1}, choose sigmoid. Default choose ReLu. Why need non-liner activation function Use linear hidden layer will be useless to have multiple hidden layers. It will become a=w′x+b′a=w&#x27;x+b&#x27;. Linear may sometime use at output layer but with non-linear at hidden layers. 1.3.3 Forward and Backward Propogation Derivative of activation function Sigmoid: g′(z)=ddzg(z)=11+e[−z](1−11+e[−z])=g(z)(1−g(z))=a(1−a)g&#x27;(z)=\\dfrac{d}{dz}g(z)=\\dfrac{1}{1+e^{[-z]}}(1-\\dfrac{1}{1+e^{[-z]}})=g(z)(1-g(z))=a(1-a) Tanh: g′(z)=ddzg(z)=1−(tanh(z))2g&#x27;(z)=\\dfrac{d}{dz}g(z)=1-(tanh(z))^2 ReLU: g′(z)={0if z&lt;01if z≥0\\usepackageundefined\\usepackageif z=0g&#x27;(z)=\\left\\{\\begin{array}{lr}0&amp;if \\space z&lt;0 \\\\1&amp;if \\space z\\geq0\\\\\\usepackage{undefined}&amp;\\usepackage{if \\space z=0}\\end{array}\\right. Leaky ReLU: g′(z)={0.01if z&lt;01if z≥0g&#x27;(z)=\\left\\{\\begin{array}{lr}0.01&amp;if \\space z&lt;0 \\\\1&amp;if \\space z\\geq0\\end{array}\\right. Gradient descent for neural networks Parameters: w[1](n[1],n[2]),b[1](n[2],1),w[2](n[2],n[1]),b[2](n[2],1)w^{[1]}(n^{[1]},n^{[2]}), b^{[1]}(n^{[2]},1),w^{[2]}(n^{[2]},n^{[1]}), b^{[2]}(n^{[2]},1) nx=n[0],n[1],n[2]=1n_x=n^{[0]},n^{[1]},n^{[2]}=1 Cost function: J(w[1],b[1],w[2],b[2])=1m∑_i=1nL(y^,y)J(w^{[1]}, b^{[1]},w^{[2]}, b^{[2]})=\\dfrac{1}{m}\\sum\\limits\\_{i=1}^nL(\\hat{y},y) Forward propagation: Z[1]=W[1]⋅X+b[1]Z^{[1]}=W^{[1]}\\cdot X+b^{[1]} A[1]=g[1](Z[1])A^{[1]}=g^{[1]}(Z^{[1]}) Z[2]=W[2]⋅A[1]+b[2]Z^{[2]}=W^{[2]}\\cdot A^{[1]}+b^{[2]} A[2]=g[2](Z[2])=σ(Z[2])A^{[2]}=g^{[2]}(Z^{[2]})=\\sigma(Z^{[2]}) Back Propogation: dZ[2]=A[2]−YdZ^{[2]}=A^{[2]}-Y Y=[y(1),y(2),...,y(m)]Y=[y^{(1)},y^{(2)},...,y^{(m)}] dW[2]=1mdZ[2]A[1]TdW^{[2]}=\\dfrac{1}{m}dZ^{[2]}A^{[1]T} db[2]=1mnp.sum(dZ[2],axis=1,keepdims=True)db^{[2]}=\\dfrac{1}{m}np.sum(dZ^{[2]},axis=1,keepdims=True) dZ[1]=W[2]TdZ[2]\\*g′[1](Z1)dZ^{[1]}=W^{[2]T}dZ^{[2]}\\*g&#x27;^{[1]}(Z^{1}) (n[1],m)−&gt;element−wise product−&gt;(n[1],m)(n^{[1]},m)-&gt;element-wise\\space product-&gt;(n^{[1]},m) dW[1]=1mdZ[1]XTdW^{[1]}=\\dfrac{1}{m}dZ^{[1]}X^{T} db[1]=1mnp.sum(dZ[1],axis=1,keepdims=True)db^{[1]}=\\dfrac{1}{m}np.sum(dZ^{[1]},axis=1,keepdims=True) Random Initialization x1,x2−&gt;a1[1],a2[1]−&gt;a1[2]−&gt;y^x_1,x_2-&gt;a_1^{[1]},a_2^{[1]}-&gt;a_1^{[2]}-&gt;\\hat{y} w[1]=np.random.randn((2,2))\\*0.01w^{[1]}=np.random.randn((2,2))\\*0.01 b[1]=np.zeros((2,1))b^{[1]}=np.zeros((2,1)) w[2]=np.random.randn((1,2))\\*0.01w^{[2]}=np.random.randn((1,2))\\*0.01 b[2]=0b^{[2]}=0 1.4 Deep Neural Networks1.4.1 Deep L-Layer Neural Network Deep neural network notation L=4L=4 (#layers) n[l]=# units in layer ln^{[l]}= \\#\\space units\\space in\\space layer\\space l n[1]=5,n[2]=5,n[3]=3,n[4]=n[l]=1n^{[1]}=5,n^{[2]}=5,n^{[3]}=3,n^{[4]}=n^{[l]}=1 n[0]=nx=3n^{[0]}=n_x=3 a[l]=activations in layer la^{[l]}=activations\\space in\\space layer\\space l a[l]=g[l](z[l]), w[l]=weights for z[l], b[l]=bias for z[l]a^{[l]}=g^{[l]}(z^{[l]}),\\space w^{[l]}=weights\\space for\\space z^{[l]},\\space b^{[l]}=bias\\space for\\space z^{[l]} x=a[0], y^=alx=a^{[0]},\\space \\hat{y}=a^{l} 1.4.2 Forward Propagation in a Deep Network General: Z[l]=w[l]A[l−1]+b[l],A[l]=g[l](Z[l])Z^{[l]}=w^{[l]}A^{[l-1]}+b^{[l]}, A^{[l]}=g^{[l]}(Z^{[l]}) x:z[1]=w[1]a[0]+b[1],a[1]=g[1](z[1])x: z^{[1]}=w^{[1]}a^{[0]}+b^{[1]}, a^{[1]}=g^{[1]}(z^{[1]}) a[0]=Xa^{[0]}=X z[2]=w[2]a[1]+b[2],a[1]=g[2](z[2])z^{[2]}=w^{[2]}a^{[1]}+b^{[2]}, a^{[1]}=g^{[2]}(z^{[2]}) … z[4]=w[4]a[3]+b[4],a[4]=g[4](z[4])=y^z^{[4]}=w^{[4]}a^{[3]}+b^{[4]}, a^{[4]}=g^{[4]}(z^{[4]})=\\hat{y} Vectorizing: Z[1]=w[1]A[0]+b[1],A[1]=g[1](Z[1])Z^{[1]}=w^{[1]}A^{[0]}+b^{[1]}, A^{[1]}=g^{[1]}(Z^{[1]}) A[0]=XA^{[0]}=X Z[2]=w[2]A[1]+b[2],A[2]=g[2](Z[2])Z^{[2]}=w^{[2]}A^{[1]}+b^{[2]}, A^{[2]}=g^{[2]}(Z^{[2]}) Y^=g(Z[4])=A[4]\\hat{Y}=g(Z^{[4]})=A^{[4]} Matrix dimensions z[1]=w[1]⋅x+b[1]z^{[1]}=w^{[1]}\\cdot x+b^{[1]} z[1]=(3,1),w[1]=(3,2),x=(2,1),b[1]=(3,1)z^{[1]}=(3,1),w^{[1]}=(3,2),x=(2,1),b^{[1]}=(3,1) z[1]=(n[1],1),w[1]=(n[1],n[0]),x=(n[0],1),b[1]=(n[1],1)z^{[1]}=(n^{[1]},1),w^{[1]}=(n^{[1]},n^{[0]}),x=(n^{[0]},1),b^{[1]}=(n^{[1]},1) w[l]/dw[l]=(n[l],n[l−1]),b[l]/db[l]=(n[l],1)w^{[l]}/dw^{[l]}=(n^{[l]},n^{[l-1]}),b^{[l]}/db^{[l]}=(n^{[l]},1) z[l],a[l]=(n[l],1),Z[l]/dZ[l],A[l]/dA[l]=(n[l],1)z^{[l]},a^{[l]}=(n^{[l]},1),Z^{[l]}/dZ^{[l]},A^{[l]}/dA^{[l]}=(n^{[l]},1) l=0,A[0]=X=(n[0],m)l=0, A^{[0]}=X=(n^{[0]},m) Why deep representation? Earier layers learn simple features; later deeper layers put together to detect more complex things. Circuit theory and deep learning: Informally: There are functions you can compute with a “small” L-layer deep neural network that shallower networks require exponentially more hidden units to compute. 1.4.3 Building Blocks of Deep Neural Networks Forward and backward functions Layer l:w[l],b[l]l:w^{[l]},b^{[l]} Forward: Input a[l−1]a^{[l-1]}, output a[l]a^{[l]} z[l]:w[l]a[l−1]+b[l]z^{[l]}:w^{[l]}a^{[l-1]}+b^{[l]} cache z[l]cache\\space z^{[l]} a[l]:g[l](z[l])a^{[l]}:g^{[l]}(z^{[l]}) Backward: Input da[l],cache(z[l])da^{[l]}, cache(z^{[l]}), output da[l−1],dw[l],db[l]da^{[l-1]},dw^{[l]},db^{[l]} One iteration of gradient descent of neural network How to implement? Forward propagation for layer ll Input a[l−1]a^{[l-1]}, output a[l],cache (z[l])a^{[l]},cache\\space (z^{[l]}) z[l]=w[l]a[l−1]+b[l]z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]} a[l]=g[l](z[l])a^{[l]}=g^{[l]}(z^{[l]}) Vectoried Z[l]=W[l]A[l−1]+b[l]Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]} A[l]=g[l](Z[l])A^{[l]}=g^{[l]}(Z^{[l]}) Backward propagation for layer ll Input da[l],cache(z[l])da^{[l]}, cache(z^{[l]}), output da[l−1],dw[l],db[l]da^{[l-1]},dw^{[l]},db^{[l]} dz[l]=da[l]\\*g′[l](z[l])dz^{[l]}=da^{[l]}\\*g&#x27;^{[l]}(z^{[l]}) dw[l]=dz[l]⋅a[l−1]dw^{[l]}=dz^{[l]}\\cdot a^{[l-1]} db[l]=dz[l]db^{[l]}=dz^{[l]} da[l−1]=w[l]T⋅dz[l]da^{[l-1]}=w^{[l]T}\\cdot dz^{[l]} Vectorized: dZ[l]=dA[l]\\*g′[l](Z[l])dZ^{[l]}=dA^{[l]}\\*g&#x27;^{[l]}(Z^{[l]}) dW[l]=1mdZ[l]A[l−1]TdW^{[l]}=\\dfrac{1}{m}dZ^{[l]}A^{[l-1]T} db[l]=1mnp.sum(dZ[l],axis=1,keepdims=True)db^{[l]}=\\dfrac{1}{m}np.sum(dZ^{[l]},axis=1,keepdims=True) dA[l−1]=W[l]T⋅dZ[l]dA^{[l-1]}=W^{[l]T}\\cdot dZ^{[l]} 1.4.4 Parameters vs. Hyperparameters Parameters: W[1],b[1],W[2],b[2],...W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]},... Hyperparameters (will affect&#x2F;control&#x2F;determine parameters): learning rate α\\alpha # iterations # of hidden units n[1],n[2],...n^{[1]},n^{[2]},... # of hidden layers Choice of activation function Later: momemtum, minibatch size, regularization parameters,… II. Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization2.1 Practical Aspects of Deep Learning2.1.1 Train &#x2F; Dev &#x2F; Test sets Big data may need only 1% or even less dev&#x2F;test sets. Mismatched: Make sure dev&#x2F;test come from same distribution Not having a test set might be okay. (Only dev set.) 2.1.2 Bias &#x2F; Variance Assume optimal (Bayes) error: ≈0%\\approx0\\% High bias (underfitting): The prediction cannot classify different elemets as we want. Training set error 15%15\\%, Dev set error 16%16\\%. Training set error 15%15\\%, Dev set error 30%30\\%. “just right”: The prediction perfectly classify different elemets as we want. Training set error 0.5%0.5\\%, Dev set error 1%1\\%. High variance (overfitting): The prediction 100% classify different elemets. Training set error 1%1\\%, Dev set error 11%11\\%. Training set error 15%15\\%, Dev set error 30%30\\%. 2.1.3 Basic Recipe for Machine Learning2.1.3.1 Basic Recipe High bias(training data performance) Bigger network Train longer (NN architecture search) High variance (dev set performance) More data Regulairzation (NN architecture search) 2.1.3.2 Regularization Logistic regression. min⁡_w,bJ(w,b)\\min\\limits\\_{w,b}J(w,b) w∈Rnx,b∈Rw\\in\\mathbb{R}^{n_x}, b\\in\\mathbb{R} λ=regularization parameter\\lambda=regularization\\space parameter J(w,b)=1m∑_i=1mL(y^(i),y(i))+λ2m∣∣w2∣∣_2J(w,b)=\\dfrac{1}{m}\\sum\\limits\\_{i=1}^mL(\\hat{y}^{(i)},y^{(i)})+\\dfrac{\\lambda}{2m}||w^2||\\_2 L2 regularization ∣∣w2∣∣2=∑j=1nxwj2=wTw||w^2||_2=\\sum\\limits_{j=1}^{n_x}w_j^2=w^Tw L1 regularization λ2m∑_j=1nx∣wj∣=λ2m∣∣w∣∣_1\\dfrac{\\lambda}{2m}\\sum\\limits\\_{j=1}^{n_x}|w_j|=\\dfrac{\\lambda}{2m}||w||\\_1 ww will be spouse(for L1) (will have lots of 0 in it, only help a little bit) Neural network J(w[1],b[1],...,w[l],b[l])=1m∑∗i=1mL(y^(i),y(i))+λ2m∑∗l=1l∣∣w2∣∣_FJ(w^{[1]},b^{[1]},...,w^{[l]},b^{[l]})=\\dfrac{1}{m}\\sum\\limits*{i=1}^{m}L(\\hat{y}^{(i)},y^{(i)})+\\dfrac{\\lambda}{2m}\\sum\\limits*{l=1}^{l}||w^2||\\_F ∣∣w[l]∣∣F2=∑i=1n[l−1]∑∗j=1n[l](w∗ij[l])2||w^{[l]}||_F^2=\\sum\\limits_{i=1}^{n^{[l-1]}}\\sum\\limits*{j=1}^{n^{[l]}}(w*{ij}^{[l]})^2 w:(w[l],w[l−1])w: (w^{[l]},w^{[l-1]}) Frobenius norm: Square root of square sum of all elements in a matrix. dw[l]=(from backprop)+λmw[l]dw^{[l]}=(from\\space backprop)+\\dfrac{\\lambda}{m}w^{[l]} w[l]:=w[l]−αdw[l]w^{[l]}:=w^{[l]}-\\alpha dw^{[l]} (keep the same) Weight decay w[l]:=w[l]−α[(from backprop)+λmw[l]]w^{[l]}:=w^{[l]}-\\alpha[(from\\space backprop)+\\dfrac{\\lambda}{m}w^{[l]}] ​ =w[l]−αλmw[l]−α(from backprop)=w^{[l]}-\\dfrac{\\alpha\\lambda}{m}w^{[l]}-\\alpha(from\\space backprop) ​ =(1−αλm)w[l]−α(from backprop)=(1-\\dfrac{\\alpha\\lambda}{m})w^{[l]}-\\alpha(from\\space backprop) How does regularization prevent overfitting: λ\\lambda bigger w[l]w^{[l]} smaller z[l]z^{[l]} smaller, which will make the activation function nearly linear(take tanh as an example). This will cause the network really hard to draw boundary with curve. Dropout regularization Implementing dropout(“Inverted dropout”) Illustrate with layer l=3l=3 keep−prob=0.8keep-prob=0.8 (means 0.2 chance get dropout&#x2F;be 0 out) d3=np.random.rand(a3.shape[0],a3.shape[1])&lt;keep−probd3 = np.random.rand(a3.shape[0],a3.shape[1]) &lt; keep-prob #This will set d3 to be a same shape matrix as a3 with True (1), False (0) value. a3=np.multiply(a3,d3)a3 = np.multiply(a3, d3) #a3\\*=d3; This will let some neruons been dropout a3/=keep−proba3/=keep-prob #inverted dropout, keep the total avtivation the same before and after dropout. Why work: Can’t rely on any one feature, so have to spread out weights.(shrink weights) First make sure the J is decreasing during iteration, then turn on dropout. Data augmentation Image: crop, flop, twist… Early stopping Mid-size ∣∣w∣∣_F2||w||\\_F^2 May caused optimize cost function and not overfir at the same time. Orthogonalization Only consider optimize cost function or consider not overfit at one time. 2.1.3.3 Setting up your optimization problem Normalizing training sets x=[x1x2]x=\\begin{gathered}\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}\\end{gathered} Subtract mean: μ=1m∑_i=1mx(i)\\mu=\\dfrac{1}{m}\\sum\\limits\\_{i=1}^{m}x^{(i)} x:=x−μx:=x-\\mu Normalize variance: σ2=1m∑_i=1mx(i)∗∗2\\sigma^2=\\dfrac{1}{m}\\sum\\limits\\_{i=1}^{m}x^{(i)}**2 \"**\" element-wise x/=σ2x/=\\sigma^2 Use same μ,σ2\\mu,\\sigma^2 to normalize test set. Why normalize inputs? When inputs in very different scales will help a lot for performance and gradient descent&#x2F;learning rate. Vanishing&#x2F;exploding gradients w[l]&gt;Iw^{[l]}&gt;I Just slightly, will make the gradient increase really fast (exploding). w[l]&lt;Iw^{[l]}&lt;I Just slightly, will make the gradient decrease really slow (varnishing). Weight initalization (Single neuron) large nn (number of input features) –&gt; smaller wiw_i Variance(w:)=1nVariance(w:)=\\dfrac{1}{n} (sigmoid/tanh) ReLU: 2n\\dfrac{2}{n} (variance can be a hyperparameter, DO NOT DO THAT) w[l]=np.random.randn(shapeOfMatrix)\\*np.sqrt(1n[l−1])w^{[l]}=np.random.randn(shapeOfMatrix)\\*np.sqrt(\\dfrac{1}{n^{[l-1]}}) ReLU: 2n[l−1]\\dfrac{2}{n^{[l-1]}} Xavier initialization: 1n[l−1])\\sqrt{\\dfrac{1}{n^{[l-1]}})} Sometime 2n[l−1]+n[l])\\sqrt{\\dfrac{2}{n^{[l-1]}+n^{[l]}})} Numerical approximation of gradients f(θ+ϵ)−f(θ−ϵ)2ϵ\\dfrac{f(\\theta+\\epsilon)-f(\\theta-\\epsilon)}{2\\epsilon} Gradient checking (Grad check) Take W[1],b[1],...,W[L],b[L]W^{[1]},b^{[1]},...,W^{[L]},b^{[L]} and reshape into a big vector θ\\theta. Take dW[1],db[1],...,dW[L],db[L]dW^{[1]},db^{[1]},...,dW^{[L]},db^{[L]} and reshape into a big vector dθd\\theta. for each i: dθ_approx[i]=J(θ1,θ2,...,θi+ϵ,...)−J(θ1,θ2,...,θi−ϵ,...)2ϵ≈dθ[i]=∂J∂θid\\theta\\_{approx}[i]=\\dfrac{J(\\theta_1,\\theta_2,...,\\theta_i+\\epsilon,...)-J(\\theta_1,\\theta_2,...,\\theta_i-\\epsilon,...)}{2\\epsilon}\\approx d\\theta[i]=\\dfrac{\\partial J}{\\partial \\theta_i} Check Euclidean distance ∣∣dθ∗approx−dθ∣∣_2∣∣dθ∗approx∣∣_2+∣∣dθ∣∣_2\\dfrac{||d\\theta*{approx}-d\\theta||\\_2}{||d\\theta*{approx}||\\_2+||d\\theta||\\_2} (∣∣.∣∣_2||.||\\_2 is Euclidean norm, sqare root of the sum of all elements’ power of 2) take ϵ=10−7\\epsilon=10^{-7}, if above Euclidean distance is ≈10−7\\approx10^{-7} or smaller, is great. If is 10−510^{-5} or bigger may need to check. If is 10−310^{-3} or bigger may need to worry, maybe a bug. Check which i approx is difference between the real value. notes: Don’t use in training - only to debug. If algorithm fails grad check, look at components to try to identify bug. Remember regularization. (include the λ2m\\dfrac{\\lambda}{2m}) Doesn’t work with dropout. (since is random, implement without dropout) Run at random initialization; perhaps again after some training. (not work when w,b≈0w,b\\approx0) 2.2 Optimization Algorithms2.2.1 Mini-batch gradient descent Batch vs. mini-batch gradient descent Normal batch may have large amount of data like millions of elements. set m=5,000,000m=5,000,000 X=[x(1),x(2),x(3),...,x(1000),x(1001),...,x(2000),...,x(m)](nx,m)X=[x^{(1)},x^{(2)},x^{(3)},...,x^{(1000)},x^{(1001)},...,x^{(2000)},...,x^{(m)}] (n_x,m) Y=[y(1),y(2),y(3),...,y(m)](1,m)Y=[y^{(1)},y^{(2)},y^{(3)},...,y^{(m)}] (1,m) Mini-batches make 1,000 xx each. Mini-batch number t:X{t},Y{t}t:X^{\\{t\\}},Y^{\\{t\\}} x(i)x^{(i)} ith in trainning set, z[l]z^{[l]} layer in network X{t}X^{\\{t\\}} batch in mini-batch X=[X{1},X{2},...,X{5000}]X = [X^{\\{1\\}},X^{\\{2\\}},...,X^{\\{5000\\}}] Y=[Y{1},Y{2},Y{3},...,Y(5,000)]Y=[Y^{\\{1\\}},Y^{\\{2\\}},Y^{\\{3\\}},...,Y^{(5,000)}] Mini-batch gradient descent 1 step of gradient descent using X{t},Y{t}X^{\\{t\\}},Y^{\\{t\\}} (1000) 1 epoch: single pass through training set. for t=1,...,5000for\\space t=1,...,5000 Forward prop on X{t}X^{\\{t\\}} Z[1]=W[1]X{t}+b[1]Z^{[1]}=W^{[1]}X^{\\{t\\}}+b^{[1]} A[1]=g[1](Z[1])A^{[1]}=g^{[1]}(Z^{[1]}) … A[l]=g[l](Z[l])A^{[l]}=g^{[l]}(Z^{[l]}) Compute cost J{t}=11000∑∗i=1lL(y^(i),y(i))+λ2⋅1000∑∗l=1l∣∣w[l]∣∣_F2J^{\\{t\\}}=\\dfrac{1}{1000}\\sum\\limits*{i=1}^{l}L(\\hat{y}^{(i)},y^{(i)})+\\dfrac{\\lambda}{2\\cdot1000}\\sum\\limits*{l=1}^{l}||w^{[l]}||\\_F^2 y^(i),y(i)\\hat{y}^{(i)},y^{(i)} --> from X{t},Y{t}X^{\\{t\\}},Y^{\\{t\\}} Backprop to compute gradient cost J{t} (using (X{t},Y{t}))J^{\\{t\\}}\\space (using\\space (X^{\\{t\\}},Y^{\\{t\\}})) w[l]:=w[l]−αdw[l],b[l]:=b[l]−αdb[l]w^{[l]}:=w^{[l]}-\\alpha dw^{[l]}, b^{[l]}:=b^{[l]}-\\alpha db^{[l]} Understanding mini-batch gradient descent If mini-batch size&#x3D;m:batch gradient descent (Too long per iteration).–(X{t},Y{t})=(X,Y)(X^{\\{t\\}},Y^{\\{t\\}})=(X,Y) If mini-batch size&#x3D;1:Stochatic gradient descent (noisy, not converge, loos speedup from vectorization).– Every example is it own mini-batch. In practice: select in-between 1 and m. Get lots of vectorization Make progress without needing to wait entire training set. Choosing mini-batch size No need for small training set (m&lt;2000m&lt;2000) Typical mini-batch size: 64, 128, 256, 512. (Use power of 2) Make sure minibatch fir in CPU&#x2F;GPU memory. 2.2.2 Exponentially weighted averages V∗t=βV∗t−1+(1−β)θtV*t = \\beta V*{t-1} + (1 - \\beta) \\theta_t VtV_t is the weighted average at time tt. θt\\theta_t is the actual observed value at time tt. β\\beta is the decay rate (usually between 0 and 1). V_t−1V\\_{t-1} is the weighted average at the previous time step. Impact of Decay Rate β\\beta: The value of β\\beta significantly affects the smoothness of the weighted average curve: A larger β\\beta makes the curve smoother, as it gives more weight to past observations, thereby reducing the impact of recent changes on the weighted average. A smaller β\\beta makes the curve more responsive to recent changes, as it gives more weight to recent observations. Interpretation of (1−ϵ)1ϵsome constant=1e\\dfrac{(1-\\epsilon)^{\\frac{1}{\\epsilon}}}{\\text{some constant}} = \\dfrac{1}{e} Defining ϵ\\epsilon as 1−β1 - \\beta provides insight into how the influence of past data gradually diminishes as β\\beta approaches 1 (i.e., ϵ\\epsilon approaches 0). As ϵ\\epsilon approaches 0, (1−ϵ)1ϵ(1-\\epsilon)^{\\frac{1}{\\epsilon}} approaches 1e\\dfrac{1}{e}, indicating that even though past data is given more weight (high β\\beta), its actual impact on the current value is decreasing. Implementation v_θ:=0v\\_{\\theta}:=0 Repear for each day: Get the next θt\\theta_t v∗θ:=βv∗θ+(1−β)θtv*\\theta:=\\beta v*\\theta+(1-\\beta)\\theta_t Bias correction in exponentially weighted averages Bias correction is applied to counteract the initial bias in exponentially weighted averages, especially when the number of data points is small or at the start of the calculation. vt1−βt\\dfrac{v_t}{1-\\beta^t} Here, vtv_t is the uncorrected exponentially weighted average at time tt, and β\\beta is the decay rate. It ensures that the moving averages are not underestimated, particularly when β\\beta is high and in the early stages of the iteration. With iteration goes on, the affect of this correction will become smaller since βt\\beta^t is closer to 1. Gradient descent with momentum On iteration t: Compute dw,dbdw, db on current mini-batch (whole batch if not using mini-batch) v∗dw=βv∗dw+(1−β)dwv*{dw}=\\beta v*{dw}+(1-\\beta)dw v∗db=βv∗db+(1−β)dbv*{db}=\\beta v*{db}+(1-\\beta)db w:=w−αv∗dw,b:=b−αv∗dbw:=w-\\alpha v*{dw}, b:=b-\\alpha v*{db} initiate v∗dw and v∗db=0v*{dw}\\space and\\space v*{db} = 0 Smooth out gradient descent The momentum term vv effectively provides a smoothing effect since it is an average of past gradients. This means that extreme gradient changes in a single iteration are averaged out, reducing the volatility of the update steps. This smoothing effect is particularly useful on loss function surfaces that are not flat or have many local minima. Consider set β\\beta as 0.90.9 (common, about the average last 10 gradients), it gives more weight to v_dwv\\_{dw}, consider dwdw as the acceleration. With betabeta decreasing, velocity increasing slower and acceleration increasing faster. 2.2.3 RMSprop and Adam optimization RMSprop (Root Mean Square Propagation) On iteration t: Compute dw,dbdw, db on current mini-batch s∗dw=β2s∗dw+(1−β2)dw2s*{dw}=\\beta_2 s*{dw}+(1-\\beta_2)dw^2 Hope to be relative small. s∗db=β2s∗db+(1−β2)db2s*{db}=\\beta_2 s*{db}+(1-\\beta_2)db^2 Hope to be relative large. w:=w−αdws∗dw+ϵ,b:=b−αdbs∗db+ϵw:=w-\\alpha\\dfrac{dw}{\\sqrt{s*{dw}}+\\epsilon}, b:=b-\\alpha\\dfrac{db}{\\sqrt{s*{db}}+\\epsilon} ϵ\\epsilon is a realative small number(10−810^{-8}) ot prevent nominaotr being 0. Slow down in vertical direction, fast in horizontal direction. Adam (Adaptive moment estimation) optimization algorithm v∗dw=0,s∗dw=0.v∗db=0,s∗dw=0v*{dw}=0, s*{dw}=0. v*{db}=0, s*{dw}=0 On iteration t: Compute dw,bddw, bd using current mini-batch v∗dw=β1v∗dw+(1−β∗1)dw,v∗db=β∗1v∗db+(1−β1)dbv*{dw}=\\beta_1v*{dw}+(1-\\beta*1)dw,v*{db}=\\beta*1v*{db}+(1-\\beta_1)db s∗dw=β2s∗dw+(1−β∗2)dw2,s∗db=β∗2s∗db+(1−β2)dbs*{dw}=\\beta_2s*{dw}+(1-\\beta*2)dw^2,s*{db}=\\beta*2s*{db}+(1-\\beta_2)db v∗dwcorrected=v∗dw1−β∗1t,v∗dbcorrected=v_db1−β1tv*{dw}^{corrected}=\\dfrac{v*{dw}}{1-\\beta*1^t}, v*{db}^{corrected}=\\dfrac{v\\_{db}}{1-\\beta_1^t} s∗dwcorrected=s∗dw1−β∗2t,s∗dbcorrected=s_db1−βsts*{dw}^{corrected}=\\dfrac{s*{dw}}{1-\\beta*2^t}, s*{db}^{corrected}=\\dfrac{s\\_{db}}{1-\\beta_s^t} w:=w−αv∗dwcorrecteds∗dwcorrected+ϵ,b:=b−αv∗dbcorrecteds∗dbcorrected+ϵw:=w-\\alpha\\dfrac{v*{dw}^{corrected}}{\\sqrt{s*{dw}^{corrected}}+\\epsilon}, b:=b-\\alpha\\dfrac{v*{db}^{corrected}}{\\sqrt{s*{db}^{corrected}}+\\epsilon} Hyperparameters choice: α\\alpha: needs to be tune β1\\beta_1: 0.9 (dwdw) First moment β2\\beta_2: 0.999 (dw2dw^2) Second moment ϵ:10−8\\epsilon: 10^{-8} Not affect performance Learning rate decay 1 epoch &#x3D; 1 pass through the data α=11+decay−rate\\*epoch−numα0\\alpha=\\dfrac{1}{1+decay-rate\\*epoch-num}\\alpha_0 Other methods α=0.95epoch−num⋅α0\\alpha=0.95^{epoch-num}\\cdot \\alpha_0 ---- exponentially decay α=kepoch−num⋅α0\\alpha=\\dfrac{k}{\\sqrt{epoch-num}}\\cdot\\alpha_0 or kt⋅α0\\dfrac{k}{\\sqrt{t}}\\cdot\\alpha_0 ---- discrete staircase Manual decay (small number of model) The problem of local optima Unlikely to stuck in a bad local optima, since there are too many dimensions and all algorithms in deep learning. saddle point —- gradient &#x3D; 0 Problem of plateaus: Make learning slow 2.3.1 Tuning process Hyperparameters α\\alpha: learning rate (1st) β\\beta: momentum (2nd) β1,β2,ϵ\\beta_1, \\beta_2, \\epsilon # of layers (3rd) # of hidden units (2nd) learning rate decay (3rd) mini-batch size (2nd) Try random values: Don’t use a grid Coarse to fine: Trying coarse random first, then fine in working well range. 2.3.2 Using an appropriate scale to pick hyperparameters Learning rate: α=0.0001,...,1\\alpha = 0.0001,...,1 r=−4\\*np.random.rand()r=-4\\*np.random.rand() ---- r∈[−4,0]r\\in[-4,0] r∈[a,b]r\\in[a,b] a=log∗100.0001=−4,b=log∗101=0a=log*{10}0.0001 = -4, b=log*{10}1 = 0 α=10r\\alpha=10^r ----- α∈[10−4...100]\\alpha\\in[10^{-4}...10^0] Exponentially Weighted Averages Decay Rate: β=0.9(last 10),...,0.999(last 1000)\\beta=0.9(last\\space 10),...,0.999(last\\space1000) 1−β=0.1,...,0.001 r∈[−3,−1]1-\\beta=0.1,...,0.001\\space r\\in[-3,-1] Reason for focusing on this instead of single β\\beta: β\\beta is too close to 1, small changes may have big affects. 1−β=10r1-\\beta=10^r β=1−10r\\beta=1-10^r In practice: Re-test&#x2F;Re-evaluate occasionally. Babysitting one model (don’t have enough training capacity) (Panda): One model at one time. Training many models in parallel (Caviar): Can try many at same time. 2.3.3 Batch Normalization Implementing Batch Norm Batch Norm: make sure hidden units have standardized mean and variance. Given some intermediate value in NN z(1),...,z(m)−z[l](i)z^{(1)},...,z^{(m)}-z^{[l](i)} (ll for some hidden layers, ii for 1 through mm) μ=1m∑_iz(i)\\mu=\\dfrac{1}{m}\\sum\\limits\\_{i}z^{(i)} (Mean) σ2=1m∑i(zi−μ)2\\sigma^2=\\dfrac{1}{m}\\sum\\limits_i(z_i-\\mu)^2 (Variance) z_norm(i)=z(i)−μσ2+ϵz\\_{norm}^{(i)}=\\dfrac{z^{(i)}-\\mu}{\\sqrt{\\sigma^2+\\epsilon}} (Make sure mean=0, variance=1. ϵ\\epsilon prevent denominator=0) z~(i)=γz_norm(i)+β\\widetilde{z}^{(i)}=\\gamma z\\_{norm}^{(i)}+\\beta (γ,β\\gamma, \\beta are learnable parameters of model) If γ=σ2+ϵ\\gamma=\\sqrt{\\sigma^2+\\epsilon} β=μ\\beta=\\mu Then z~(i)=z_norm(i)\\widetilde{z}^{(i)}=z\\_{norm}^{(i)} Use z~[l](i)\\widetilde{z}^{[l](i)} instead of z[l](i)z^{[l](i)} Adding Batch Norm to a network Parameters: w[1],b[1],β[1],γ[1],...,w[l],b[l],β[l],γ[l]w^{[1]},b^{[1]},\\beta^{[1]},\\gamma^{[1]},...,w^{[l]},b^{[l]},\\beta^{[l]},\\gamma^{[l]} May use gradient&#x2F;Adam&#x2F;momentum to tune dβ[l]d\\beta^{[l]} β[l]=β[l]−αdβ[l]\\beta^{[l]}=\\beta^{[l]}-\\alpha d\\beta^{[l]} Working with mini-batches: Work the same but on single batches. No need for b[l]b^{[l]}, since variance are all 1. β γ\\beta\\space\\space\\gamma have same dimension with bb. Implementing gradient descent (works with momentum, RMSprop, Adam) for t=1t=1…numMiniBatches Compute forwardProp on X^. In each hidden layer use BN to replace z[l]z^{[l]} with z~[l]\\widetilde{z}^{[l]} Use backprop to compute dw[l],db[l],dβ[l],dγ[l]dw^{[l]},db^{[l]},d\\beta^{[l]},d\\gamma^{[l]} (no dbdb) Update parameters w[l]:=w[l]−αdw[l]w^{[l]}:=w^{[l]}-\\alpha dw^{[l]} β[l]:=β[l]−αdβ[l]\\beta^{[l]}:=\\beta^{[l]}-\\alpha d\\beta^{[l]} γ[l]:=γ[l]−αdγ[l]\\gamma^{[l]}:=\\gamma^{[l]}-\\alpha d\\gamma^{[l]} Why does Batch Norm work Covariate Shift: Different test and training data (training on black cats but try to test on other color of cats). Internal Covariate Shift: Between different layers of the network, the distribution of inputs to each layer changes. Recursively it changes the input of the latter layer. May lead to instability and reduced efficiency. Batch norm reduces the problem of input values changes. Make input stable. Let the network learn more independent. Batch norm as regularization In mini-batch, each batch is scaled by the mean&#x2F;variance computed on just that mini-batch. May adds some noise to each hidden layer’s (since is not consider the whole training set) (similar to dropout). This has a slight regularization effect. (Use larger mini-batch size could reduce regularization) Batch Norm at test time μ,σ2\\mu, \\sigma^2: estimate using exponentially weighted average (across mini-batch). μ∗global=βμ∗global+(1−β)μ\\mu*{\\text{global}} = \\beta \\mu*{\\text{global}} + (1 - \\beta) \\mu σ2∗global=βσ2∗global+(1−β)σ2\\sigma^2*{\\text{global}} = \\beta \\sigma^2*{\\text{global}} + (1 - \\beta) \\sigma^2 During testing, use the global mean and variance estimates for normalization, instead of the statistics from the current test sample or mini-batch. 2.3.4 Multi-class classification Softmax regression CC = # classes = 4 (0,...,3) Output layer: 4 nodes for each class. y^\\hat{y} is (4,1) matrix, sum should be 1. z[L]=w[L]a[L−1]+bLz^{[L]}=w^{[L]}a^{[L-1]}+b^{L} (4,1) vector (L represents the output layer) Activation function: t=e(z[L])t=e^{(z^{[L]})} (4,1) vector a[L]=ez[L]∑∗j=14ti (4,1),a[L]_i=ti∑∗j=14tia^{[L]}=\\dfrac{e^{z^{[L]}}}{\\sum\\limits*{j=1}^4t_i}\\space\\space(4,1), a^{[L]}\\_i=\\dfrac{t_i}{\\sum\\limits*{j=1}^4t_i} Hardmax: Change beigest to 1, rest all set to 0. Training a softmax classifier If C=2C=2, softmax reduces to logistic regression. Loss function: z[L]z^{[L]}->a[L]=y^a^{[L]}=\\hat{y}->L(y^,y)L(\\hat{y},y) (4,1) Backprop: dz[L]=y^−ydz^{[L]}=\\hat{y}-y (4,1) dz[L]=∂J∂Z[L]dz^{[L]}=\\dfrac{\\partial{J}}{\\partial{Z^{[L]}}} Deep Learning frameworks TensorFlow III. Structuring Machine Learning Projects3.1 ML Strategy (1)3.1.1 Setting up your goal Orthogonalization Chain of assumptions in ML Fir training set well on cost function: bigger network; Adam Fit dev set well on cost function: Regularization; Bigger training set Fit test set well on cost function: Bigger dev set Perorms well in real world: Change dev set or cost function Single number evaluation metric Precision: In examples recognized, what percentage are actually true. Recall: What percentage of target are correctly recognized in whole test set. F1 Score: Average of precision and recall. 11P+1R\\dfrac{1}{\\dfrac{1}{P}+\\dfrac{1}{R}} (harmonic mean) Dev set + Single number evaluation matric: Speed up iteration Use average error rate instead of single error rate for each classes in estimate many classes at same time. Satisficing and optimizing matrics Consider classifiers with accuracy and running time. maximize accuracy and subject to running time &lt;&#x3D; 100ms Accuracy: optimizing Running time: satisfiying N metic: 1 optimizing, n-1 satisficing Train&#x2F;dev&#x2F;test distributions Come from same distribution. (Use randomly shuffle) Choose a dev set and test set to reflect data you expect to get in the future and consider important to do well on. Size of dev&#x2F;test set For large data set, use 98% training, 1% dev, 1% test Size of test set: Set your test set to be big enough to give high confidence in the overall performance of your system. Sometime use only train+dev, without test set. When to change dev&#x2F;test sets and metrics Filter pornographic images out of error rate: Two Steps How to define a metric to evaluate classifiers. How to do well on this metric. If doing well on your metric + dev&#x2F;test set does not correspond to doing well on your application, change your metric and&#x2F;or dev&#x2F;test set. 3.1.2 Comparing to human-level performance Why human-level performance: Bayes (optimal) error: best possible error. Can never surpass. Humans are quite good at a lot of tasks. So long as ML is worse than humans, you can: Get labeled data from humans. Gain insight from manual error analysis: Why did a person get this right? Better analysis of bias&#x2F;variance. 3.1.3 Analyzing bias and variance Avoidable bias If training error is far from human error (bayes error), focus on bias (avoidable bias). If training error is close to human error but far from dev error, focus on variance. Consider human-level error as a proxy for Bayes error (since is not too far from human-level error to Bayes error). Understanding human-level performance: Based on purpose defined which is the human-level error want to use. If human can perform really well, we can use human-level error as proxy for Bayes error. Surpassing human-level performance Not natural perception Lots of data Improving your model performance The two fundamental assumptions of supervised learning You can fit the training set pretty well. (Avoidable bias) The training set performance generalizes pretty well to the dev&#x2F;test set. (Variance) Reducing (avoidable) bias and variance Avoidable bias: Train bigger model. Train longer&#x2F;better optimization, algorithms (momentum, RMSprop, Adam). NN architecture&#x2F;hyperparameters search (RNN, CNN). Variance: More data. Regularization (L2, dropout, data augmentation). NN architecture&#x2F;hyperparameters search. 3.2 ML Strategy (2)3.2.1 Error analysis Carrying out error analysis Error analysis (count mislabel, minus from the error rate get the ceiling of error rate) Get ~100 mislabeled dev set examples. Count up how many are dogs. Evaluate multiple ideas in parallel (ideas for cat detection) Fix pictures of dogs being recognized as cats Fix great cats （lions, panthers, etc.） being misrecognized Improve performance on blurry images Check the details of mislabeled images (only few minutes&#x2F;hours) Cleaning up incorrectly labeled data DL algorithms are quite robust to random errors in the training set. (random error will not affect the algorithm too much) DL algorithms are less robust to systematic errors. When a high fraction of mistake is due to incorrectly label, should spend time to fix it. Correcting incorrect dev&#x2F;test set examples Apply same process to your dev and test sets to make sure they continue to come from the same distribution. Consider examining examples your algorithm got right as well as ones it got wrong. Train and dev&#x2F;test data may now come from slightly different distributions. Build your first system quickly, then iterate Set up dev&#x2F;test set and metric Build initial system quickly Use Bias&#x2F;Variance analysis &amp; Error analysis to prioritize next steps. Training and testing on different distributions 200,000 from high quality webpages, 10,000 from low quality mobile app (but we care about this). Shuffle before use those data. (not a good option, will cause the influence of what we care small.) Use mobile app as dev&#x2F;test set, and just really small part of training set from app. (This we will make our target to what we want.) Maybe 50% in training, 25% in dev, and 25% test. 3.2.2 Mismatched training and dev&#x2F;test set Training-dev set: Same distribution as training set, but not used for training. Training error - Training-dev error - Dev error Human level - traning set error: avoidable bias Traning error - Training-dev error: Variance Training-dev error - Dev error: Data mismatch Dev error - Test error: degree of overfitting to dev set. Addressing data mismatch Carry out manual error analysis to try to understand difference between training and dev&#x2F;test sets. Make training data more similar; or collect more data similar to dev&#x2F;test sets. Artificial data synthesis: Possible issue (overfitting): Original data is 10000, only have the noise of 1, maybe overfit to this 1. Transfer learning Pre-training&#x2F;Fine-tune From relatively large data to relatively small data. But if the target data is too small may not be suitable for transfer learning. (Depend on the outcome we want, it would be valuable to have more data) When makes sense (transfer from A-&gt; B): Task A and B have the same input x. You have a lot more data for Task A than Task B (want this one). Low level features from A could be helpful for learning B. 3.2.3 Learning from multiple tasks Loss function for multiple tasks Loss: y^(4,1)(i)=1m∑i=1m∑_j=14L(y^_j(i),yj(i))\\hat{y}_{(4,1)}^{(i)}=\\dfrac{1}{m}\\sum\\limits_{i=1}^m\\sum\\limits\\_{j=1}^4L(\\hat{y}\\_j^{(i)},y_j^{(i)}) Sum only over valid of j with 0&#x2F;1 label. (some of them may only labeled some feature) Unlike softmax regression: One image can have multiple labels When multi-task learning makes sense Training on a set of tasks that could benefit from having shared lower-level features. Usually: Amount of data you have for each task is quite similar. Can train a big enough neural network to do well on all the tasks. 3.2.4 End-to-end deep learning End-to-end needs lots of data to work well. Breaking small data scenario into different deep learning will be better results. Wether to use end-to-end learning Pros: Let the data speak. Less hand-designing of components needed. Cons: May need large amount of data Excludes potentially useful hand-designed components. Key question: Do you have sufficient data to learn a function of the complexity needed to map x to y？ Use DL to learn individual components. Carefully choose X-&gt;Y mappping depending on what tasks you can got data for. IV. Convolutional Neural Networks4.1 Foundations of Convolutional Neural Networks4.1.1 Convolutional operatin Vertical Edge Detection Used to identify vertical edges in images, which is a crucial step in image analysis and understanding. A small matrix, typically 3x3 or 5x5, is used as a convolution kernel to detect vertical edges. The kernel slides over the image, moving one pixel at a time. At each position, element-wise multiplication is performed between the kernel and the overlapping image area, followed by a sum to produce an output feature map. High values in the output feature map indicate the presence of a vertical edge at that location. $\\begin{bmatrix}1 &amp; 0 &amp; -1 \\1 &amp; 0 &amp; -1 \\1 &amp; 0 &amp; -1\\end{bmatrix}$ Based on this matrix example below, it will detect lighter on the left and darker on the right. Horizontal Edge Detection Brighter on the top and darker on the bottom $\\begin{bmatrix}1 &amp; 1 &amp; 1 \\0 &amp; 0 &amp; 0 \\-1 &amp; -1 &amp; -1\\end{bmatrix}$ TBC Other Common Filters Sobel filter $\\begin{bmatrix}1 &amp; 0 &amp; -1 \\2 &amp; 0 &amp; -2 \\1 &amp; 0 &amp; -1\\end{bmatrix}$ Scharr filter $\\begin{bmatrix}3 &amp; 0 &amp; -3 \\10 &amp; 0 &amp; -10 \\3 &amp; 0 &amp; -3\\end{bmatrix}$ Padding nxn * fxf &#x3D; n-f+1 x n-f+1 Problems of convolution: Shrinking output Through away information from edge. Add a padding(p) of 0 n+2pxn+2p * fxf &#x3D; n+2p-f+1 x n+2p-f+1 Valid convolutions: No padding Same convolutions: Pad so that output size is the same as the input size. (padding is f−12\\dfrac{f-1}{2}) f is usually odd. Strided convolution Stepping s steps instead of 1. n+2p−fs+1\\dfrac{n+2p-f}{s}+1 x n+2p−fs+1\\dfrac{n+2p-f}{s}+1 (If not integer, bound down to the nearest integer.) cross-correlation is the real name of convolution in DL. Convolution over volume Set the filter into the same volume as the input matrix. (e.g. RGB image with 3x3x3 filter) If only look at an individual channel, just make other channel with all 0. If consider vertical and horitental seperately, each output 4x4, the final could stack together get a 4x4x2 volume. n×n×nc\\*f×f×ncn\\times n\\times n_c \\* f\\times f \\times n_c -> n−f+1×n−f+1×nc′n-f+1 \\times n-f+1 \\times n_c^{&#x27;} (\\# of filters) One layer of a CNN Each output add a bias and apply non-learner to it. ReLU(Output+b) –&gt; Consider stack all outputs after this as volume as the a in a&#x3D;g(z) Consider output as the same as the w in z&#x3D;wa+b. Number of parameters in one layer: If you have 10 filters that are 3x3x 3 in one layer of a neural network, how many parameters does that layer have？(Consider 3x3x3 + bias, it will be 280 parameters) Summary of notation (If layer 1 is a convolution layer) f[l]=f^{[l]}= filter size (3x3 filter will be f=3) p[l]=p^{[l]}= padding s[l]=s^{[l]}= stride Input: nH[l−1]×nW[l−1]×nC[l−1]n_H^{[l-1]}\\times n_W^{[l-1]}\\times n_C^{[l-1]} Output: nH[l]×nW[l]×nC[l]n_H^{[l]}\\times n_W^{[l]}\\times n_C^{[l]} n∗H/W[l]=n∗H/W[l−1]+2p[l]−f[l]x[l]+1n*{H/W}^{[l]}=\\dfrac{n*{H/W}^{[l-1]}+2p^{[l]}-f^{[l]}}{x^{[l]}}+1 Round down to nearest integer Each filter is: f[l]×f[l]×nC[l−1]f^{[l]}\\times f^{[l]}\\times n_C^{[l-1]} Activations: a[l]a^{[l]} -&gt; nH[l]×nW[l]×nC[l]n_H^{[l]}\\times n_W^{[l]}\\times n_C^{[l]} Batch gradient descent A[l]A^{[l]} -&gt; m×nH[l]×nW[l]×nC[l]m\\times n_H^{[l]}\\times n_W^{[l]}\\times n_C^{[l]} Weights: f[l]×f[l]×nC[l−1]×nC[l]f^{[l]}\\times f^{[l]}\\times n_C^{[l-1]}\\times n_C^{[l]} (The last quantity is # filters in layer l) bias: nC[l]n_C^{[l]} - (1,1,1,nC[l])(1,1,1,n_C^{[l]}) A simple example ConvNet Get the final output(7x7x40) and take it as a 1960 vector pass through logistic&#x2F;softmax to get out actual final value. Types of layer in a convolutional network Convolution (CONV) Pooling (POOL) Fully connected (FC) 4.1.2 Pooling layers No parameters to learn. Max pooling Consider input is 4x4 matrix, output a 2x2 matrix. f(filter) &#x3D; 2, s(stride) &#x3D; 2. Just max each 2x2 in the input and put it into one cell in the output matrix. Hyperparameters: f(filter) and s(stride). Average pooling Instead of take the maxium, take the average. Input: nH×nW×nCn_H\\times n_W\\times n_C Output: nH−fs+1×nW−fs+1×nC\\dfrac{n_H-f}{s}+1\\times \\dfrac{n_W-f}{s}+1\\times n_C Down to the nearest integer. 4.1.3 CNN example Fully Connected layer After several convolutional and pooling layers, the high-level reasoning in the neural network is done via FC layers. The output of the last pooling or convolutional layer, which is typically a multi-dimensional array, is flattened into a single vector of values. This vector is then fed into one or more FC layers. Role: Integration of Learned Features: FC layers combine all the features learned by previous convolutional layers across the entire image. While convolutional layers are good at identifying features in local areas of the input image, FC layers help in learning global patterns in the data. Dimensionality Reduction: FC layers can be seen as a form of dimensionality reduction, where the high-level, spatially hierarchical features extracted by the convolutional layers are compacted into a form where predictions can be made. Classification or Regression: In classification tasks, the final FC layer typically has as many neurons as the number of classes, with a softmax activation function being applied to the output. For regression tasks, the final FC layer’s output size and activation function are adjusted according to the specific requirements of the task. Operation is similar to neurons in a standard neural network. Example Why convolutions? Parameter sharing: A feature detector (such as a vertical edge detector) that’s useful in one part of the image is probably useful in another part of the image. Sparsity of connections: In each layer, each output value depends only on a small number of inputs. Training set (x(1),y(1))...(x(m),y(m))(x^{(1)},y^{(1)})...(x^{(m)},y^{(m)}) Cost J=1m∑_i=1mL(y^(i),y(i))J=\\dfrac{1}{m}\\sum\\limits\\_{i=1}^{m}L(\\hat{y}^{(i)},y^{(i)}) 4.2 Deep Convolutional Models: Case Studies4.2.1 Case studies (LeNet-5, AlexNet, VGG, ResNets) Red notations in the image below are what the network original designed but not suitable for nowadays. 4.2.1.1 LeNet-5 Pioneer in CNNs: One of the earliest Convolutional Neural Networks, primarily used for digit recognition tasks. Architecture: Consists of 7 layers (excluding input). Includes convolutional layers, average pooling layers, and fully connected layers. Activation Functions: Uses sigmoid and tanh activation functions in different layers. (Not using nowadays) Local Receptive Fields: Utilizes 5x5 convolution filters to capture spatial features. Subsampling Layers: Employs average pooling for subsampling. (Using max pool nowadays) 4.2.1.2 AlexNet Multiple GPUs in the paper is outdated for today. LRN is not useful after lots of other researches. Deeper Architecture: Contains 8 learned layers, 5 convolutional layers followed by 3 fully connected layers. ReLU Activation: One of the first CNNs to use ReLU (Rectified Linear Unit) activation function for faster training. Overlapping Pooling: Uses overlapping max pooling, reducing the network’s size and overfitting. Data Augmentation and Dropout: Employs data augmentation and dropout techniques for better generalization. 4.2.1.3 VGG-16 Simplicity and Depth: Known for its simplicity and depth, with 16 learned layers. Uniform Architecture: Features a very uniform architecture, using 3x3 convolution filters with stride and pad of 1, max pooling, and fully connected layers. Convolutional Layers: Stacks convolutional layers (2-4 layers) before each max pooling layer. Large Number of Parameters: Has a high number of parameters (around 138 million), making it computationally intensive. Transfer Learning: Proved to be an excellent model for transfer learning due to its performance and simplicity. 4.2.1.4 ResNets Residual block Main Path: a[l]a^{[l]} –&gt; Linear –&gt; ReLU –&gt; a[l+1]a^{[l+1]} –&gt; Linear –&gt; ReLU –&gt; a[l+2]a^{[l+2]} z[l+1]=W[l+1]a[l]+b[l+1]z^{[l+1]}=W^{[l+1]}a^{[l]}+b^{[l+1]} a[l+1]=g(z[l+1])a^{[l+1]}=g(z^{[l+1]}) z[l+2]=W[l+2]a[l+1]+b[l+2]z^{[l+2]}=W^{[l+2]}a^{[l+1]}+b^{[l+2]} a[l+2]=g(z[l+2])a^{[l+2]}=g(z^{[l+2]}) Short Cut &#x2F; Skip Connection: a[l]a^{[l]} –&gt; ReLU –&gt; a[l+2]a^{[l+2]} a[l+2]=g(z[l+2]+a[l])a^{[l+2]}=g(z^{[l+2]}+a^{[l]}) In normal plain network, the trainning error with increasing number of layers in theory will continuesly decrease. But in reality it will decrease but increase after a sweet point. What ResNet performs is decreasing training error with numbers of layers increase and the training error not increasing again. Why do residual networks work? Residual networks introduce a shortcut or skip connection that allows the network to learn identity functions effectively. This is crucial for training very deep networks by avoiding the vanishing gradient problem. In a residual block: XX -> BigNN -> a[l]a^{[l]} -> Residual block -> a[l+2]a^{[l+2]} Input XX is passed through a standard neural network (BigNN) to obtain a[l]a^{[l]}, and then it goes through the residual block to produce a[l+2]a^{[l+2]}. The formulation of a residual block can be represented as:a[l+2]=g(z[l+2]+a[l])=g(w[l+2]a[l+1]+b[l+2]+a[l])a^{[l+2]} = g(z^{[l+2]} + a^{[l]}) = g(w^{[l+2]} a^{[l+1]} + b^{[l+2]} + a^{[l]}) Here, gg is the activation function. z[l+2]z^{[l+2]} is the output of the layer just before the activation function. w[l+2]w^{[l+2]} and b[l+2]b^{[l+2]} are the weight and bias of the layer, respectively. If w[l+2]=0w^{[l+2]} = 0 and b[l+2]=0b^{[l+2]} = 0, then a[l+2]=g(a[l])=a[l]a^{[l+2]} = g(a^{[l]}) = a^{[l]}, effectively allowing the network to learn the identity function. In cases where the dimensions of a[l+2]a^{[l+2]} and a[l]a^{[l]} differ (e.g., a[l]∈R128a^{[l]} \\in \\mathbb{R}^{128} and a[l+2]∈R256a^{[l+2]} \\in \\mathbb{R}^{256}), a linear transformation wsw_s (e.g., ws∈R256×128w_s \\in \\mathbb{R}^{256 \\times 128}) is applied to a[l]a^{[l]} to match the dimensions. This architecture enables training deeper models without performance degradation, which was a significant challenge in deep learning before the development of ResNet. Understand through backdrop(personal notes not from the class content) Consider input as x, the residual block calculation as F(x), identity mapping just drag the x and add it to the residual block’s calculation which makes the final value y=F(x)+xy=F(x)+x Backprop for this will be as follow Gradient of the Residual Blokc’s Output: ∂y∂w\\dfrac{\\partial y}{\\partial w} This represents the gradient of the output yy with respect to the weights ww. By chain rule: ∂y∂w=∂y∂F(x)∂F(x)∂x∂x∂w+∂y∂x∂x∂w\\dfrac{\\partial y}{\\partial w} = \\dfrac{\\partial y}{\\partial F(x)}\\dfrac{\\partial F(x)}{\\partial x}\\dfrac{\\partial x}{\\partial w} + \\dfrac{\\partial y}{\\partial x}\\dfrac{\\partial x}{\\partial w} Since y=F(x)+xy=F(x)+x, ∂y∂F(x)\\dfrac{\\partial y}{\\partial F(x)} and ∂y∂x\\dfrac{\\partial y}{\\partial x} should be 1 So the formula become ∂y∂w=∂F(x)∂x∂x∂w+∂x∂w\\dfrac{\\partial y}{\\partial w} = \\dfrac{\\partial F(x)}{\\partial x}\\dfrac{\\partial x}{\\partial w} + \\dfrac{\\partial x}{\\partial w} Compare to without the identity mapping xx added. ∂y∂w=∂F(x)∂x∂x∂w\\dfrac{\\partial y}{\\partial w} = \\dfrac{\\partial F(x)}{\\partial x}\\dfrac{\\partial x}{\\partial w}, there is a ∂x∂w\\dfrac{\\partial x}{\\partial w} less. Add this xx to F(x)F(x) makes the network will not get worse results compare to before. 4.2.2 Network in Network and 1 X 1 convolutions 1x1 convolutions Functionality of 1x1 Convolutions: A 1x1 convolution, despite its simplicity, acts as a fully connected layer applied to each pixel separately across depth. It’s effectively used for channel-wise interactions and dimensionality reduction. Increasing Network Depth: 1x1 convolutions can increase the depth of the network without a significant increase in computational complexity. Dimensionality Reduction: They are often used for reducing the number of channels (depth) before applying expensive 3x3 or 5x5 convolutions, thus reducing the computational cost. Feature Re-calibration: 1x1 convolutions can recalibrate the feature maps channel-wise, enhancing the representational power of the network. Using 1x1 convolutions: Reduce dimension: Consider a 28x28x192 input with CONV 1x1 with 32 filters, the output will be 28x28x32. 4.2.3 Inception network Motivation for inception network Input 28x28x192 Use 1x1x192 with 64 filters, output 28x28x64 Use same dimension 3x3x192, output 28x28x128 Use same dimension 5x5x192, output 28x28x32 use same dimension and s&#x3D;1 Max-Pool, output 28x28x32. Final output 28x28x256. The problem of computational cost (Consider 5x5x192) 5x5x192x28x28x32 is really big, 120M. Bottleneck layer (Using 1x1 convolution): shrink 28x28x192 –&gt; CONV, 1x1, 16, 1x1x192 –&gt; 28x28x16 (Bottleneck layer) –&gt; CONV 5x5, 32, 5x5x16 –&gt; 28x28x32 In total only 28x28x16+28x28x32x5x5x16&#x3D;12.4M Inception moule The softmax in the itermediate position is used for regularization which is used avoid overfitting. 4.2.4 MobileNet Depthwise Separable Convolution Depthwise Convolution Computational cost &#x3D; #filter params x #filter positions x #of filters Ppointwise Convolution Computational cost &#x3D; #filter params x #filter positions x # of filters n∗c∗n∗c∗filtersn*c * n*c * filters Cost of depthwise seprable convolution &#x2F; normal convolution 1nc+1f2\\dfrac{1}{n_c} + \\dfrac{1}{f^2} MobileNet v2 Bottleneck Residual Connection Expansion Depthwise Pointwise (Projection) Similar computational cost as v1 MobileNet V2 improves upon V1 by introducing an inverted residual structure with linear bottlenecks, which enhances the efficiency of feature extraction and information flow through the network. This architectural advancement allows V2 to achieve better performance than V1, despite having similar computational costs. Essentially, V2 optimizes the way features are processed and combined, providing more effective and complex feature representation within the same computational budget as V1. 4.2.5 EfficientNet EfficientNet is a series of deep learning models known for high efficiency and accuracy in image classification tasks. Compound Scaling: It introduces a novel compound scaling method, scaling network depth, width, and resolution uniformly with a set of fixed coefficients. High Efficiency and Accuracy: EfficientNets provide state-of-the-art accuracy for image classification while being more computationally efficient compared to other models. 4.2.6 Inception network Transfer Learning Small training set: Freeze all hidden layers (save to disk), only train the softmax unit. Big training set: Freeze less hidden layers, train some of the hidden layers (or use new hidden units), and also own softmax unit. Lots of data: Use the already trained weights and bias as initalization, re-train based on it, as well as the softmax unit. Data augmentation Common augmentation method: Mirroring, Random Cropping, (Rotation, Shearing, Local warping, …) Color shifting: add&#x2F;minus from RGB. Advanced: PCA &#x2F; PCA color augmentation. Implementing distortions during training: One CPU thread doing augmentation, and other threads or GPU doing the training at same time. State of CV Data needed (little data to lots of data): Object detection &lt; Image recognition &lt; Speach recognition Lots of data - Simpler algotithms (Less hand-engineering) Little data - more hand-engineering (“hacks”) - Transfer learning Two sources of knowledge Labeled data Hand engineered features&#x2F;network architecture&#x2F;other components Tips for doing well on benchmarks&#x2F;winning competitions Ensembling: Train several networks independently and average their outputs (y^\\hat{y}) 1-2% better. (3-15 networks) Multi-crop at test time: Run classifier on multiple versions of test images and average results. (10-crop: center, four corner, also on mirror image the same 5 crops) Use open source code Use architectures of networks published in the literature. Use open source implementations if possible. Use pretrained models and fine-tune on your dataset. 4.3 Object Detection4.3.1 Object localization Want to detect 4 class: 1-pedestrian, 2-car, 3-mtorcycle, 4-background. Defining the target label y: Need to out put bx,by,bh,bwb_x, b_y, b_h, b_w, class label (1-4). (In total 9 elements in the output vector). y=[pc,bx,by,bh,bw,c1,c2,c3]y=[p_c, b_x, b_y, b_h, b_w, c_1, c_2, c_3] There is an object y=[1,bx,by,bh,bw,c1,c2,c3]y=[1, b_x, b_y, b_h, b_w, c_1, c_2, c_3] No object y=[0,?,?,?,?,?,?,?]y=[0, ?, ?, ?, ?, ?, ?, ?] Don’t care for all of other Lost function: L(y^,y)=(y1^−y1)2+(y2^−y2)2+...+(y8^−y8)2L(\\hat{y}, y)=(\\hat{y_1} - y_1)^2 + (\\hat{y_2} - y_2)^2 + ... + (\\hat{y_8} - y_8)^2 if y1=1y_1=1 L(y^,y)=(y1^−y1)2L(\\hat{y}, y)=(\\hat{y_1} - y_1)^2 if y1=0y_1=0 4.3.2 Landmark detection Annotate key positions (points-xy coordinate) as landmarks. 4.3.3 Object detection Object detection Starts with closely crops images. A window sliding from the top left to bottom right, once and once. If not find increase the window’s size and redo the sliding. Run each individual image to the convnet. Turning FC layer into convolutional layers Instead directly to FC, use conv filter. Convolution implementation of sliding windows Instead of do 4 times 14x14x3, new conv fc share the computation, directly using the 2x2x4. Output accurate bounding boxes YOLO algorithm Find the medium point of target and working into the boundary box that contains that point. Intersection over union (IoU) Use to check accuracy. Size of intersection &#x2F; size of reunion (normally “Correct” if loU ≥\\geq 0.5) Non-max suppression Leave the maximum accuracy one, supprese all with high IoU. Anchor Boxes Predefine anchor boxes, associate ojects with anchor boxes. If objects more than assigned anchor boxes, not works. Not same shape, not works. Training set y is 3x3x2x8 (which is # of grids x # of anchors x # classes(5(pc.bx,by,bh,bwp_c. b_x, b_y, b_h, b_w) + classes)) Regision Proposals R-CNN: Propose regions. Classify proposed regions one at a time. Output label + bounding box. Fast R-CNN: Propose regions. Use convolution implementation of sliding windows to classify all the proposed regions. Faster R-CNN: Use convolutional network to propose regions. Semantic Segmentation with U-Net Per-pixel class labels Deep Learning for Semantic Segmentation Transpose Convolution Increase the image size. U-Net Architecture Skip Connections: Left one get more details in color or anything like that. Right one is more spatial information to figure out where is the object really is. 4.4 Special Applications: Face Recognition &amp; Neural Style Transfer4.4.1 Face recognition Face verification vs. face recognition verification vs recognition —- 1:1 vs 1:K Verification Input image, name&#x2F;ID. Output whether the input image is that of the claimed person. Recognition Has a database of K persons Get an input image Output ID if the image is any of the K persons (or “not recognized”) One-shot learning Learning from one example to recognize the person again. Learning a “similarity” function d(img1, img2) &#x3D; degree of difference between images If d(img1, img2) ≤τ\\le \\tau “same” \\textgreater τ\\textgreater \\space \\tau “Different” Siamese network Input two differnet images into two CNN and ge the result of them. Such as input x(1),x(2)x^{(1)}, x^{(2)} seperately into two differnt CNN, and the output will be the encoding of each of them f(x(1)),f(x(2))f(x^{(1)}), f(x^{(2)}) Then compare the distance between them d(x(1),x(2))=∣∣f(x(1))−f(x(2))∣∣_22d(x^{(1)}, x^{(2)}) = ||f(x^{(1)}) - f(x^{(2)})||\\_2^2 Parameters of NN define an encoding f(x(i))f(x^{(i)}) Learn parameters so that: If x(i),x(j)x^{(i)}, x^{(j)} are the smae person, ∣∣f(x(i))−f(x(j))∣∣2||f(x^{(i)}) - f(x^{(j)})||^2 is small. If x(i),x(j)x^{(i)}, x^{(j)} are the different person, ∣∣f(x(i))−f(x(j))∣∣2||f(x^{(i)}) - f(x^{(j)})||^2 is large.. Triplet Loss Learning objective: (Anchor, Positive), (Anchor, Negative) Want: ∣∣f(A)−f(P)∣∣2+α≤∣∣f(A)−f(N)∣∣2||f(A) - f(P)||^2 + \\alpha \\le ||f(A) - f(N)||^2 α\\alpha is the margin (similar to SVM) ∣∣f(A)−f(P)∣∣2−∣∣f(A)−f(N)∣∣2+α≤0||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + \\alpha \\le 0 Loss function Given 3 images A, P, N: L(A,P,N)=max(∣∣f(A)−f(P)∣∣2−∣∣f(A)−f(N)∣∣2+α,0)L(A, P, N) = max(||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + \\alpha, 0) J=∑_i=1mL(A(i),P(i),N(i))J = \\sum\\limits\\_{i=1}^m L(A^{(i)},P^{(i)},N^{(i)}) If have a training set of 10K pictures of 1k persons. Put those 10K into triplet A, P, N, then put into the loss function. Choosing the triplets A, P, N During training, if A, P, N are chosen randomly, d(A,P)+α≤d(A,N)d(A,P) +\\alpha \\le d(A, N) is easily satisfied. Choose triplets that’re “hard” to train on. (such as choose d(A,P)≈d(A,N)d(A,P) \\approx d(A,N)) Training set using triplet loss to make J smaller. And make distance of d for same person small and different large. Face Verification and Binary Classification y^=σ(∑_k=1128wk∣f(x(i))_k−f(x(j))_k∣+b)\\hat{y} = \\sigma (\\sum\\limits\\_{k=1}^{128}w_k|f(x^{(i)})\\_k-f(x^{(j)})\\_k| + b) Only store the f(x(j))f(x^{(j)}) as pre-compute, save storage and computational resources. Face verification supervised learning. 4.4.2 Neural style transfer What is it? Cost Function J(G)=αJ∗content(C,G)+βJ∗Style(S,G)J(G) = \\alpha J*{content}(C, G) + \\beta J*{Style}(S, G) Find the generated image G Initiate G randomly G: 100x100x3 Use gradient descent to minimize J(G) G:=G−∂∂GJ(G)G:=G-\\dfrac{\\partial}{\\partial G}J(G) V. Sequence Models5.1 Recurrent Neural Networks5.1.1 RNN model5.1.2 Backpropagation through time5.1.3 Different types of RNNs5.2 Natural Language Processing &amp; Word Embeddings5.2.1 Word Representation5.2.2 Embedding matrix5.2.3 Word embeddings in TensorFlow5.3 Sequence Models &amp; Attention Mechanism5.3.1 Sequence to sequence model5.3.2 Beam search5.3.3 Attention model","categories":[{"name":"软件","slug":"软件","permalink":"https://www.zl-asica.com/categories/%E8%BD%AF%E4%BB%B6/"}],"tags":[{"name":"DL","slug":"DL","permalink":"https://www.zl-asica.com/tags/DL/"}]},{"title":"Python简单加密器-反转字符串","slug":"python-cipher-reverse-string","date":"2023-01-18T06:55:00.000Z","updated":"2024-12-02T16:04:42.808Z","comments":true,"path":"2023/python-cipher-reverse-string/","permalink":"https://www.zl-asica.com/2023/python-cipher-reverse-string/","excerpt":"1. 什么是反转字符串反转字符串其实就是把一个string类型的字符串变量进行从头到尾的一个反转，比如’hello!’经过反转后我们就可以得到’!olleh’。如果只想看代码可以直接翻到!2.5看代码段即可","text":"1. 什么是反转字符串反转字符串其实就是把一个string类型的字符串变量进行从头到尾的一个反转，比如’hello!’经过反转后我们就可以得到’!olleh’。如果只想看代码可以直接翻到!2.5看代码段即可 2. 如何实现反转字符串实际上我们在实现这个加密器的时候首先要考虑的是如何利用字符串本身的一些特性去实现这个功能，从而使用最简单的代码达到最佳的效果。 2.1 字符串的基本特性我们的String本质上就是一个类似列表(List)的存在。我们看下面的图，假设我们有一个String类型的变量为test = &quot;screen&quot;，我们可以通过下图的方式对它进行索引。每一个单独的字母就是这个”列表”里的一个单独的元素。我们可以使用如test[0]的方式来获取元素’s’，也可以通过如test[0:2]的方式来获取元素’sc’，此处[0:2]对应的是一个[0,2)左闭右开的区间。那大家可能就想到了列表是可以反转(reverse)的，那么我们这个所谓的’列表’可不可以做同样的操作呢？答案是不行，String变量并没有reverse函数，所以我们就需要先讲字符串转换为真正的列表才可以进行反转。 2.2 字符串转列表字符串转换为列表其实本质上非常简单。一般来说我们把数据转换为列表对象有两种方式，第一种方式是通过split()函数进行String对象的切分，第二种方式是通过list()函数直接将String对象强制类型转换为列表(List)类型的变量。在这里我们直接使用list()函数将String变量转换为列表对象即可。 12test = &#x27;screen&#x27;test_list = list(test) # test_list = [&#x27;s&#x27;, &#x27;c&#x27;, &#x27;r&#x27;, &#x27;e&#x27;, &#x27;e&#x27;, &#x27;n&#x27;] 2.3 列表的反转在我们将String转换为列表后，我们就要开始反转了。列表的反转非常地简单，直接对list类型的变量使用.reverse()函数进行列表的反转即可。 1test_list.reverse() # test_list = [&#x27;n&#x27;, &#x27;e&#x27;, &#x27;e&#x27;, &#x27;r&#x27;, &#x27;c&#x27;, &#x27;s&#x27;] 2.4 列表转字符串反转完成后我们就需要把列表重新转换为字符串String类型的对象了。将list转换为String实际上非常简单，只需要使用&#39;&#39;.join()在引号中间就是用什么元素把你的list中的每一个元素连接起来形成一个String。举个?，如果你在引号间用的是&#39;,&#39;，那么你最后列表就会通过,连接起来。 1reverse_test = &#x27;&#x27;.join(test_list) # reverse_test = &#x27;neercs&#x27; 2.5 反转字符串的最终实现这里我们就把所有的代码合起来，假设我们有一个变量cipher = &quot;hello!&quot;，我们要得到它反转后的String字符串。 12345678def cipher_reverse_string(cipher: str) -&gt; str: cipher_list = list(cipher) cipher_list.reverse() result = &#x27;&#x27;.join(cipher_list) return resultif __name__ == &#x27;__main__&#x27;: print(cipher_reverse_string(&quot;hello!&quot;)) 3. 总结我们这里使用了Python中String和List的一些小特性和方法实现了一个非常简单的加密器(完全没加密的加密器)，使用到了String和List的互相转换、list的反转。本质上来说是一个非常简单的小程序，只需要理解其中的逻辑即可。","categories":[{"name":"代码","slug":"代码","permalink":"https://www.zl-asica.com/categories/%E4%BB%A3%E7%A0%81/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://www.zl-asica.com/tags/Python/"}]},{"title":"仅使用numpy实现混淆矩阵计算（不使用sklearn）","slug":"numpy-only-confusion-matrix","date":"2023-01-16T05:28:00.000Z","updated":"2024-12-02T16:04:42.808Z","comments":true,"path":"2023/numpy-only-confusion-matrix/","permalink":"https://www.zl-asica.com/2023/numpy-only-confusion-matrix/","excerpt":"1 什么是混淆矩阵(confusion matrix)当我们拿到数据后，经过数据清洗、预处理和整理之后，我们做的第一步是利用这些数据训练出一个模型。我们究竟如何衡量模型的准确度和有效性？性能和效率又如何？这就是混淆矩阵被用来解决的问题。混淆矩阵是机器学习分类的性能的一种方式。","text":"1 什么是混淆矩阵(confusion matrix)当我们拿到数据后，经过数据清洗、预处理和整理之后，我们做的第一步是利用这些数据训练出一个模型。我们究竟如何衡量模型的准确度和有效性？性能和效率又如何？这就是混淆矩阵被用来解决的问题。混淆矩阵是机器学习分类的性能的一种方式。 2 混淆矩阵有什么用我们可以使用这个矩阵直观地看到每个数据的真实值和我们模型的预测值的关系。 3 如何实现3.1 使用sklearn.metrics的confusion_matrix本方法适用了sklearn.metrics下的confusion_matrix函数直接进行生成 12345from sklearn.metrics import confusion_matrixy_true = [2, 0, 2, 2, 0, 1]y_pred = [0, 0, 2, 2, 0, 2]confusion_matrix(y_true, y_pred) 3.2 仅使用numpy本方法仅使用了numpy，没有使用任何其他的库，包括sklearn在内实现混淆矩阵的计算。 123456789101112131415import numpy as npdef compute_confusion_matrix(true, pred): &#x27;&#x27;&#x27;输入true和pred 其输出结果与以下内容相同（计算时间也相似）。 &quot;from sklearn.metrics import confusion_matrix&quot; 然而，这个函数避免了对sklearn的依赖。&#x27;&#x27;&#x27; K = len(np.unique(true)) # features的数量 result = np.zeros((K, K)) for i in range(len(true)): result[true[i]][pred[i]] += 1 return result 来源：https://stackoverflow.com/a/48087308/20626353","categories":[{"name":"软件","slug":"软件","permalink":"https://www.zl-asica.com/categories/%E8%BD%AF%E4%BB%B6/"}],"tags":[{"name":"ML","slug":"ML","permalink":"https://www.zl-asica.com/tags/ML/"}]},{"title":"C++进行特定位数的四舍五入/向上取整/向下取整/截断","slug":"cpp-round","date":"2022-03-25T23:26:00.000Z","updated":"2024-12-02T16:04:42.808Z","comments":true,"path":"2022/cpp-round/","permalink":"https://www.zl-asica.com/2022/cpp-round/","excerpt":"1.C++的四舍五入&#x2F;向上取整&#x2F;向下取整&#x2F;截断我们在C++中有时会需要使用到数学运算，同样的可能会使用到标题提到的几种方式进行运算。实际上C++有一个库可以直接实现这些方法操作。","text":"1.C++的四舍五入&#x2F;向上取整&#x2F;向下取整&#x2F;截断我们在C++中有时会需要使用到数学运算，同样的可能会使用到标题提到的几种方式进行运算。实际上C++有一个库可以直接实现这些方法操作。 123456789101112131415161718192021222324#include &lt;cmath&gt;int main()&#123; double a = 43.5555; // 四舍五入 a = std::round(a); std::cout &lt;&lt; a &lt;&lt; std::endl; // 44.0000 // 向下取整 a = std::floor(a); std::cout &lt;&lt; a &lt;&lt; std::endl; // 43.0000 // 向上取整 a = std::ceil(a); std::cout &lt;&lt; a &lt;&lt; std::endl; // 44.0000 // 截断 a = std::trunc(a); std::cout &lt;&lt; a &lt;&lt; std::endl; // 43.0000 return 0;&#125; 那么这就是最简单的四种算法在C++中的使用了。 2.C++中获取特定位数的方法那么有时候我们进行这样的数学运算后，目的就是为了得到一个更方便看的数，那当然要取特定位数的小数了。写法规则如下(以四舍五入方法std::round做演示)。 1a = std::round(a * 保留到几分位) / 保留到几分位; // 保留到十分位 = 保留两位小数 我们来一个保存两位小数(十分位)的实例来看一下。 1234567891011#include &lt;iostream&gt;#include &lt;cmath&gt;int main()&#123; double a = 43.5555; a = std::round(a * 10) / 10; std::cout &lt;&lt; a &lt;&lt; std::endl; return 0;&#125; 那么这样就可以非常轻松的在C++中获取到我们需要的小数位数了。 3.C++中对定位数的double进行输出的方法我们上面说到了如何获取到特定的位数，但是这样获取到的还是double值，如果想要和std::string一起操作是会出现问题的。如果我们想要把这个double和其他的string放到一起的话，我们就需要按照如下代码进行操作。 123456789101112131415#include &lt;iostream&gt;#include &lt;cmath&gt;#include &lt;string&gt;int main()&#123; double a = 43.5555; a = std::round(a * 10) / 10; // 先转换为std::string类型 std::string tempre = std::to_string(a); // 对std::string类型进行10位小数的格式化（下面语句会输出10位小数） std::cout &lt;&lt; tempre.substr(0, tempre.find(&quot;.&quot;) + 2) &lt;&lt; std::endl; return 0;&#125; 这就是本文的全部内容了。","categories":[{"name":"代码","slug":"代码","permalink":"https://www.zl-asica.com/categories/%E4%BB%A3%E7%A0%81/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://www.zl-asica.com/tags/C/"}]},{"title":"ZL Asica的Adobe Premier Pro 2022基础教程思维导图(mindmap)","slug":"zla-pr-2022-intro-level","date":"2021-12-31T04:12:00.000Z","updated":"2024-12-02T16:04:42.808Z","comments":true,"path":"2021/zla-pr-2022-intro-level/","permalink":"https://www.zl-asica.com/2021/zla-pr-2022-intro-level/","excerpt":"有需要的吗！！！我终于把PR教程做完了，一共10期，从今天10:30开始到9号10:30每天一期，都有CC字幕可以打开方便看点击订阅合集第一时间收到推送因为我就是从0基础小白开始自学的PR，我按照我的学习路径制作的一系列教程可能会比用了很多年Pr的大佬制作的更适合0基础小白，大佬们请手下留情","text":"有需要的吗！！！我终于把PR教程做完了，一共10期，从今天10:30开始到9号10:30每天一期，都有CC字幕可以打开方便看点击订阅合集第一时间收到推送因为我就是从0基础小白开始自学的PR，我按照我的学习路径制作的一系列教程可能会比用了很多年Pr的大佬制作的更适合0基础小白，大佬们请手下留情 bilibili视频合集链接 https://space.bilibili.com/29018759/channel/collectiondetail?sid=94665 一、课程与剪辑基础与流程介绍1.剪辑软件介绍 Premiere Pro Final Cut Pro Vegas Pro 达芬奇 iMovie 2.剪辑流程介绍 1.整理素材文件 2.新建工程文件 3.开始剪辑 4.重新预览确认没有错误 5.渲染导出 6.再次确认视频没有错误 7.二压&#x2F;交片 3.视频基础知识 分辨率 帧率 编码格式 封装格式 码率&#x2F;比特率 4.项目文件管理二、初步接触Premiere Pro1.创建工程文件 项目名称 保存位置 2.首选项与快捷键 首选项 媒体缓存 内存 快捷键查询与设置 3.工作区概述 面板 工作区 蓝色高亮 4.导入媒体素材 拖拽导入 文件-导入(⌘I&#x2F;control+I) 双击导入 媒体浏览器 5.创建序列 文件-新建-序列(⌘+N&#x2F;control+N) 直接拖拽 右键-从剪辑新建序列 三、Premiere Pro面板1.项目面板 素材箱 显示方式 排序与搜索 2.源监视器与节目监视器 源监视器 节目监视器介绍 3.时间轴面板 时间点 轨道 各种小工具 四、素材的导入和初步调整1.对视频进行导入删除移动操作 1)将片段添加到序列 i与o选择入点出点 选择拖入音频OR视频OR全部 2)从序列中删除片段 删除剪辑并保留其空间 删除片段并自动关闭间隙 向前选择轨道工具(向后选择轨道工具) 摁住shift选中多个片段 摁住alt(option)复制片段 ⌘+Z&#x2F;control+Z撤销 3)按顺序移动片段 直接拖拽进行移动 摁住⌘&#x2F;control以后进行拖拽 链接选择项 剃刀工具 2.导入图像及调整片段大小 1)处理图像文件 PSD 普通图像 2)更改剪辑片段的大小 右键-设置为帧大小 效果面板(缩放 位置) 3.添加及删除轨道五、关键帧、简易音频调整及标记1.关键帧 增减关键帧 前后关键帧 2.进行简单的音频调整 音频剪辑混合器 静音(mute) 独奏(solo) 左右声道 3.标记功能 快捷键M增加标记 增加标记到特定素材OR整个时间轴 调整标记颜色、名称、注释 标记面板 六、添加效果1.效果面板及效果控件2.转场与过渡 视频转场与过渡 音频转场与过渡 3.常用效果 裁剪 高斯模糊 变形稳定器 4.自定义预设七.调整片段时长、帧率及播放速度1.修剪时间轴上的剪辑片段时长 白色小三角 直接拖拽 波纹编辑工具(B) 滚动编辑工具(N) 内滑工具(U) 外滑工具(Y) Q(开头)和W(结尾) 2.使用轨道锁和同步锁 源修补 插入(,) 覆盖(.) 轨道目标定位 轨道锁定 同步锁定 3.更改素材帧率以适应序列4.更改片段的播放速度 帧采样 帧混合 光流法 预渲染 5.比率拉伸工具(R)八、基础调色教程1. 颜色工作区2. Lumetri 范围面板 小扳手 矢量示波器YUV 3. 基本校正面板 LUT 白平衡选择 手动调整 自动调整 4.其他调整面板 曲线 色轮 HSL 晕影 5.复制调整好的颜色预设九、基础音频教程1.录制画外音2.利用关键帧调整音量3.双声道切换为单声道4.将音量调整为一致音量 剪辑-音频选项-音频增益-标准化所有峰值为 全选-右键-音频增益-标准化所有峰值为 5.音频工作区 基本声音窗口 预设 6.Audition联动 使用Audition编辑音频 使用Audition对音频做简单的降噪处理 十、视频导出1.Premiere Pro自己导出 ⌘+M&#x2F;control+M 预设 保存位置 2.Adobe Media Encoder导出 设置预设&#x2F;导出预设&#x2F;导入预设 渲染队列 显卡硬件加速&#x2F;CPU软件编码","categories":[{"name":"随想","slug":"随想","permalink":"https://www.zl-asica.com/categories/%E9%9A%8F%E6%83%B3/"}],"tags":[{"name":"PR","slug":"PR","permalink":"https://www.zl-asica.com/tags/PR/"}]},{"title":"西联汇款Western Union薅羊毛-向国内转账100刀即可送 $20亚马逊GC(0成本0手续费)","slug":"western-union-20-amazon-gc","date":"2021-11-09T07:03:00.000Z","updated":"2024-12-02T16:04:42.808Z","comments":true,"path":"2021/western-union-20-amazon-gc/","permalink":"https://www.zl-asica.com/2021/western-union-20-amazon-gc/","excerpt":"1.西联汇款介绍(Western Union)西联汇款（Western Union, NYSE：WU）于1851年在纽约成立，現在總部在美國科羅拉多州丹佛市（Denver）。是世界上领先的特快汇款公司，迄今已有150年的历史，它拥有全球最大最先进的电子汇兑金融网络，代理网点遍布全球近200个国家和地区。 西联公司是美国财富五百强之一的第一数据公司（FDC）的子公司。","text":"1.西联汇款介绍(Western Union)西联汇款（Western Union, NYSE：WU）于1851年在纽约成立，現在總部在美國科羅拉多州丹佛市（Denver）。是世界上领先的特快汇款公司，迄今已有150年的历史，它拥有全球最大最先进的电子汇兑金融网络，代理网点遍布全球近200个国家和地区。 西联公司是美国财富五百强之一的第一数据公司（FDC）的子公司。 2.薅羊毛 一定要使用下面的链接进行操作，不要自行去西联注册，否则你讲薅不到羊毛！！！你需要有美国开户的借记卡账户&#x2F;且之前没有注册过西联 1.首先点击这个链接，你会显示下面这个页面，点击I Accept，填写你的个人信息及邮箱注册。 2.然后你的邮箱会收到验证码，像这样，把它填过去。 3.下一步便是汇款回国内了，那么我们所说的0手续费的方式是美国这边的账户直接转到国内的支付宝里，目前我测试的都是实时到账，登陆你的BOA、chase、Discover等主流银行的onlinebanking，就可以直接选择使用checking account或者saving account付款了，支付宝收钱是0手续费的。 4.首先我们选择地区为中国(China). 5.然后我们输入金额(需要大于等于100刀)，这里它会显示西联的汇率，那么这个价格基本上就是银行钞买价略高一点，比我们直接用谷歌看到的实时汇率略低一些，比如本篇文章撰写的时候谷歌显示汇率为6.39(6.3315&#x2F;6.39 &#x3D; 基本上就是一个99折，损失1刀的汇率差价)。我们还需要选择收款方式为Alipay(支付宝)，付款方式选择Bank Account(银行账户)，这样我们才能够0手续费。 6.下一步就可以填写收款人信息啦，填写的时候注意一定要详细填写，比如收款人叫做张三，我们就在last name填写ZHANG，first name填写SAN，地址填写你自己国内住址即可，手机号也填国内的就可以。注意支付宝的钱包手机号就是你支付宝绑定的手机号，一定要正确填写！！！转款目的选择生活费就好，不要选择工资，防止被风控。 7.然后就是登陆你的网银确认账单付钱就可以了，过程非常的简单 8.付款完成后大多数人会秒到账，如下图所示，如果你没有用过支付宝官方的闪速收款第一次可能会放在支付宝里然后你会收到短信，设置收款账户就会自动汇入你的银行卡里了。当然还有一种情况是你的付款被refused了，不需要担心，即使refused你还是能够拿到20刀的礼品卡，稍等一会如果确认没有礼品卡再重新转账也不迟。 9.大约付款完10分钟左右，你的邮箱就会收到西联发给你的礼品卡兑换码了，自己兑换使用就可以了。 3.总结这次薅羊毛的流程非常的简单，基本只需要2-3min的操作就可以获得20刀的amazon GC，大家没有使用过西联汇款的豆可以操作一下。注意一定要使用我放在这里的链接进行注册，否则你不仅不会获得这20刀的羊毛，以后的新用户注册活动也和你无关了。西联汇款羊毛链接(注册后汇款100刀即可获得20刀GC)","categories":[{"name":"随想","slug":"随想","permalink":"https://www.zl-asica.com/categories/%E9%9A%8F%E6%83%B3/"}],"tags":[{"name":"羊毛","slug":"羊毛","permalink":"https://www.zl-asica.com/tags/%E7%BE%8A%E6%AF%9B/"}]},{"title":"C++实现动态二维数组的两种简易方法","slug":"cpp-2d-dynamical-array","date":"2021-10-28T00:21:00.000Z","updated":"2024-12-02T16:04:42.808Z","comments":true,"path":"2021/cpp-2d-dynamical-array/","permalink":"https://www.zl-asica.com/2021/cpp-2d-dynamical-array/","excerpt":"1.C++的动态数组(Dynamically-allocated Single-Dimension arrays)当无法使用静态分配的数组时 — 因为你在编译时不知道合适的大小，或该数组太大而无法合理地放入运行时堆栈，或该数组的生命周期比创建它的函数更长。动态分配数组涉及到使用new运算符，类似于其他对象的动态分配，不同之处在于数组需要我们另外指定大小。","text":"1.C++的动态数组(Dynamically-allocated Single-Dimension arrays)当无法使用静态分配的数组时 — 因为你在编译时不知道合适的大小，或该数组太大而无法合理地放入运行时堆栈，或该数组的生命周期比创建它的函数更长。动态分配数组涉及到使用new运算符，类似于其他对象的动态分配，不同之处在于数组需要我们另外指定大小。 动态分配数组是使用new表达式完成的，该表达式会动态分配足够的内存来存储整个数组，然后返回指向数组第一个单元格的指针。 1int* a = new int[10]; 表达式 new int[10] 在堆上分配一个足够大的内存块来存储 10 个整数，并返回一个指向第一个的指针。第二个的位置直接跟在第一个之后，第三个直接跟在第二个之后，依此类推，所有的单元格都是相同的类型。所以给定一个指向第一个单元格的指针和一个索引，在幕后，可以使用以下计算来计算任何给定单元格的索引： 1单元格 i 的地址 = (单元格 0 的地址) + (sizeof(int) * i) 注意，计算具有大索引的单元格的地址并不比使用小索引更消耗性能。这个计算只是一个乘法和加法；如果我们假设给定地址的内存访问需要恒定时间，那么访问数组中的任何单元都需要恒定时间。（在实践中，内存访问时间可能会因缓存等影响而有所不同，尽管你可以合理地认为主内存访问花费恒定时间。） 有趣的是，返回的指针类型并未指定有关数组的任何内容。当我们动态分配一个int数组时，我们得到的是一个int，即一个指向int的指针。数组在一般情况下，实现为指针，以他们的第一个单元，它是由我们来了解是否特定int指向一个单一的int或int数组。 一旦有了指向数组的指针，就可以像访问静态分配的数组一样访问它的每一个数字： 123int* a = new int[10];a[3] = 4;std::cout &lt;&lt; a[3] &lt;&lt; std::endl; a[3]相当于假想表达*q，其中q是一个指向int占用三个单位的一个点。换句话说，a[3]给你提供的是这个单元格中的int（从理论上讲，是对它的引用），而不是指向这个单元格的指针。 当完成动态分配的数组时，需要释放它，就像您处理任何其他动态分配的对象一样。但是，重要的是要注意你要使用不同的运算符delete[]来执行此操作。像delete一样，你给delete[]一个指向数组的指针，它会为你释放整个数组（以及它所有单元格中的所有对象）。 1delete[] a; 由你决定哪些指针指向数组，并在需要时使用delete[]。在指向数组的指针上使用delete而不是delete[ ]，或在指向单个值的指针上使用delete[]而不是delete，会导致“undefined behavior”。就像我们之前看到的使用delete一样，指针a不受影响，尽管它现在指向未分配的内存，因此它现在就不会被再使用。 2.C++动态分配二维数组-使用常规方式(即不使用除iostream外的内置库)动态二维数组的分配 123int** test = new int*[rows]; for(int i = 0; i &lt; rows; i++) test[i] = new int[columns]; 释放动态分配二维数组占用的空间 123for(int i = 0; i &lt; rows; i++) delete []test[i]; delete []test; 3.C++动态分配二维数组-使用Vector(需要额外的库vector)注意使用这种方法你需要在文件头 1#include &lt;vector&gt; 使用vector动态分配二维数组 123std::vector&lt;std::vector&lt;int&gt;&gt; test(rows); for (i = 0; i &lt; test.size(); i++) test[i].resize(columns); 使用swap()方法清空vector占用的内存空间 123// vector的clear方法无法清空其占用的内存//可以使用swap来使一个空的vector来替换掉原来test里有的内容。vector&lt;T&gt;().swap(test);","categories":[{"name":"代码","slug":"代码","permalink":"https://www.zl-asica.com/categories/%E4%BB%A3%E7%A0%81/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://www.zl-asica.com/tags/C/"}]},{"title":"Java判断一个日期几个工作日后的日期（忽略节假日）","slug":"java-get-date-after-several-work-day","date":"2021-10-27T23:09:00.000Z","updated":"2024-12-02T16:04:42.808Z","comments":true,"path":"2021/java-get-date-after-several-work-day/","permalink":"https://www.zl-asica.com/2021/java-get-date-after-several-work-day/","excerpt":"1.前言前几天写作业的时候碰到一道题目，题目大意如下 不同的物品配送速度不同，常规商品需要5个工作日，食品需要1个工作日，数码产品立即送达（通过邮箱）。根据用户的下单日期和类别判断哪天能送到（忽略节假日）。","text":"1.前言前几天写作业的时候碰到一道题目，题目大意如下 不同的物品配送速度不同，常规商品需要5个工作日，食品需要1个工作日，数码产品立即送达（通过邮箱）。根据用户的下单日期和类别判断哪天能送到（忽略节假日）。 在网上搜了一圈发现基本上都是使用的各种非常复杂的方式并且有各种兼容性问题，基本上使用的都是数组+Calendar的形式来写的。这里根据教授的提示了解到了一种非常方便的形式来解决这个问题。 2.思路1.题目分解首先我们需要判断是什么类型的产品，然后再根据产品对日期进行处理并切返回给用户。 2.日期格式这道题目是用的格式为yyyy-mm-dd，所以这道题我们也需要根据这个方式来写。 3.方法根据教授的提示Java有一个非常适合解决这个问题的类 java.time.LocalDate 这里是官方文档https://docs.oracle.com/javase/8/docs/api/java/time/LocalDate.html 有兴趣的可以自行查看。根据文档我们可以获得几个方法来帮我们解决问题。 LocalDate 12LocalDate a = LocalDate.of(2012, 6, 30);// 这里就是把2021.6.30转换到了java.time可以识别的日期格式LocalDate。 plusDays 12a = a.plusDays(1);// 此处的a就会变成2021.7.1，日期会自动更新到下一个月。 getDayOfWeek 123a.getDayOfWeek();// 这里可以获取到a的日期对应的是星期几。该方法返回是周中哪天的枚举DayOfWeek。这就避免了对int值含义的混淆。如果你需要访问原始的int值，那么枚举提供了int值。// 比如这里是2021.7.1，也就是星期四。 4.代码下面是我们正式实现功能的函数 123456789101112131415161718192021public String purchase(String purchaseDate) &#123; // 以食品类举例，配送时间为一个工作日，purchaseDate是String类别以格式为YYYY-MM-DD的日期。 String[] strOfdate = purchaseDate.split(&quot;-&quot;, 3); // 以-分割用户输入的字符串保存到名为strOfdate的字符串数组中 LocalDate finaldate = LocalDate.of(Integer.parseInt(strOfdate[0]), Integer.parseInt(strOfdate[1]), Integer.parseInt(strOfdate[2])); // 设定一个LocalDate变量，变量名为finaldate，将分割好的日期赋值给它 int daycalc = 0; do &#123; finaldate = finaldate.plusDays(1); // 把用户输入的日期加一天 if ((finaldate.getDayOfWeek() != DayOfWeek.SATURDAY &amp;&amp; finaldate.getDayOfWeek() != DayOfWeek.SUNDAY)) &#123; // 判断加了一天以后是否是周六或者周日，如果是就继续循环 ++daycalc; // 如果不是就把daycalc参数加一，计作一个工作日。 &#125; &#125;while (daycalc &lt; 1); // 当工作日等于1的时候停止循环 return finaldate.toString(); // 将日期转换为String类型返回给用户&#125; 如果有任何问题欢迎在下面留言，转载文章请保留本文链接及我的IDZL Asica","categories":[{"name":"代码","slug":"代码","permalink":"https://www.zl-asica.com/categories/%E4%BB%A3%E7%A0%81/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://www.zl-asica.com/tags/Java/"}]},{"title":"FFmpeg的安装与基础使用教程","slug":"ffmpeg","date":"2020-11-21T03:01:28.000Z","updated":"2024-12-02T16:04:42.808Z","comments":true,"path":"2020/ffmpeg/","permalink":"https://www.zl-asica.com/2020/ffmpeg/","excerpt":"介绍FFmpeg 是一个开放源代码的自由软件，可以运行音频和视频多种格式的录影、转换、流功能，包含了libavcodec——这是一个用于多个项目中音频和视频的解码器库，以及libavformat——一个音频与视频格式转换库。 “FFmpeg”这个单词中的“FF”指的是“Fast Forward”。","text":"介绍FFmpeg 是一个开放源代码的自由软件，可以运行音频和视频多种格式的录影、转换、流功能，包含了libavcodec——这是一个用于多个项目中音频和视频的解码器库，以及libavformat——一个音频与视频格式转换库。 “FFmpeg”这个单词中的“FF”指的是“Fast Forward”。 安装本文只单独介绍如何在Windows和macOS下安装FFmpeg，暂不讨论在Linux下的情况。FFmpeg的官网为https://ffmpeg.org/download.html Windows 首先打开上面的官网下载链接，找到Windows模块下的Windows builds from gyan.dev 在新打开的gyan.dev的页面中找到Release部分，Links里第一个full(如红箭头所示)的链接直接点击下载FFmpeg的最新版压缩包。 下载下来的7z安装包先解压，然后将解压后的文件夹放至你不会随意删掉或改动为止的路径下(如C盘的Program Files但不是必须放到C盘)。 复制ffmpeg解压后文件夹内的bin文件夹路径(如下图所示) 打开设置-系统-关于-高级系统设置 打开高级系统设置后点开环境变量，找到系统变量中的Path变量双击点开。 新打开的页面点击右边的新建,粘贴进去在第四步复制的bin文件夹链接 添加完后一步一步确定-确定-确定。 win+R，输入cmd，回车，打开cmd 输入FFmpeg并回车测试是否安装成功，显示类似下图即为安装成功 macOS command+空格打开聚焦搜索，输入terminal并回车打开终端 输入下述命令安装homebrew 1/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)&quot; homebrew安装过程中可能会需要root权限(管理员权限)，届时需要输入你的系统密码，输入时不会显示你输入的内容，输入完成回车即可。3. homebrew安装完成后输入下述命令安装ffmpeg 1brew install ffmpeg 安装完成后输入ffmpeg测试是否安装成功，显示类似下图即为安装成功 基础使用格式转换FFmpeg转换格式最简单最常用的命令如下： 1ffmpeg -i input.xxx output.xxx 例如我们有原视频a.mov想要转成mp4格式并更改文件名为b我们可以使用如下命令 1ffmpeg -i a.mov b.mp4 mkv解封，直接复制音频与视频流到mp4中进行重新封装(此方式适用于flv格式，例如B站下下来的)，由于不需要重新编码，此代码的转换速度取决于你电脑的硬盘速度。 1ffmpeg -i a.mkv -vcodec copy -acodec copy b.mp4 视频压缩FFmpeg压缩视频应使用类似如下格式的命令： 12345ffmpeg -i input.mp4 -r 10 -b:a 32k output.mp4 #对它降低fps和音频码率的方法大大压缩文件大小，而清晰度不变。#或者ffmpeg -i input.mp4 -vcodec libx264 -crf 22 output.mp4 #将原视频转换成H.264格式并压缩，只压缩码率，其他不变#再或者ffmpeg -i input.webm -vcodec libx264 -crf 20 -acodec aac output.mp4 #将YouTube vp9编码转换为h264编码 命令选项介绍-r 码率-b:a 音频码率-vcodec 视频编码-crf 控制不变码率(量化比例的范围为0 ~ 51，其中0为无损模式，23为缺省值，51可能是最差的，推荐日常使用18-22。)-acodec 音频编码如果想要在转码压制视频时保持音频不对音频进行处理请在命令行里加入下述命令直接复制音频流到新的视频里可保存原视频同等的音频流。 1-acodec copy 转换视频到gifFFmpeg转换视频到gif可使用下述命令 12345#把视频的前 30 帧转换成一个 Gifffmpeg -i in.mp4 -vframes 30 -f gif out.gif#将视频转成 gif 将输入的文件从 (-ss) 设定的时间开始以 10 帧频率，输出到 320x240 大小的 gif 中，时间长度为 -t 设定的参数。ffmpeg -ss 00:00:00.000 -i in.mp4 -pix_fmt rgb24 -r 10 -s 320x240 -t 00:00:10.000 out.gif 进阶使用音视频编码转换-vcodec 可以用来选择你索要使用的编码器(如h264&#x2F;hevc&#x2F;mpeg4)，例如： 123ffmpeg -i in.mp4 -vcodec h264 out.mp4ffmpeg -i in.mp4 -vcodec hevc out.mp4ffmpeg -i in.mp4 -vcodec mpeg4 out.mp4 额外的选项：-s 指定分辨率，-b 指定比特率，-r 指定帧率，-acodec 指定音频编码，-ab 指定音频比特率，-ac 指定声道数，例如： 1ffmpeg -i in.mp4 -s 1920x1080 -b 200k -vcodec h264 -r 60 -acodec libfaac -ab 48k -ac 2 out.mp4 转换封装保留编码和其他选项(如mkv或flv解封装后重新封装为mp4)，例如： 12ffmpeg -i in.mkv -vcodec copy -acodec copy out.mp4ffmpeg -i in.flv -vcodec copy -acodec copy out.mp4 合并视频我们经常会需要将两个视频合并到一起，可以使用以下命令进行合并 1ffmpeg -i &quot;concat:1.ts|2.ts&quot; -acodec copy -vcodec copy -absf aac_adtstoasc output.mp4 更改视频分辨率或比例视频分辨率可以使用-s来指定，视频比例可以使用-aspect来指定，例如： 12ffmpeg -i input.mp4 -s 1280x720 -acodec copy output.mp4ffmpeg -i input.mp4 -aspect 16:9 output.mp4 剪辑视频和裁剪视频画面一些基础的剪辑视频和画面裁剪也可以通过FFmpeg实现-ss表示开始的时间，-t表示时间的长度 例如： 12345#从30s开始截取10秒的视频并封装进h264，aac编码的out.mp4里ffmpeg -i in.mp4 -ss 00:00:30 -t 00:00:10 -acodec aac -vcodec h264 -acodec aac out.mp4#将1920x1080的视频截取中间的1080x1080部分，crop的参数选择为width:height:x:y，width:height为裁剪后的视频分辨率，x:y为裁剪出来的左上角的点的坐标，故本视频需要为x轴(1920-1080)/2=420，y轴不变故用0占位。ffmpeg -i in.mp4 -vf crop=1080:1080:420:0 -acodec aac out.mp4 提取(去除)视频中的视频(或音频)-an 为去除音频，-vn 为去除视频，例如： 12345#去除视频中的音频(提取视频)ffmpeg -i in.mp4 -vcodec copy -an out.mp4#去除视频中的视频(提取音频)ffmpeg -i in.mp4 -acodec copy -vn out.mp4 合并音视频本操作等同于将纯视频(无音频)的视频里的视频流和单独的音频文件里的音频流进行合并，例如： 1ffmpeg –i in.mp4 –i in.mp3 –vcodec copy –acodec copy out.mp4 旋转视频将视频按照弧度制进行旋转，使用-vf rotate&#x3D;参数，例如： 12#将视频旋转90度ffmpeg -i in.mp4 -vf rotate=PI/2 out.mp4 视频(音频)变速视频变速使用-filter:v setpts&#x3D;参数，音频变速使用-filter:a atempo&#x3D;参数，例如： 12345#将视频调整为0.5倍速ffmpeg -i in.mp4 -filter:v setpts=0.5*PTS out.mp4#音频变速为原先的两倍ffmpeg -i in.mp3 -filter:a atempo=2.0 out.mp3 总结FFmpeg是一个非常厉害的格式转化与压制的软件，虽然没有GUI，但是只要掌握了几个基本的命令就足以完成绝大多数人的使用需求，Windows、macOS、Linux全平台试用。而且由于FFmpeg是一个开源软件，所以你可以根据你的个性化需求对该软件进行定制。同样如果你有更多的使用需求可以去查阅FFmpeg的官方文档选择你所需要的参数。如果对本文中有任何建议或者问题欢迎在下方评论区留言~","categories":[{"name":"软件","slug":"软件","permalink":"https://www.zl-asica.com/categories/%E8%BD%AF%E4%BB%B6/"}],"tags":[{"name":"ffmpeg","slug":"ffmpeg","permalink":"https://www.zl-asica.com/tags/ffmpeg/"}]},{"title":"WordPress，你好！","slug":"hello-wordpress","date":"2020-02-29T16:00:00.000Z","updated":"2024-12-02T16:04:42.808Z","comments":true,"path":"2020/hello-wordpress/","permalink":"https://www.zl-asica.com/2020/hello-wordpress/","excerpt":"Hexo？在来到WordPress以前，我的博客使用的是Hexo作为静态页面生成器，主题使用的是nexmoe主题，并做了稍微改动，WordPress站 https://www.zla.moe。","text":"Hexo？在来到WordPress以前，我的博客使用的是Hexo作为静态页面生成器，主题使用的是nexmoe主题，并做了稍微改动，WordPress站 https://www.zla.moe。 hexo有好处也有坏处好处1.首先使用hexo的好处就是不用学PHP（WP的基本使用也是不需要的，正常的html和css足够了） 2.其次就是纯静态网站对服务器资源的占用，非常的小，而WP使用的PHP和数据库会对服务器产生一定的压力。 3.WP稳定性也不如纯静态网站，即使在有定期备份的情况下也无法保证全部数据不丢失，而hexo由于都是从本地上传上去的，所以本地至少有一份备份，数据丢失的可能性极低。 4.hexo原生支持markdown语法，wordpress需要安装插件后才能够使用（所有文章都会以这样的形式在两个站点发布）。 坏处1.首先就是文章管理，相对于WP非常的复杂，很不舒服。 2.跟在文章后面的就是评论的管理了，由于网站搭建在境内需要对评论有一定的限制，而hexo我使用的是valine，管理起来还算是比较舒服的，但是还是和WP的没有可比性。 3.如果想要自己定制主题的话，就需要学习一下ejs和style，自己定制一个主题出来。当然在ejs中是可以直接使用html代码的，但是有一个问题就是你的html代码是否能够让主题完美的适配，这是个很大的问题。在wordpress里由于主题非常多，基本上可以找到比较适合自己的主题。如若颜色或某些细节样式觉得不适合，可以使用css对主题进行一定的修饰，使其更加美化。 4.SEO优化非常的复杂，在WP中可以使用各种插件来帮助你做优化，也可以自定义一些meta之类的 5.so on~(欢迎在评论区增加一下) 投入Wordpress的怀抱wordpress吧，现在我使用的主题为Sakura，作者是Mashiro，可以点前面的超链接去看看相关的主题作者的博客和主题介绍页面。之后我会对WP的主题进行一定的定制，基本上就是加一些小玩意改改样式，还敬请期待咯~ 如果你对hexo和wp有一些和我不一样的见解还欢迎在下方评论区留言哦~~~","categories":[{"name":"随想","slug":"随想","permalink":"https://www.zl-asica.com/categories/%E9%9A%8F%E6%83%B3/"}],"tags":[{"name":"WordPress","slug":"WordPress","permalink":"https://www.zl-asica.com/tags/WordPress/"}]},{"title":"树莓派安装qBittorrent-nox实现种子服务器自动下载上传","slug":"raspberry-pie-qbittorrent-nox","date":"2019-11-19T01:21:54.000Z","updated":"2024-12-02T16:04:42.808Z","comments":true,"path":"2019/raspberry-pie-qbittorrent-nox/","permalink":"https://www.zl-asica.com/2019/raspberry-pie-qbittorrent-nox/","excerpt":"前言&nbsp;&nbsp;我买回家树莓派 3B+以后，就一直没有把它充分的利用起来，仅仅做了一个无线打印机还有一个内网访问的网站。这段时间正好接触到了种子，感觉拿来做一个种子的服务器还是不错的。","text":"前言&nbsp;&nbsp;我买回家树莓派 3B+以后，就一直没有把它充分的利用起来，仅仅做了一个无线打印机还有一个内网访问的网站。这段时间正好接触到了种子，感觉拿来做一个种子的服务器还是不错的。 平台介绍&nbsp;&nbsp;树莓派 3b+、一张插在树莓派上作为系统盘的金士顿 64GB 的 tf 卡、固定的公网 ip、长时间通电以及能够稳定的网速。&nbsp;&nbsp;系统版本：Raspbian GNU&#x2F;Linux 10 (buster)&nbsp;&nbsp;软件：qBittorrent-nox&nbsp;&nbsp;网络环境：山东联通 准备工作&nbsp;&nbsp;首先你需要固定你的公网 IP，这是为了稳定你的种子下载和上传的速度，并且能够方便你在外网对你的树莓派的种子下载上传进行管理。如果你不知道公网 ip 为何物亦或是不知道如何固定公网 ip 请自行查阅，本文将不多赘述。&nbsp;&nbsp;要在 Linux 上使用 qBittorrent Web UI，你无需安装完整的 qBittorent 桌面应用程序，有一个基于终端的 qBittorrent 应用程序可用，它被称为 qBittorrent-Nox。&nbsp;&nbsp;注意：Web UI 功能不仅限于 qBittorrent-mox 应用程序，此功能还可以与传统的 qBittorent Linux 桌面应用程序一起使用，该应用程序可通过 Flapak 安装。 安装过程安装 qBittorrent-nox&nbsp;&nbsp;在树莓派上获取 qBittorrent-nox 非常简单，因为它位于“Main”软件存储库中，但是，由于操作系统的软件更新方式，main 中的版本可能会略微过时，要安装它，请在下面输入 apt-get 命令： 1sudo apt-get install qbittorrent-nox 使用方法开启 qBittorrent-nox 服务并映射 8070 端口&nbsp;&nbsp;请直接输入以下命令并回车。其中 8070 为映射的端口，-d 为后台运行 1qbittorrent-nox --webui-port=8070 -d 至于为何使用 8070 端口而不使用默认的 8080 端口原因很简单，国内运营商封禁了家庭宽带外网的 8080 端口，故本教程使用 8070 作为了映射的端口以便能够在外网访问，可以自己随意算则端口，本教程后面的 8070 全部更改为你自己的端口即可 防火墙放行 8070 端口在终端输入以下命令，回车运行 1sudo iptables -I INPUT -p tcp --dport 8070 -j ACCEPT 如果终端返回-bash: iptables: command not found请执行以下命令安装 iptables 后再重新执行上述命令 1sudo apt-get update &amp;&amp; apt-get install iptables 放行端口后请执行下面的命令以保存放行规则 1sudo iptables-save 设置完就已经放行了指定的端口，但重启后会失效，下面设置持续生效规则；安装iptables-persistent 1sudo apt-get install iptables-persistent 执行下述命令保存规则持续生效 12netfilter-persistent savenetfilter-persistent reload 将 qBt-nox 加入开机自启项输入以下命令打开 rc.local 文件以编辑 1sudo nano /etc/rc.local 在 exit 0 前加入 1sudo qbittorrent-nox --webui-port=8070 -d 输入完成后摁 control+X 再摁一下 Y 再摁一下回车即可完成保存设置完成开机自启 打开 qBt-nox 的 webUI 并更改默认账户与密码打开任意浏览器，在浏览器地址栏内输入 http:&#x2F;&#x2F;你的公网IP:8070 回车 默认用户名为 admin 密码为 adminadmin 点击上方工具栏的 tool-Options-WebUI 在上方可以更改语言为简体中文，更改后会重新进入 qBt 的 WebUI 下方可以更改默认的账户和密码，强烈建议更改，请勿使用默认设置！！！ 至此，树莓派安装 qBittorrent-nox 实现种子服务器自动下载上传教程已全部结束，qBittorrent-nox 的使用方式与 PC 端的方法一致。","categories":[{"name":"软件","slug":"软件","permalink":"https://www.zl-asica.com/categories/%E8%BD%AF%E4%BB%B6/"}],"tags":[{"name":"树莓派","slug":"树莓派","permalink":"https://www.zl-asica.com/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"}]},{"title":"Microsoft Office 图标换新-Office图标迭代史","slug":"Microsoft-Office-图标换新-Office图标迭代史","date":"2019-10-14T01:00:00.000Z","updated":"2024-12-02T16:04:42.808Z","comments":true,"path":"2019/microsoft-office-tu-biao-huan-xin-office-tu-biao-die-dai-shi/","permalink":"https://www.zl-asica.com/2019/microsoft-office-tu-biao-huan-xin-office-tu-biao-die-dai-shi/","excerpt":"前言&nbsp;&nbsp;如果你安装的是 Office 2019 或者 Office 365，那么一定发现，你的 Office 图标…换…新…了…！和旧版相比，新版 Office 图标延续了扁平画风，但现代感更强。微立体、渐变色这些 2019 年流行的元素，几乎都可以在这套新图标中找到。","text":"前言&nbsp;&nbsp;如果你安装的是 Office 2019 或者 Office 365，那么一定发现，你的 Office 图标…换…新…了…！和旧版相比，新版 Office 图标延续了扁平画风，但现代感更强。微立体、渐变色这些 2019 年流行的元素，几乎都可以在这套新图标中找到。 链接：微软官方商城-Office &nbsp;&nbsp;有人还发现 2019 版的 office 新图标配合全新的 Windows 10 Light Theme 时，更是逼格满满。下面便是 office 每代的发展史和他们不同的图标 发展史Office 2019、Office 365（最新）&nbsp;&nbsp;最新版 Office 2019 和 Office 365 使用了新图标，整体风格简约大气，融合了微立体、渐变色等很多时下流行元素，再配合 Windows 10 的 Light Theme 更显逼格满满。正版用户现在已经可以更新使用了，快来更新增加逼格吧！ Office 2019（早期）&nbsp;&nbsp;Office 2019 最早发布于 2018 年，初代使用 Office 2016 的图标设计，这套图标其实早在 2013 年即开始定型， 16 版则对它进行了小幅改动。虽然也是扁平化设计，但历时 7 年时间，早已让这套图标倍显老气。2018 年底，微软宣布将更新 Office 图标，Office 开始走上新路途。 Office 2016&nbsp;&nbsp;发布于 2015 年的 Office 2016，延续了 13 版设计风格，只是在其基础上做了一点点小幅优化，看起来更加明显。其实这个版本也是很多网友在用的版本。 Office 2013&nbsp;&nbsp;Office 2013 发布于 2012 年，正值微软首款跨平台操作系统 Win8 亮相。整体风格偏向于扁平化，恰好与 Win8 的诉求相互契合。Office 2013 是一个跨时代版本，它的很多设计最终也影响到后续产品的发展。包括 Office 2016、Office 2019 早期版，无论是功能还是图标设计，几乎都是在 13 版的基础上进行的小幅优化。还有一点就是，从这个版本开始 Office 开始启用新 LOGO，不再是沿用多年的四叶草图标。 Office 2010&nbsp;&nbsp;Office 2010 发布于 2009 年，此时的微软已经被 Vista 的颓势搞得焦头烂额，同年发布的 Office 2010 自然也延续了很多前辈 Office 2007 的设计，图标是在原有风格基础上做了一些小幅优化，通过加大首字母元素，突出各个模块的功能定位。总之你也可以把它看作是 Office 2007 的一个翻版，只不过看上去更加华丽。 Office 2007&nbsp;&nbsp;Office 2007 是一个跨时代版本，从这一版本开始，微软首次采用 RIBBON 作为软件界面。从图标上说，Office 2007 的设计也和前版风格迥异，给人了一种耳目一新的感觉。更加有趣的是，微软还将这一设计复制到实体包装盒上，同样的圆滑造型，同样的颜色布设，几乎可以堪称当年最“IN”的办公软件。 Office 2003&nbsp;&nbsp;Office 2003 可谓是 Office 系列中的经典，以至于若干年后的今天，仍然能在很多地方见到它。这套图标相比之前增加了渐变效果，配合全新的蓝色界面，时尚感非常不错。同时也正因为其超高的曝光率，成为受众人数最广的 ICON 之一。此外四叶草 LOGO 在这一版中也首次替换掉拼图 LOGO，既时尚又更能凸显新版的意图。 Office XP(2002 版)&nbsp;&nbsp;和前后两版相比，Office 2002 知道的人很少，一方面是其功能变化不大，另一方面也恰巧排在了 Office 2000 和 Office 2003 这两个史上最经典 Office 版本之间。这一版本诞生于 2001 年，与著名的 Windows XP 同年而生，因此命名上也罕见地采用了“XP”称号，而非此前的年份命名。和很多过渡版本一样，Office 2002 也沿用了 Office 2000 的图标风格，并非在上面进行任何修改，依旧是突出各版块间自身的品牌定位。 Office 2000&nbsp;&nbsp;Office 2000 发布于 1999 年，同样也是 Office 历史上很著名的版本之一，和经典的 Office 97 相比，Office 2000 功能变化不大，仅仅是增加了一些人性化改进。但由于性能与前版差别不多，在实际使用中普及率很高。图标方面，Office 2000 采用了与以往完全不同的设计理念，将各版块首字母经过艺术处理后，作为对应组件的 LOGO。而这一设计也引领了后代 Office 设计理念，可谓开创了一个先河。 Office 97&nbsp;&nbsp;Office 97 诞生于 1996 年，堪称 Office 历史上第一个经典之作。整体设计沿袭此前的 Office 95 设计，图标大体模拟了组件功能，用现在的话说，就是纯粹的拟物化设计。而 Office 97 相比 Office 95 的最大看点，是整体界面更加养眼和耐看，这在 20 年多前，还是相当有吸引力的。 Office 95&nbsp;&nbsp;Office 95 是首款以年代为代号的 Office 软件，很多人知道 Office，其实就是从这一版开始的。Office 95 的设计带有浓郁的上世纪味道，几乎看不到任何现代 Office 套件的影子。而 Office 95+Windows 95，几乎是那个时代的标配之作。 Office “上古时代”&nbsp;&nbsp;当然 Office 95 也不是 Office 历史上第一个版本，在此之前，还有 Office 1.0、2.0、3.0、4.0 四个版本。不过那时的操作系统远没有 Win95 好用，基本上可以看作是挂接在 DOS 系统上的一套 GUI 环境，而那时的 Office 也仅限于 Word、Excel、PowerPoint 三大件。以现在的眼光来看，简直可以说是丑陋无比，但在当时这样的设计已经可以算是划时代产品了。 Microsoft Office for Mac&nbsp;&nbsp;除了上面介绍的这些以外，我们也不能忽视 Office 的另一大分支 Mac Office。从最开始的 MAC Office 2001 到最新版 MAC Office 2019，你会发现它在图标风格上变化很大。早期的 MAC Office 采用了与 Windows 版本完全不同的设计，甚至你会认为这是两款完全不同的软件，直到 2016 版开始，MAC Office 才正式与 Windows Office 保持统一，但依旧会有细节上的差别。 Microsoft Office for iPad&nbsp;&nbsp;Office for iPad 是 Office365 的一个附赠的功能，由于微软的政策是 10.1 英寸以上设备使用 office 均无免费版，所以目前市面上除了 9.7 寸外的所有 iPadd 设备均需购买 Office 365 服务才能使用 Office 的全部功能，不付费仅能使用查看功能。但是即便如此 Office for iPad 也大大提高了 iPad 的办公能力。 小结看完了整个 Office 图标进化史之后，你们都有哪些感慨呢？那么你第一次接触过的 Office 又是个什么样子呢？欢迎在下方评论区留言。","categories":[{"name":"随想","slug":"随想","permalink":"https://www.zl-asica.com/categories/%E9%9A%8F%E6%83%B3/"}],"tags":[{"name":"Microsoft","slug":"Microsoft","permalink":"https://www.zl-asica.com/tags/Microsoft/"}]},{"title":"Mac+centOS7+XAMPP 搭建discuz!论坛","slug":"Mac+centOS7+XAMPP 搭建discuz!论坛","date":"2019-08-18T16:00:00.000Z","updated":"2024-12-02T16:04:42.808Z","comments":true,"path":"2019/mac-centos7-xampp-da-jian-discuz-lun-tan/","permalink":"https://www.zl-asica.com/2019/mac-centos7-xampp-da-jian-discuz-lun-tan/","excerpt":"前言我本人当年在建站的时候使用的就是上述配置，但由于网上教程基本都是适用于 Windows 和老版 xampp 的教程，在 Mac 系统和新版的 xampp 有些地方已经不是很适用，所以当时我建站路程格外的辛苦，几乎翻看过所有能从谷歌上找到的相关文章，今天我就给大家写一下我当时建站的流程，供大家参考借鉴。如有错误，欢迎在下放留言指出。","text":"前言我本人当年在建站的时候使用的就是上述配置，但由于网上教程基本都是适用于 Windows 和老版 xampp 的教程，在 Mac 系统和新版的 xampp 有些地方已经不是很适用，所以当时我建站路程格外的辛苦，几乎翻看过所有能从谷歌上找到的相关文章，今天我就给大家写一下我当时建站的流程，供大家参考借鉴。如有错误，欢迎在下放留言指出。 系统介绍电脑：macOS Mojave 10.14.3 (18D109)服务器：阿里云 系统是 CentOS 7.4 64 位Discuz!版本： X3.4 建站前需要做的事确认你要建站的目的不同的目的建设的网站是不一样的,比如博主所建的就是一个 bbs 论坛，那么需要用到论坛的模板比如 wordpress、phpwin、Discuz!等等。 选择论坛使用的模板博主一开始选择的模板是 wordpress，由于这个模板的模式功能过于强大同时也过于复杂，而且这种论坛的交互模式并不适合国内使用，国内用户并不习惯，所以在多番斟酌及选择后，博主选择了 Discuz！作为论坛的模板。 确认网站受众及可能会产生的峰值流量在我们购买服务器时，我们需要考虑到我们所需要多少带宽，而带宽则由网站流量决定，如果 服务器配置 等待更新中。。。催更请点这里发送邮件催更","categories":[{"name":"软件","slug":"软件","permalink":"https://www.zl-asica.com/categories/%E8%BD%AF%E4%BB%B6/"}],"tags":[{"name":"Discuz!","slug":"Discuz","permalink":"https://www.zl-asica.com/tags/Discuz/"}]}],"categories":[{"name":"代码","slug":"代码","permalink":"https://www.zl-asica.com/categories/%E4%BB%A3%E7%A0%81/"},{"name":"随想","slug":"随想","permalink":"https://www.zl-asica.com/categories/%E9%9A%8F%E6%83%B3/"},{"name":"软件","slug":"软件","permalink":"https://www.zl-asica.com/categories/%E8%BD%AF%E4%BB%B6/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://www.zl-asica.com/tags/Python/"},{"name":"Summary","slug":"Summary","permalink":"https://www.zl-asica.com/tags/Summary/"},{"name":"DL","slug":"DL","permalink":"https://www.zl-asica.com/tags/DL/"},{"name":"ML","slug":"ML","permalink":"https://www.zl-asica.com/tags/ML/"},{"name":"C++","slug":"C","permalink":"https://www.zl-asica.com/tags/C/"},{"name":"PR","slug":"PR","permalink":"https://www.zl-asica.com/tags/PR/"},{"name":"羊毛","slug":"羊毛","permalink":"https://www.zl-asica.com/tags/%E7%BE%8A%E6%AF%9B/"},{"name":"Java","slug":"Java","permalink":"https://www.zl-asica.com/tags/Java/"},{"name":"ffmpeg","slug":"ffmpeg","permalink":"https://www.zl-asica.com/tags/ffmpeg/"},{"name":"WordPress","slug":"WordPress","permalink":"https://www.zl-asica.com/tags/WordPress/"},{"name":"树莓派","slug":"树莓派","permalink":"https://www.zl-asica.com/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"},{"name":"Microsoft","slug":"Microsoft","permalink":"https://www.zl-asica.com/tags/Microsoft/"},{"name":"Discuz!","slug":"Discuz","permalink":"https://www.zl-asica.com/tags/Discuz/"}]}