{"meta":{"title":"ZL Asicaçš„åšå®¢","subtitle":"Blog","description":"ZL Asicaçš„å°ç«™ï¼Œåˆ†äº«å„ç§æŠ€æœ¯çŸ¥è¯†ä¸ä¸ªäººçš„æŠ€æœ¯è®°å½•ã€‚ä¸ªäººæŠ€æœ¯è®°å½•åŠå°æŠ€å·§åˆ†äº«ã€ç”Ÿæ´»å°åˆ†äº«ã€å°çŸ¥è¯†ï¼Œå–œæ¬¢çš„è¯æ¬¢è¿ç‚¹è¿›æ¥åº·åº·(*â‰§âˆªâ‰¦)","author":"ZL Asica","url":"https://www.zl-asica.com","root":"/"},"pages":[{"title":"å…³äº","date":"2019-03-20T06:00:00.000Z","updated":"2024-02-13T06:37:05.553Z","comments":true,"path":"about.html","permalink":"https://www.zl-asica.com/about.html","excerpt":"","text":"è‡ªæˆ‘ä»‹ç»è¿™ä¸ªé¡µé¢æ˜¯å…³äºæˆ‘çš„ä¸€ä¸ªè‡ªæˆ‘ä»‹ç»é¡µé¢åè¡¨åŠ åˆ©ç¦å°¼äºšå°”æ¹¾ï¼ŒUC Irvineæœ¬ç§‘åœ¨è¯»ä¸“ä¸šä¸ºComputer Science, InformaticsåŒä¸“ä¸šï¼Œè¾…ä¿®Health Informatics(åŒ»å­¦ä¿¡æ¯å­¦)ã€‚åˆ›å»ºå¹¶æ‹¥æœ‰ä¸€ä¸ªå­—å¹•ç»„ç™½é²¸å­—å¹•ç»„ç¤¾äº¤å¹³å°è”ç³»æ–¹å¼å…¨åœ¨å·¦è¾¹ï¼Œéœ€è¦çš„å¯ä»¥è”ç³»æˆ‘å“¦äº¤æ¢å‹é“¾è¯·ç‚¹å‡»å·¦ä¾§æˆ‘çš„æœ‹å‹æŸ¥çœ‹è¯¦æƒ…å¹³å¸¸ä¼šç¼–å†™ä¸€äº›å°çš„ç¨‹åºã€ä¸Šå­¦ã€å†™ä»£ç ã€å†™ç½‘ç«™ã€å‰ªè§†é¢‘ä¹‹ç±»çš„ã€‚ åˆ›å»ºæ­¤åšå®¢çš„æ„å›¾åˆ›å»ºæ­¤åšå®¢çš„æ„å›¾æ˜¯ä¸ºäº†è®°å½•ä¸€äº›æˆ‘æƒ³åˆ°çš„å°ç‚¹å­ã€åœ¨ç¼–ç¨‹çš„æ—¶å€™é‡åˆ°ä¸€äº›é—®é¢˜æ—¶çš„è§£å†³è¿‡ç¨‹åŠæ–¹æ¡ˆä»¥åŠä¸€äº›è§†é¢‘å‰ªè¾‘çš„å°æŠ€å·§å’Œèµ„æºåˆ†äº«(GitHubã€YouTubeä»¥åŠBç«™é“¾æ¥å‡åœ¨ç½‘ç«™é¦–é¡µ)ã€‚ å…³äºæ­¤åšå®¢æ­¤åšå®¢åŸºäºHexo æ­å»ºï¼Œéƒ¨ç½²åœ¨ä¸Šæµ·è…¾è®¯äº‘æœåŠ¡å™¨ï¼Œå¹¶ä¸”é‡‡ç”¨ Markdown è¯­æ³•æ¥å†™æ–‡ç« ã€‚æƒ³è¦è®¿é—®æˆ‘çš„Wordpressç«™ç‚¹å¯ä»¥ç‚¹å‡»è¿™é‡Œã€‚ ç»™äºˆæˆ‘é¼“åŠ±å„ä½å¤§ä½¬æ‰“èµä¸€ä¸‹æˆ‘å§ãƒ¾(&#x3D;ï½¥Ï‰ï½¥&#x3D;)oï¼Œç›®å‰æ”¯æŒæ”¯ä»˜å®ã€å¾®ä¿¡ã€å’Œpaypalå›½é™…ç‰ˆå“¦PayPalå›½é™…ç«™çš„èµåŠ©å¯ä»¥ç‚¹å‡»æ­¤å¤„è·³è½¬ï¼æ”¯ä»˜å® å¾®ä¿¡å°ç»“ åšå®¢å¼€å¯äº†RSSï¼Œéœ€è¦çš„å¯ä»¥å…³æ³¨ä¸€ä¸‹ã€‚"},{"title":"æ–‡ç« å½’æ¡£","date":"2024-02-13T06:37:05.553Z","updated":"2024-02-13T06:37:05.553Z","comments":true,"path":"archive.html","permalink":"https://www.zl-asica.com/archive.html","excerpt":"","text":""},{"title":"","date":"2024-02-13T06:37:05.553Z","updated":"2024-02-13T06:37:05.553Z","comments":true,"path":"custom.css","permalink":"https://www.zl-asica.com/custom.css","excerpt":"","text":""},{"title":"æœ‹å‹","date":"2019-11-13T06:00:00.000Z","updated":"2024-02-13T06:37:05.553Z","comments":true,"path":"friends.html","permalink":"https://www.zl-asica.com/friends.html","excerpt":"","text":"æˆ‘çš„é¡¹ç›® ä¸‹é¢å…¨æ˜¯å¤§ä½¬!!! å‹æƒ…é“¾æ¥"}],"posts":[{"title":"2023å¹´åº¦æ€»ç»“","slug":"2023-annual-summary","date":"2023-12-31T15:59:59.000Z","updated":"2024-02-13T06:37:05.553Z","comments":true,"path":"2023/2023-annual-summary/","permalink":"https://www.zl-asica.com/2023/2023-annual-summary/","excerpt":"æœ¬æ–‡æ¦‚æ‹¬äº†æˆ‘åœ¨2023å¹´çš„ç”Ÿæ´»ã€å­¦ä¹ å’Œå¨±ä¹æ´»åŠ¨ï¼Œä»¥åŠæˆ‘å¯¹2024å¹´çš„æœŸæœ›å’Œè®¡åˆ’ã€‚åœ¨å¨±ä¹æ–¹é¢ï¼Œæˆ‘åˆ†äº«äº†è‡ªå·±è§‚çœ‹çš„ç”µå½±ã€ç•ªå‰§å’Œçºªå½•ç‰‡ï¼Œé˜…è¯»çš„å°è¯´ï¼Œä»¥åŠåœ¨æ¸¸æˆå’Œç”µå­äº§å“ä¸Šçš„ä½“éªŒã€‚åœ¨æ—¥å¸¸ç”Ÿæ´»æ–¹é¢ï¼Œæˆ‘è®°å½•äº†è‡ªå·±çš„é¥®é£Ÿä¹ æƒ¯ã€ä½å®¿ç»å†å’Œå„åœ°çš„æ—…è¡Œä½“éªŒã€‚å­¦æœ¯ä¸Šï¼Œæˆ‘åˆ†äº«äº†è‡ªå·±çš„æˆç»©ã€å‚ä¸çš„ç ”ç©¶é¡¹ç›®å’Œå¼€å‘çš„åº”ç”¨ç¨‹åºã€‚æ­¤å¤–ï¼Œæˆ‘è¿˜ä¸ºæ–°çš„ä¸€å¹´åˆ¶å®šäº†ç›®æ ‡å’Œè®¡åˆ’ï¼Œå±•æœ›æœªæ¥ï¼ŒæœŸå¾…åœ¨æ–°çš„ä¸€å¹´ä¸­å–å¾—æ›´å¤šæˆå°±ã€‚(ChatGPT 4.0ç”Ÿæˆ)","text":"æœ¬æ–‡æ¦‚æ‹¬äº†æˆ‘åœ¨2023å¹´çš„ç”Ÿæ´»ã€å­¦ä¹ å’Œå¨±ä¹æ´»åŠ¨ï¼Œä»¥åŠæˆ‘å¯¹2024å¹´çš„æœŸæœ›å’Œè®¡åˆ’ã€‚åœ¨å¨±ä¹æ–¹é¢ï¼Œæˆ‘åˆ†äº«äº†è‡ªå·±è§‚çœ‹çš„ç”µå½±ã€ç•ªå‰§å’Œçºªå½•ç‰‡ï¼Œé˜…è¯»çš„å°è¯´ï¼Œä»¥åŠåœ¨æ¸¸æˆå’Œç”µå­äº§å“ä¸Šçš„ä½“éªŒã€‚åœ¨æ—¥å¸¸ç”Ÿæ´»æ–¹é¢ï¼Œæˆ‘è®°å½•äº†è‡ªå·±çš„é¥®é£Ÿä¹ æƒ¯ã€ä½å®¿ç»å†å’Œå„åœ°çš„æ—…è¡Œä½“éªŒã€‚å­¦æœ¯ä¸Šï¼Œæˆ‘åˆ†äº«äº†è‡ªå·±çš„æˆç»©ã€å‚ä¸çš„ç ”ç©¶é¡¹ç›®å’Œå¼€å‘çš„åº”ç”¨ç¨‹åºã€‚æ­¤å¤–ï¼Œæˆ‘è¿˜ä¸ºæ–°çš„ä¸€å¹´åˆ¶å®šäº†ç›®æ ‡å’Œè®¡åˆ’ï¼Œå±•æœ›æœªæ¥ï¼ŒæœŸå¾…åœ¨æ–°çš„ä¸€å¹´ä¸­å–å¾—æ›´å¤šæˆå°±ã€‚(ChatGPT 4.0ç”Ÿæˆ) å¨±ä¹ç”µå½±æœ¬å¹´åº¦ä¸€å…±å»ç”µå½±é™¢çœ‹è¿‡5éƒ¨ç”µå½±ï¼Œåœ¨å®¶çœ‹è¿‡çš„æ•°ç›®åº”è¯¥æ¯”è¿™æ›´å¤šä¸€äº›ã€‚å› ä¸ºç¾å›½é™¢çº¿ä¸Š**æµæµªåœ°çƒ2çš„æ—¶å€™æ²¡æ—¶é—´çœ‹ï¼Œåæ¥åœ¨å®¶é‡Œè‡ªå·±è¡¥äº†ä¸€ä¸‹ï¼Œåœ¨äº”å…­æœˆè¿˜è¡¥äº†éƒ¨åˆ†å®«å´éªçš„ç”µå½±ï¼ˆé¾™çŒ«ã€é­”å¥³å®…æ€¥ä¾¿**ï¼‰ã€‚ æµæµªåœ°çƒ2 (The Wandering Earth II) - 4.5&#x2F;5.0 xxxx - 03&#x2F;2023 - 0ç¾å…ƒ è¿™ç”µå½±æˆ‘æ˜¯çœŸçš„å–œæ¬¢ï¼Œä¸ç®¡ä»å‰§æƒ…è¿˜æ˜¯ç‰¹æ•ˆï¼Œæˆ‘ä¸ªäººéƒ½å¾ˆæ»¡æ„ã€‚å”¯ä¸€çš„é—®é¢˜å°±æ˜¯ä½ ä¸è¦è€ƒè™‘å¤ªå¤šç§‘å­¦è§’åº¦çš„é—®é¢˜ï¼Œæ¯”å¦‚ä»€ä¹ˆå¤ªç©ºç”µæ¢¯è¿™ç§æ˜æ˜¾èŠ±è´¹å¤šä½™å¯å›æ”¶ç«ç®­æ–¹æ¡ˆçš„æ–¹æ¡ˆä¸ºä»€ä¹ˆä¼šè¢«ä½¿ç”¨ä¹‹ç±»çš„ã€‚åªè¦ä¸è€ƒè™‘è¿‡å¤šç§‘å­¦è§’åº¦é—®é¢˜å°±éå¸¸å¥½çœ‹ã€‚ é“ƒèŠ½ä¹‹æ—… (Suzume) - 4.0&#x2F;5.0 AMC - IMAX - 04&#x2F;13&#x2F;2023 - 20.29ç¾å…ƒ(çº¦145äººæ°‘å¸) é“ƒèŠ½ä¹‹æ—…åœ¨åŒ—ç¾ä¸Šçº¿æ—¶é—´è¾ƒæ™šï¼Œæ‰€ä»¥æ˜¯4æœˆä»½æ‰çœ‹åˆ°ï¼Œè¿™å·²ç»æ˜¯é¦–æ˜ å½“å¤©äº†ã€‚æ–°æµ·è¯šæ–°ä½œå¤§å®¶è¯„ä»·è¿˜æ˜¯æœ‰äº›å·®åˆ«çš„ï¼Œæˆ‘ä¸ªäººæ„Ÿè§‰è¿˜æ˜¯è›®ä¸é”™çš„ï¼Œæ¯”å¤©æ°”ä¹‹å­ç¨å¼±ä¸€äº›ï¼ˆå¤©æ°”ä¹‹å­4.5&#x2F;5.0ï¼‰ã€‚çœ‹çš„æ˜¯IMAXç‰ˆæ—¥è¯­+è‹±æ–‡å­—å¹•ï¼Œè¿™ä¸ªè‹±æ–‡å­—å¹•ç¨å¾®æœ‰ç‚¹å½±å“ä½“éªŒï¼Œç¿»è¯‘é£æ ¼å’Œä¸­å›½ç¿»è¯‘é£æ ¼ä¸å¤§ä¸€æ ·ã€‚å°¤å…¶æ˜¯å…³é—¨é‚£æ®µè‹±æ–‡ç›´æ¥ç›´è¯‘ï¼Œè¿œè¿œä¸åŠå›½å†…çš„è¯—è¯ç¿»è¯‘ã€‚ è¶…çº§ç›åˆ©æ¬§å…„å¼Ÿç”µå½±ç‰ˆ (The Super Mario Bros. Movie) - 3.5&#x2F;5.0 AMC - Dolby Vision - 04&#x2F;25&#x2F;2023 - 12.25ç¾å…ƒ(çº¦86äººæ°‘å¸) å› ä¸ºæˆ‘ä¸ªäººå¹¶ä¸æ˜¯ç›åˆ©æ¬§çš„èµ„æ·±è€ç©å®¶ï¼Œä¸»æœº&#x2F;æŒæœºæ¸¸æˆç©çš„æ¯”è¾ƒå°‘ä¹Ÿæ˜¯è¿™ä¸¤å¹´æ‰å¼€å§‹æ¥è§¦Switchçš„ã€‚ç©è¿‡çš„ç›¸å…³ä½œå“åªæœ‰**è¶…çº§é©¬åŠ›æ¬§ï¼šå¥¥å¾·èµ›å’Œè·¯æ˜“åŸºé¬¼å±‹3**è¿™ä¸¤éƒ¨ï¼Œè€Œä¸”ä¹Ÿéƒ½æ²¡æœ‰é€šå…³ã€‚æˆ‘ä¸ªäººçœ‹çš„è¿˜æ˜¯æ¯”è¾ƒå¼€å¿ƒçš„ï¼Œå¹¶æ²¡æœ‰ä»€ä¹ˆè§‚çœ‹é—¨æ§›ã€‚ å˜å½¢é‡‘åˆšï¼šä¸‡å…½å´›èµ· (Transformers: Rise of the Beasts) - 2.5&#x2F;5.0 AMC - Dolby Vision - 06&#x2F;13&#x2F;2023 - 13ç¾å…ƒ(çº¦92äººæ°‘å¸) æˆ‘çš„è¯„çº§å¾ˆç®€å•ï¼Œè¿™å•¥ç©æ„ï¼Ÿè¯´å®è¯æˆ‘èƒ½ç»™è¿™ä¸ªè¯„åˆ†ä¹Ÿæ˜¯å…¨çœ‹åœ¨ç‰¹æ•ˆå’Œæƒ…æ€€äº†ï¼Œå‰§æƒ…æ–¹é¢æˆ‘ä¸æ˜¯å¾ˆå¥½è¯„ä»·ã€‚å¤ªè€å¥—äº†è¿™å‰§æƒ…ã€‚äººç‰©é‡Œä¹Ÿå°±Mirageç»™æˆ‘çš„æ„Ÿè§‰ä¼šæ¯”è¾ƒæ·±åˆ»ï¼Œè›®å¯çˆ±çš„è§’è‰²ã€‚ å¥¥æœ¬æµ·é»˜ (Oppenheimer) - 4.0&#x2F;5.0 ä¸­å›½å¤§é™† - æ˜Ÿé“IMAX - IMAX - 09&#x2F;16&#x2F;2023 - çº¦6.48ç¾å…ƒ(45.9äººæ°‘å¸) é€ åŸå­å¼¹ï¼Œæ€ä¹ˆè¯´å‘¢ï¼Œæ„¿ä¸–ç•Œå’Œå¹³ã€‚è¯ºå…°æ‹çš„ç”µå½±ï¼Œæ•´ä½“è§‚æ„Ÿè‚¯å®šæ²¡å¾—è¯´ï¼Œæˆ‘åªè¯´ä¸€ä¸‹å‰§æƒ…æ„Ÿå—ã€‚å‰§æƒ…æ–¹é¢å……æ»¡äº†ç¾å›½çš„äººæƒ…ä¸–æ•…çš„æ„Ÿè§‰ï¼Œç»™æˆ‘ä¸€ç§çª’æ¯çš„æ„Ÿè§‰ï¼Œä¸ç®¡è‡ªå·±çš„æŠ€æœ¯æ°´å¹³å¤šä¹ˆå¼ºå¤§ï¼Œåœ¨æ”¿æ²»é¢å‰åªæ˜¯ä¸€ä¸ªå°ä¸‘ï¼Œéå¸¸æ— åŠ›ã€‚ç§‘ç ”è¿‡ç¨‹ä¸­å°±æ˜¯ä¼šå‡ºç°è‡ªæˆ‘è‚¯å®šå’Œå¦å®šçš„äº¤ç»‡ï¼Œè€Œè¿™ç§äº¤ç»‡åœ¨æ”¿æ²»å®¶çœ‹æ¥å°±æ˜¯å¯ä»¥åˆ©ç”¨çš„ç‚¹ã€‚è€Œä¸”å¦‚æœçœŸçš„åˆ›é€ å‡ºä¸€ä¸ªå¯èƒ½æ¯ç­å…¨äººç±»ä¹Ÿæœ‰å¯èƒ½å¸®åŠ©å…¨äººç±»çš„ä¸œè¥¿ï¼Œåˆ°åº•åº”è¯¥æ€ä¹ˆé€‰æ‹©ï¼Œæ˜¯ç»§ç»­è¿˜æ˜¯åœä¸‹ï¼Ÿå¯¹äºæˆ‘è¿™æ ·å¹³å¹³æ— å¥‡çš„äººæ¥è¯´æ€è€ƒè¿™äº›å½“ç„¶è¿‡å¤šï¼Œä½†æ˜¯å¯¹äºé¡¶å°–çš„ç§‘å­¦å®¶æ¥è¯´ä»–ä»¬ä¸å°±æ˜¯éœ€è¦å»é¢ä¸´è¿™ç§å›°å¢ƒã€‚ æ—ºå¡ (Wonka) - 4.0&#x2F;5.0 AMC - IMAX - 12&#x2F;26&#x2F;2023 - 12ç¾å…ƒ(çº¦85äººæ°‘å¸) åœ¨å¹´åº•çœ‹äº†è¿™éƒ¨æ—ºå¡ï¼Œè¿™ç»å¯¹æ˜¯å…¨å¹´é¾„éƒ½å¯ä»¥æ‰¾åˆ°è‡ªå·±è§‚çœ‹ç‚¹çš„ç”µå½±ã€‚æˆ‘å°æ—¶å€™éå¸¸å–œæ¬¢çœ‹ç½—è¾¾å°”çš„ä¹¦ï¼Œä»–å†™çš„åŸºæœ¬æ‰€æœ‰ä¹¦æˆ‘éƒ½çœ‹è¿‡å¥½å‡ éï¼Œæˆ‘ä¸ªäººæ˜¯éå¸¸å–œæ¬¢çš„ã€‚è¿™éƒ¨ç”µå½±æ›´å¤šæ˜¯å·§å…‹åŠ›å·¥å‚çš„å‰ä¼ ï¼Œè®²å¾—æ›´å¤šçš„æ˜¯å·§å…‹åŠ›å·¥å‚æ˜¯æ€ä¹ˆæ¥çš„ï¼Œéå¸¸æ¢¦å¹»ã€‚æ³¨æ„çœ‹çš„æ—¶å€™ä¸è¦è€ƒè™‘ç§‘ä¸ç§‘å­¦çš„é—®é¢˜ï¼Œè¦ä¸ç„¶å°±ä¼šå¾ˆå®¹æ˜“å‡ºæˆã€‚ ç•ªå‰§æ¥åˆ°ç•ªå‰§éƒ¨åˆ†ï¼Œæˆ‘æœ¬å¹´åº¦çœ‹è¿‡çš„ç•ªå‰§å’Œå¾€å¹´ä¸€æ ·æ•°ç›®è¿‡å¤šã€‚è¯´å®è¯æˆ‘è‡ªå·±éƒ½æ•°ä¸æ¸…æ¥šæˆ‘çœ‹äº†å¤šå°‘ï¼ˆç»Ÿè®¡å‡ºæ¥103éƒ¨ï¼‰ï¼Œæˆ‘ä¼šä¸»è¦æŒ‘å‡ éƒ¨å½±å“æ·±åˆ»çš„æ¥åˆ†äº«å’Œæ¨èä¸€ä¸‹ã€‚è€Œä¸”å› ä¸ºBç«™ç›®å‰çš„çœ‹ç•ªç¯å¢ƒï¼Œæˆ‘ç»å¤§éƒ¨åˆ†çœ‹çš„ç•ªå‰§éƒ½æ˜¯åœ¨å…¶ä»–ç½‘ç«™çœ‹çš„ï¼Œè¿™é‡Œç‰¹åˆ«æ„Ÿè°¢ä¸€ä¸‹å„ä½ä¸“æ³¨äºç•ªå‰§çš„æ—¥è¯­å­—å¹•ç»„ï¼Œæˆ‘è‡ªå·±ä¹Ÿæœ‰å­—å¹•ç»„æ‰€ä»¥å¾ˆæ¸…æ¥šå¤§å®¶æ‰€ä»˜å‡ºçš„åŠªåŠ›å’Œè¾›å‹¤åŠ³åŠ¨ï¼Œæ„Ÿè°¢å¤§å®¶ï¼æˆ‘ä¼šæŒ‰ç…§å››ä¸ªå­£åº¦å’Œè¡¥ç•ª5ä¸ªéƒ¨åˆ†æ¥è§£æï¼Œåªä¼šåˆ—ä¸¾éƒ¨åˆ†æˆ‘èƒ½å¤Ÿäºˆä»¥è¯„ä»·å°è±¡æ¯”è¾ƒæ·±åˆ»çš„ç•ªå‰§ï¼Œæ²¡æœ‰æåˆ°çš„å¯èƒ½æ˜¯å› ä¸ºæˆ‘æ²¡æœ‰è¿½å®Œæˆ–è€…ä¸æ˜¯å¾ˆç¬¦åˆæˆ‘çš„ç±»å‹æˆ‘æ²¡åŠæ³•ä½œå‡ºè¯„ä»·ã€‚ 2023å¹´1æœˆæ–°ç•ªæœ¬å­£åº¦æˆ‘çœ‹è¿‡çš„ç•ªå‰§æœ‰16éƒ¨ï¼ˆå…±62éƒ¨å®šæ¡£ï¼Œè§‚çœ‹25.8%ï¼‰ï¼šä¹…ä¿åŒå­¦ä¸æ”¾è¿‡æˆ‘ã€ç½‘è´­æŠ€èƒ½å¼€å¯å¼‚ä¸–ç•Œç¾é£Ÿä¹‹æ—…ã€è½¬ç”Ÿå…¬ä¸»ä¸å¤©æ‰åƒé‡‘çš„é­”æ³•é©å‘½ã€å†°å±æ€§ç”·å­ä¸é…·é…·å¥³åŒäº‹ã€å› ä¸ºå¤ªæ€•ç—›å°±å…¨ç‚¹é˜²å¾¡åŠ›äº† ç¬¬2æœŸã€é—´è°æ•™å®¤ã€åˆ«å½“å“¥å“¥äº†ã€å˜æˆç‹—åè¢«å–œæ¬¢çš„äººæ¡èµ°äº†ã€å†°å‰‘çš„é­”æœ¯å¸ˆå°†è¦ç»Ÿä¸€ä¸–ç•Œã€å‚²å¨‡åæ´¾åƒé‡‘è‰æ´æ´›ç‰¹ä¸å®å†µä¸»è¿œè—¤åŒå­¦åŠè§£è¯´å‘˜å°æ—åŒå­¦ã€è¿›åŒ–ä¹‹å®è¸ä¸Šèƒœåˆ©çš„äººç”Ÿ ç¬¬2æœŸã€å…³äºé‚»å®¶çš„å¤©ä½¿å¤§äººä¸çŸ¥ä¸è§‰æŠŠæˆ‘æƒ¯æˆäº†åºŸäººè¿™ä»¶äº‹ã€å°¼å°”è‡ªåŠ¨äººå½¢ Ver1.1aã€ä¸ºäº†å…»è€é‡‘å»å¼‚ç•Œå­˜å…«ä¸‡é‡‘ã€è™šæ„æ¨ç† ç¬¬2æœŸã€JOJOçš„å¥‡å¦™å†’é™© çŸ³ä¹‹æµ· Part.3ã€æä¸»å¤«é“ ç¬¬2æœŸã€‚ ç¥ä½œ åˆ«å½“å“¥å“¥äº†ï¼ (ãŠå…„ã¡ã‚ƒã‚“ã¯ãŠã—ã¾ã„ï¼) - 5.0&#x2F;5.0 è¿™è¿˜ç”¨è§£é‡Šå—ï¼Œè¿™å·²ç»ä¸æ˜¯æ™®é€šçš„ç•ªå‰§äº†ï¼Œè¿™æ˜¯è‰ºæœ¯å“ï¼Œè¯·å¤§å®¶éƒ½ç•™å­˜ä¸€ä»½BDçè—ã€‚ä»ç¬¬ä¸€é›†åˆ°æœ€åä¸€é›†ï¼Œæ¯ä¸€å¸§ç”»é¢éƒ½é€éœ²ç€â€œæˆ‘æœ‰é’±â€ä»¨å­—ï¼Œè¿™å·²ç»æº¢å‡ºå±å¹•çš„ç»è´¹ã€‚è¯·å¤šæ¥ä¸€äº›è¿™æ ·çš„ä¼˜è´¨è‰ºæœ¯å“å†…å®¹ã€‚é¢˜ææ–¹é¢æ¶‰åŠåˆ°æ€§è½¬ï¼Œå›½å†…å®¡æ ¸åˆ¶åº¦å¯¹è¿™ä¸ªæ–¹é¢è¿‘å‡ å¹´è¿‡äºæ•æ„Ÿæ— æ³•è¿‡å®¡ï¼Œæˆ‘æ˜¯å¾ˆä¸èƒ½æ¥å—çš„ã€‚å“¦ä½ å–æ˜¯æˆ‘è®¤ä¸ºæœ¬å¹´åº¦æœ€ä½³ä¹‹ä¸€ã€‚ æ¨è ä¹…ä¿åŒå­¦ä¸æ”¾è¿‡æˆ‘ - 4.0&#x2F;5.0 çº¯çˆ±ï¼Œç±»ä¼¼é«˜æœ¨åŒå­¦ã€‚ å˜æˆç‹—åè¢«å–œæ¬¢çš„äººæ¡èµ°äº† ï¼ˆ?ï¼‰ - 4.0&#x2F;5.0 è¿™ç©æ„æˆ‘æ²¡æ³•äºˆä»¥è¯„ä»·ã€‚ è™šæ„æ¨ç† ç¬¬2æœŸ - 4.0&#x2F;5.0 æ¨èçœ‹å®Œç¬¬ä¸€å­£å†çœ‹ï¼Œçº¯çˆ±ï¼Œéå¸¸å–œæ¬¢ JOJOçš„å¥‡å¦™å†’é™© çŸ³ä¹‹æµ· Part.3 - 3.5&#x2F;5.0 è¿™ä¸ªæ— éœ€è§£é‡Šã€‚ æä¸»å¤«é“ ç¬¬2æœŸ - 3.5&#x2F;5.0 å¾ˆæœ‰è¶£ï¼Œçœ‹è¿‡å°±æ˜ç™½äº†ï¼Œé»‘é“å¤§ä½¬æ´—ç™½å˜æˆå®¶åº­ä¸»å¤«ã€‚ è¿˜ä¸é”™ ç½‘è´­æŠ€èƒ½å¼€å¯å¼‚ä¸–ç•Œç¾é£Ÿä¹‹æ—… - 3.5&#x2F;5.0 ä¸»è¦æ˜¯åšçš„é¥­å¤ªé¦™äº†æˆ‘æ— æ³•æŠµæŠ—ï¼Œè€Œä¸”è¿™ä¸ªä¸æ»‘å°è¿æ‹›ä¹Ÿå¤ªä¸æ»‘äº†ã€‚ å†°å±æ€§ç”·å­ä¸é…·é…·å¥³åŒäº‹ - 3.5&#x2F;5.0 å‰§æƒ…é£æ ¼æˆ‘å¾ˆå–œæ¬¢ï¼ŒåŠå…¬å®¤çº¯çˆ±å åŠ å¼‚èƒ½ã€‚ ç­”è¾© é—´è°æ•™å®¤ - 1.0&#x2F;5.0 è¿™çº¯çº¯ç­”è¾©ï¼Œæˆ‘å¼ºå¿ç€çœ‹çš„ï¼Œè¿™éƒ½å•¥ç©æ„å‘€ã€‚å‰§æƒ…å®Œå…¨æ²¡æ³•çœ‹ï¼Œä½œç”»è¿˜æ˜¯è›®ä¸é”™çš„ã€‚ ä¸ºäº†å…»è€é‡‘å»å¼‚ç•Œå­˜å…«ä¸‡é‡‘ - 1.0&#x2F;5.0 é€»è¾‘ä½•åœ¨ï¼Ÿæ•´ä½“çš„é€»è¾‘å°±ç¦»è°±ï¼Œå±äºæ˜¯æŠŠè§‚ä¼—çš„æ™ºå•†æŒ‰åœ¨åœ°ä¸Šæ‘©æ“¦ã€‚åŒæ ·ä½œç”»æ—¶è›®ä¸é”™çš„ 2023å¹´4æœˆæ–°ç•ªæœ¬å­£åº¦æˆ‘çœ‹è¿‡çš„ç•ªå‰§æœ‰30éƒ¨ï¼ˆå…±54éƒ¨å®šæ¡£ï¼Œè§‚çœ‹55.6%ï¼‰ï¼šç†Šç†Šå‹‡é—¯å¼‚ä¸–ç•Œ ç¬¬2æœŸã€å¼‚ä¸–ç•Œé‡Œå¾—åˆ°äº†å¼€æŒ‚èˆ¬èƒ½åŠ›çš„æˆ‘ç°å®ä¸–ç•Œä¸­ä¹Ÿä¸¾ä¸–æ— åŒã€ç»Šä¹‹Alleleã€å¸¦ç€æ™ºèƒ½æ‰‹æœºé—¯è¡å¼‚ä¸–ç•Œ ç¬¬2æœŸã€å…¬çˆµçš„å¥‘çº¦æœªå©šå¦»ã€æ”¾å­¦åå¤±çœ çš„ä½ ã€äº¡éª¸æ¸¸æˆã€è·ƒåŠ¨é’æ˜¥ã€THE MARGINAL SERVICEã€ä¸ºç¾å¥½çš„ä¸–ç•ŒçŒ®ä¸Šçˆ†ç„°ã€æ— ç¥ä¸–ç•Œä¸­çš„ç¥æ˜æ´»åŠ¨ã€æˆ‘æ¨çš„å­©å­ã€çŸ³çºªå…ƒ ç¬¬3æœŸã€ç™¾åˆæ˜¯æˆ‘çš„å·¥ä½œã€å‹‡è€…æ­»äº†ã€å›½ç‹æ’å å‹‡æ°”çš„å®ç®±ã€å¯çˆ±è¿‡å¤´å¤§å±æœºã€æ€»ä¹‹å°±æ˜¯éå¸¸å¯çˆ± ç¬¬2æœŸã€å°é¸Ÿä¹‹ç¿¼ é«˜å°”å¤«å°‘å¥³ ç¬¬2æœŸã€å¥³ç¥çš„éœ²å¤©å’–å•¡å…ã€æ±Ÿæˆ·å‰çš„åºŸæŸ´ç²¾çµã€å¤©å›½å¤§é­”å¢ƒã€åœ°ç‹±ä¹ã€å’Œå±±ç”°è¿›è¡ŒLv999çš„æ‹çˆ±ã€æˆ‘å¿ƒé‡Œå±é™©çš„ä¸œè¥¿ã€å¼‚ä¸–ç•Œä¸€å‡»æ— åŒå§å§ã€é‚»äººä¼¼é“¶æ²³ã€ç¬¬äºŒæ¬¡è¢«å¼‚ä¸–ç•Œå¬å”¤ã€è½¬ç”Ÿè´µæ—çš„å¼‚ä¸–ç•Œå†’é™©å½•ã€æˆ‘å®¶çš„è‹±é›„ã€è¿›å‡»çš„å·¨äºº æœ€ç»ˆå­£ å®Œç»“ç¯‡ å‰ç¯‡ã€‚ ç¥ä½œ è·ƒåŠ¨é’æ˜¥ (ã‚¹ã‚­ãƒƒãƒ—ã¨ãƒ­ãƒ¼ãƒ•ã‚¡ãƒ¼) - 4.5&#x2F;5.0 è¿™ä¸ªæ ‡é¢˜çš„ç¿»è¯‘å¾®åå‘äºæ„è¯‘ï¼Œç›´è¯‘çš„è¯æ˜¯è·³è·ƒå’Œä¹ç¦é‹ï¼ˆSkip and Loaferï¼‰ï¼Œè‹±æ–‡å°±æ˜¯è¿™æ ·ç¿»è¯‘çš„ã€‚æ•´ä½“çš„å‰§æƒ…ä¹Ÿæ˜¯é«˜ä¸­ç”Ÿæ´»äº¤ç»‡ç€é’æ˜¥æ‹çˆ±ï¼Œè™½ç„¶å’Œå›½å†…æ™®é«˜çš„ç”Ÿæ´»æœ‰äº›ä¸åŒï¼Œä½†æ˜¯é«˜ä¸€çš„éƒ¨åˆ†æ—¶å…‰å’Œåˆä¸­è¿˜æ˜¯å¯ä»¥åšåˆ°ç±»ä¼¼çš„æƒ…å†µçš„ï¼Œæ˜¯æ¯”è¾ƒå®¹æ˜“äº§ç”Ÿå…±é¸£çš„ç±»åˆ«å’Œå‰§æƒ…äº†å·²ç»ç®—ã€‚è¿™éƒ¨ç•ªå‰§çš„å†…å®¹å±•ç°çš„éå¸¸å…¨é¢ï¼Œå¹¶ä¸æ˜¯å±•ç°ä¸»è§’æœ€å®Œç¾çš„ä¸€é¢ç»™ä½ ï¼Œä¹Ÿä¼šç»™ä½ å±•ç°ä»–ä»¬çœŸå®çš„æ‰€æœ‰æ–¹é¢ã€‚è¿™å°±æ˜¯æˆ‘ä»¬åœ¨é«˜ä¸­é˜¶æ®µæ‰€ç»å†çš„çœŸæ­£çš„ç»å†ï¼Œä»é’æ¶©çš„é’æ˜¥æœŸå’Œç›´æ¥ç›´å»å‘æˆå¹´äººçš„äººæƒ…ä¸–æ•…è½¬å˜ï¼Œå˜å¾—æ›´åŠ æŸ”å’Œã€‚è¿™éƒ¨ç•ªå‰§ä¸­çš„ç”Ÿæ´»å’ŒèŠ‚å¥æ­£å°±æ˜¯æˆ‘ä»¬æ‰€æœŸæœ›çš„çœŸæ­£é«˜ä¸­ç”Ÿæ´»çš„å†™ç…§ï¼Œé™¤äº†å­¦ä¹ å’Œè€ƒè¯•ï¼Œæˆ‘ä»¬ä¹Ÿå¸Œæœ›æœ‰è‡ªå·±çš„è¯¾åç”Ÿæ´»ï¼Œä¸ç®¡æ˜¯å’ŒåŒå­¦ä¸€èµ·å‡ºå»çœ‹ç”µå½±è¿˜æ˜¯åƒé¥­å”±Kï¼Œè¿™éƒ½æ˜¯æˆ‘ä»¬å¿ƒç›®ä¸­æœ€ç†æƒ³é‡Œçš„ç”Ÿæ´»ã€‚å­¦ç”Ÿç”Ÿæ´»å¸¦ç»™æˆ‘ä»¬çš„æ·±åˆ»è®°å¿†ä¸ä»…ä»…æ˜¯ç´§å¼ çš„å­¦ä¹ ç”Ÿæ´»å’Œè€ƒè¯•ï¼Œç”Ÿæ´»ä¸­çš„ç‚¹ç‚¹æ»´æ»´ã€å’Œæœ‹å‹çš„ç›¸å¤„æ›´æ˜¯æˆ‘ä»¬ç”Ÿæ´»ä¸­çš„é‡è¦éƒ¨åˆ†ã€‚ æˆ‘æ¨çš„å­©å­ (æ¨ã—ã®å­) - 4.5&#x2F;5.0 é¦–å…ˆï¼ŒOPå·²ç»è¡€æ´—TikTokäº†ï¼Œä¹Ÿå± äº†æ—¥è¯­æ­Œçš„æ¦œï¼Œç¡®å®éå¸¸å¥½å¬ã€‚å…¶æ¬¡ï¼Œæœ¬ç‰‡åˆåï¼Œçˆ¸çˆ¸å»å“ªäº†ã€‚æ•´ä½“ç»è´¹çˆ†ç‚¸ï¼Œå°¤å…¶ç¬¬ä¸€é›†éå¸¸ç‚¸è£‚ï¼Œå†…å®¹å’Œåˆ¶ä½œéƒ½éå¸¸å®Œç¾ã€‚åŠ¨ç”»å·¥åŠç‰›é€¼ï¼è™½ç„¶åé¢æœ‰å‡ é›†æ¯”è¾ƒå¹³æ·¡ï¼Œä½†æ˜¯æ•´ä½“å†…å®¹å¹¶æ²¡æœ‰ä»€ä¹ˆé—®é¢˜ã€‚åœ¨è¿™éƒ¨ç•ªä¸­ä¹Ÿæ¢è®¨åˆ°äº†éå¸¸é‡å¤§çš„ç½‘ç»œæš´åŠ›é—®é¢˜ï¼Œè¿™å·²ç»æ˜¯éå¸¸å¤šæˆ‘è®¤ä¸ºçš„ç¥ä½œé‡Œä¸æ–­æåˆ°çš„ç‚¹äº†ï¼ˆæ¯”å¦‚é’æ˜¥çŒªå¤´å°‘å¹´ä¸ä¼šé‡åˆ°å…”å¥³éƒå­¦å§ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸ä¸¥é‡çš„é—®é¢˜ã€‚ä¸»è¦çš„å†…å®¹èšé›†åœ¨æ—¥æœ¬çš„å¶åƒåœˆ&#x2F;å¨±ä¹åœˆçš„ä¸€äº›é—®é¢˜ï¼Œæ­éœ²äº†å¾ˆå¤šçš„é—®é¢˜ï¼Œå¯èƒ½ä¼šè®©ä¸€äº›äººä¸æ˜¯å¾ˆæ»¡æ„ã€‚ å¤©å›½å¤§é­”å¢ƒ (å¤©å›½å¤§é­”å¢ƒ) - 5.0&#x2F;5.0 é¦–å…ˆæ˜¯æœ«ä¸–ç”Ÿå­˜ç•ªå’Œç±»ä¼¼äºæ¢¦å¹»å²›çš„æœºæ„å‰§æƒ…ï¼Œåœ¨å¼€å¤´å°±æŠŠå¸å¼•æ‹‰æ»¡äº†è®©äººèƒ½å¤ŸæŒç»­çœ‹ä¸‹å»ã€‚ä¸­é€”é€éœ²äº†å§å§æ˜¯æ€§è½¬ï¼Œåœ¨æœºæ„ä¸­ä¹Ÿç‚¹å‡ºäº†æ¶ˆç­æ€§åˆ«æ¦‚å¿µçš„ç†å¿µï¼Œåœ¨å½“å‰çš„ç¤¾ä¼šä¸‹æˆ‘è§‰å¾—æ˜¯ä¸€ä¸ªéå¸¸å€¼å¾—æ·±å…¥ç ”ç©¶çš„é—®é¢˜ï¼Œç»™å¤§å®¶ä»å‡ºç”Ÿå°±è´´æ ‡ç­¾çš„é€»è¾‘æ˜¯ä»€ä¹ˆï¼Ÿè¿™ç§é€»è¾‘æ˜¯å¦åº”è¯¥é€‚ç”¨äºæ‰€æœ‰åœºæ™¯ï¼ŸåŒæ—¶åˆ¶ä½œä¹Ÿéå¸¸ä¼˜è‰¯ï¼Œç»è´¹æ‹‰æ»¡ã€‚å†…å®¹ä¸­åŒ…å«éƒ¨åˆ†çŒå¥‡å†…å®¹ï¼Œå®³æ€•çš„ä¸è¦çœ‹ä¼šå¼•èµ·ä¸é€‚ã€‚ æˆ‘å¿ƒé‡Œå±é™©çš„ä¸œè¥¿ - 4.0&#x2F;5.0 çº¯çˆ±æ‹çˆ±ç•ªï¼Œå¯¹æ¯”æ”¾å­¦åå¤±çœ çš„ä½ å—ä¼—é¢æ›´å¤§ï¼Œå¤§å¤ªå¤šäº†ã€‚å¼€å¤´ä¸æ˜¯éå¸¸å¸å¼•äººä½†æ˜¯æ…¢æ…¢çœ‹ä¸‹å»èŠ‚å¥æ„Ÿéå¸¸å¥½ï¼Œä½œç”»ä¹Ÿéå¸¸å¥½ã€‚ æ¨è æ”¾å­¦åå¤±çœ çš„ä½  - 4.0&#x2F;5.0 å…¶å®æˆ‘æ˜¯æƒ³ç»™å®ƒç¥ä½œçš„ï¼Œä¸ç®¡ä»ä½œç”»è§’åº¦è¿˜æ˜¯å‰§æƒ…è§’åº¦æˆ‘éƒ½éå¸¸å–œæ¬¢ã€‚åªæ˜¯å› ä¸ºè¿™ä¸ªå­£åº¦çš„ç¥ä½œæœ‰ç‚¹å¤šï¼Œä¸€å¯¹æ¯”è¿™ä¸ªåè€Œæ²¡é‚£ä¹ˆé«˜äº†ã€‚æˆ‘æƒ³ç»™ç¥ä½œå®Œå…¨æ˜¯å› ä¸ºæˆ‘ä¸ªäººæ¯”è¾ƒå–œæ¬¢è¿™ä¸ªç±»åˆ«ï¼Œå®¢è§‚æ¥çœ‹ç¡®å®åœ¨æ¨èè¿™ä¸ªçº§åˆ«ã€‚ä¸»è¦ä¸ºæ‘„å½±å’Œå¤©æ–‡äº¤å‰çš„çº¯çˆ±ç•ªã€‚ç”·å¥³ä¸»çš„å¤±çœ ç›¸é‡å¼€å§‹èµ°åˆ°ä¸€ä¸ªåŒå¥½ä¼šï¼Œå†åˆ°ä¹‹åçš„ä¸€èµ·æ‹ç…§å’Œéœ²è¥ï¼Œæ•´ä½“çš„é£æ ¼éå¸¸å¥½ï¼Œè¿™æ‰æ˜¯é’æ˜¥æ‹çˆ±è¯¥æœ‰çš„æ ·å­ï¼Œæˆ–è€…è¯´ç†æƒ³çš„æ ·å­ã€‚ä¸»çº¿çš„å¤©æ–‡å’Œæ‘„å½±ç›¸å¯¹å°ä¼—ä¸€äº›ï¼ŒèŠ‚å¥è¾ƒä¸ºç¼“æ…¢ä¸é€‚åˆæ‰€æœ‰äººï¼Œæ‰€ä»¥å¯èƒ½æœ‰äº›äººä¸æ˜¯å¾ˆå–œæ¬¢è¿™ä¸ªé£æ ¼ã€‚ ä¸ºç¾å¥½çš„ä¸–ç•ŒçŒ®ä¸Šçˆ†ç„° - 3.5&#x2F;5.0 é˜¿å…‹è¥¿æ–¯æ•™å›¢ä¸‡å²ï¼æ…§æ…§çš„è§’åº¦çš„ç•ªã€‚å°±ç®—æ²¡æœ‰çœ‹è¿‡ä¸ºç¾å¥½çš„ä¸–ç•ŒçŒ®ä¸Šç¥ç¦çš„æœ‹å‹ä»¬ä¹Ÿå¯ä»¥ç›´æ¥çœ‹ï¼Œé˜¿åº“å¨…åªåœ¨æœ€åæ‰å‡ºç°ã€‚ä½œç”»æ–¹é¢æœ‰ä¸€äº›å°é—®é¢˜æ‰€ä»¥æ‰£äº†0.5ã€‚ çŸ³çºªå…ƒ ç¬¬3æœŸ - 4.0&#x2F;5.0 ä¸»è¦æ˜¯æˆ‘ä¸€ç›´éƒ½åœ¨è¿½è¿™ä¸ªï¼Œæˆ‘ä¸ªäººæ˜¯éå¸¸å–œæ¬¢è¿™ä¸ªé£æ ¼çš„ç§‘æ™®ç•ªçš„ï¼Œå†…å®¹å’Œåˆ¶ä½œéƒ½éå¸¸å¥½ï¼Œä¸æ˜¯å•çº¯çš„å„¿ç«¥ç§‘æ™®ï¼Œä¹Ÿæœ‰å¯¹æŠ—ä¸æ‰“æ–—ã€‚ åœ°ç‹±ä¹ - 4.0&#x2F;5.0 æˆ‘å¾ˆçº ç»“è¿™ä¸ªç•ªè¦ä¸è¦æ¨èè¯´å®è¯ã€‚æˆ‘å¯ä»¥ç”¨è¿™ä¸ªæ ‡é¢˜æ¥æ€»ç»“è¿™éƒ¨ç•ªçš„å‰§æƒ…æ–¹é¢å†…å®¹ï¼Œä¸»æ‰“ä¸€ä¸ªåˆåœ°ç‹±åˆä¹ã€‚éƒ¨åˆ†å†…å®¹ç¨å¾®æœ‰ç‚¹çŒå¥‡ï¼Œå®³æ€•çš„æœ‹å‹ä¸è¦çœ‹ï¼Œç¡®å®ä¼šå¼•å‘ä¸é€‚ã€‚MAPPAçš„ä½œç”»çœŸçš„æ˜¯éå¸¸ä¸é”™ï¼Œéå¸¸å¥½ï¼Œå‰§æƒ…æ‹–åè…¿ã€‚ å’Œå±±ç”°è¿›è¡ŒLv999çš„æ‹çˆ± - 4.0&#x2F;5.0 è™½ç„¶å¥³ä¸»ä¸€å¼€å§‹å¾ˆæ‹çˆ±è„‘è¢«æ¸£ç”·æ¸£äº†ï¼Œä½†æ˜¯å¥³ä¸»è¿˜æ˜¯æƒ…å•†å¾ˆé«˜ã€‚æ¯”è¾ƒå¸¸è§„çš„å‘ç³–ç•ªï¼Œç½‘æ¸¸åªæ˜¯ä¸€ä¸ªåª’ä»‹è€Œå·²ã€‚ æˆ‘å®¶çš„è‹±é›„ - 3.5&#x2F;5.0 è¢«ä½œç”»å’Œç»è´¹æ‹–ç´¯çš„å‰§æƒ…ï¼Œå›é€†æœŸå¥³å„¿è¦è¢«é»‘é“å¤§å“¥è¿«å®³ä¹‹å‰ï¼Œè€çˆ¸æŠŠé»‘é“å¤§å“¥ç›´æ¥æ€æ‰ç„¶åæŠ¹æ¸…è¯æ®é€ƒäº¡çš„æ•…äº‹ã€‚å‰§æƒ…éå¸¸ä¸é”™ï¼Œä½œç”»éå¸¸æ‹‰è·¨å’Œå´©åã€‚ è¿˜ä¸é”™ æ— ç¥ä¸–ç•Œä¸­çš„ç¥æ˜æ´»åŠ¨ - 3.0&#x2F;5.0 è¿™ä¸ªæˆ‘å¿…é¡»æ”¾ä¸ªå›¾ï¼Œæˆ‘çœŸå´©ä¸ä½ï¼Œè¿™ç•ªçš„ç»è´¹æ˜¯10å—é’±å—ã€‚å°±ï¼Œä»å‰§æƒ…å’Œå†…å®¹æ¥è¯´æˆ‘æ˜¯æ¨èçš„ï¼Œä½†æ˜¯è¿™ä¸ªä½œç”»ç®€ç›´å°±æ˜¯ä¾æ‰˜ç­”è¾©ï¼Œå†…å®¹éå¸¸çš„ç‚¸è£‚ï¼Œä½ æ°¸è¿œæƒ³ä¸åˆ°ä¸‹ä¸€ç§’å®ƒèƒ½æ•´å‡ºæ¥ä»€ä¹ˆèŠ±æ´»ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºå•¥æˆ‘ç»™äº†è¿™ä¹ˆé«˜çš„è¯„åˆ†è¿˜æ”¾åœ¨è¿˜ä¸é”™é‡Œï¼ˆè¯´å®è¯æˆ‘ç”šè‡³æƒ³æ”¾åˆ°æ¨èä½†æ˜¯è¿™ä¸ªä½œç”»ç¡®å®æœ‰ç‚¹emmmmï¼Œæ€»ä½“æ¥è¯´å’Œå…¶ä»–çš„æ¨èæ²¡æ³•æ¯”ï¼‰ã€‚ æ€»ä¹‹å°±æ˜¯éå¸¸å¯çˆ± ç¬¬2æœŸ - 3.5&#x2F;5.0 ç»­ä½œï¼Œçœ‹çš„æ—¶å€™èººåœ¨åºŠä¸Šå½“è›†å°±å¯ä»¥äº†ï¼Œæ³¨æ„çœ‹å®Œç¬¬ä¸€å­£å†çœ‹è¿™ä¸ªè§‚æ„Ÿä¼šå¥½ä¸€äº›ã€‚ å°é¸Ÿä¹‹ç¿¼ é«˜å°”å¤«å°‘å¥³ ç¬¬2æœŸ - 3.5&#x2F;5.0 è½»ç™¾ï¼Œä½†æˆ‘è§‰å¾—æ›´å¤šçš„æ˜¯æ£‹é€¢å¯¹æ‰‹çš„å‹æƒ…ã€‚è§’è‰²ä¹‹é—´çš„å…³ç³»å¤„ç†æˆ‘ä¸ªäººè§‰å¾—è¿˜æ˜¯éå¸¸ä¸é”™çš„ã€‚å¯ä»¥å½“ä½œä¸€ä¸ªæ­£å¸¸ç»è´¹å……è¶³ï¼Œå‰§æƒ…ä¸é”™çš„ç•ªæ¥çœ‹ã€‚ æ±Ÿæˆ·å‰çš„åºŸæŸ´ç²¾çµ - 3.5&#x2F;5.0 éå¸¸æ…¢èŠ‚å¥çš„æ—¥å¸¸ç•ªï¼ŒåæœŸå†…å®¹ç¨å°¬ã€‚ å•çº¯æƒ³è°ˆè°ˆ ç†Šç†Šå‹‡é—¯å¼‚ä¸–ç•Œ ç¬¬2æœŸ - 3.0&#x2F;5.0 ç»­ä½œï¼Œå› ä¸ºç¬¬ä¸€å­£å¯¹æˆ‘æ¥è¯´å°±å±äºå¹³å¹³æ— å¥‡çš„å¼‚ä¸–ç•Œç±»ï¼Œæ‰€ä»¥è¿™ç»­ä½œæˆ‘ä¹Ÿæ²¡æ³•æ¨èï¼Œå±äºä½ è¦æ˜¯å–œæ¬¢å¥³ä¸»å’Œæ•´ä½“ç”»é£çš„è¯å¯ä»¥ä¸€çœ‹çš„æ°´å¹³ã€‚ äº¡éª¸æ¸¸æˆ - 2.0&#x2F;5.0 åˆ«çœ‹æˆ‘ç»™äº†è¿™ä¹ˆä½çš„è¯„åˆ†ï¼Œä½†æ˜¯æˆ‘è¿˜è¿½å®Œäº†å°±ç¦»è°±ï¼Œçœ‹çš„è¿˜è›®ä»”ç»†çš„ã€‚ä½†ä¸€æ ·çš„é—®é¢˜é¾™å‚²å¤©ç©¿è¶Šï¼Œä¸çŸ¥é“åœ¨å¹²å•¥ï¼Œé€»è¾‘ä¹Ÿå¾ˆå¥‡æ€ªã€‚æƒ³æ”¾åˆ°ç­”è¾©é‡Œï¼Œä½†æ˜¯æ„Ÿè§‰æœ¬å­£åº¦ç­”è¾©ä¹Ÿæœ‰ç‚¹å¤šï¼Œè¿™ä¸ªè¿˜æ²¡é‚£ä¹ˆç­”è¾©å°±æ”¾åˆ°è¿™é‡Œäº†ã€‚ è¾¹ç¼˜æœåŠ¡ (THE MARGINAL SERVICE) - 2.5&#x2F;5.0 é¦–å…ˆï¼Œå‰§æƒ…æœ‰ç‚¹å¥‡æ€ªï¼Œåˆ¶ä½œå’Œç”»é£è¿˜å¯ä»¥ï¼Œä½†æ˜¯æ€»ä½“éå¸¸å°¬ã€‚è¿™æ˜¯ä¸ºæ•°ä¸å¤šæ‰€æœ‰å­—å¹•ç»„éƒ½å¼ƒå‘ç¬¬ä¸€æ—¶é—´æ›´æ–°çš„ç•ªã€‚è¯´å®è¯æˆ‘ä¸€å¼€å§‹çœ‹çš„è¿˜å¯ä»¥ï¼Œä½†æ˜¯è¶Šçœ‹è¶Šæ„Ÿè§‰å¥‡æ€ªï¼Œéå¸¸æ— åŠ›åæ§½äº†å¯ä»¥è¯´ã€‚ ç™¾åˆæ˜¯æˆ‘çš„å·¥ä½œ - 3.0&#x2F;5.0 æœ¬èº«æ˜¯è½»ç™¾ï¼Œå…³ç³»æ–¹é¢å¤„ç†çš„å¹¶ä¸å¥½ã€‚å°¤å…¶æ˜¯åŒå¥³ä¸»çš„è¿™ä¸ªå…³ç³»å¤„ç†ï¼Œæ„Ÿè§‰æœ‰ç‚¹ç”¨åŠ›è¿‡çŒ›ã€‚ç”»é£éå¸¸ä¸é”™ã€‚ å›½ç‹æ’å å‹‡æ°”çš„å®ç®± - 3.0&#x2F;5.0 å¦å¤–ä¸€ä¸ªè§’åº¦ï¼Œä¸»è¦æˆ‘æ¯”è¾ƒå–œæ¬¢æ³¢å‰è¿™ä¸ªè§’è‰²ï¼Œè¿˜æ˜¯æ„Ÿè§‰æ€»ä½“ä¸é”™çš„ã€‚ å¥³ç¥çš„éœ²å¤©å’–å•¡å… - 3.0&#x2F;5.0 ç¦åˆ©ç•ªï¼Œä¸è¦åœ¨æ„å…¶ä»–çš„æ–¹é¢äº†ã€‚ é‚»äººä¼¼é“¶æ²³ - 3.5&#x2F;5.0 æˆ‘ä¸ªäººéå¸¸å–œæ¬¢è¿™éƒ¨ç•ªå¸¦æ¥çš„æ„Ÿè§‰ï¼Œçº¯çˆ±ç•ªéå¸¸çš„æ…¢èŠ‚å¥ã€‚æŠŠç”·å¥³ä¸»ä¹‹é—´çš„å…³ç³»é€»è¾‘åŠä¸¤ä¸ªå®¶åº­ä¹‹é—´çš„å…³ç³»æ¢³ç†å¾—éå¸¸å¥½ã€‚æ…¢æ…¢çœ‹æ˜¯éå¸¸å¥½çš„ç•ªï¼Œå¾ˆå¤šå‘ç³–å’Œç£•ç‚¹ã€‚ è¿›å‡»çš„å·¨äºº æœ€ç»ˆå­£ å®Œç»“ç¯‡ å‰ç¯‡ - 3.5&#x2F;5.0 æˆ‘ä¸ªäººä¸æ˜¯å¾ˆå¥½è¯„ä»·è¿™ä¸ªåå­—è´¼é•¿è´¼éš¾è®°çš„æœ€ç»ˆå­£ã€‚å·¨äººåå¹´ï¼Œä»£è¡¨çš„æ˜¯æˆ‘ä»¬è¿™ä»£è¿½ç•ªäººçš„åå¹´ï¼Œå‰§æƒ…æ¯”è¾ƒå¥‡æ€ªï¼Œæ‰€æœ‰äººéƒ½åœ¨å¿æ‚”æ„Ÿè§‰ï¼Œæ›´å¤šçš„æ˜¯ç»™æˆ‘ä»¬è‡ªå·±çš„ä¸€ä¸ªçºªå¿µï¼Œä»£è¡¨æˆ‘ä»¬çš„åå¹´ã€‚ ç­”è¾© å¼‚ä¸–ç•Œé‡Œå¾—åˆ°äº†å¼€æŒ‚èˆ¬èƒ½åŠ›çš„æˆ‘ç°å®ä¸–ç•Œä¸­ä¹Ÿä¸¾ä¸–æ— åŒ - 2.0&#x2F;5.0 ç»è´¹æ‹‰æ»¡ï¼ŒMAPPAä½œç”»é¡¶çº§ã€‚ä½†æ˜¯è¿™ä¸ªå‰§æƒ…çœŸçš„æˆ‘æ— åŠ›åæ§½ï¼Œæ­»è‚¥å®…å˜å¸…å“¥ï¼Œå¥³ç¥æŠ•æ€€é€æŠ±ã€‚2023å¹´æ¥è¿˜æœ‰è¿™æ ·çš„å•çº¸ã€‚å¦‚æœå–œæ¬¢é¾™å‚²å¤©çš„å¯ä»¥çœ‹ã€‚ ç»Šä¹‹Allele - 0.5&#x2F;5.0 ï¼Ÿï¼Ÿï¼Ÿè¿™å°±æ˜¯æˆ‘çš„è¯„ä»·ã€‚ä½ åœ¨å¼€ç©ç¬‘å—çˆ±é…±éƒ½ä¼‘çœ äº†ï¼Œè¿˜åœ¨è¿™æ¶ˆè´¹å‘¢ï¼Ÿ å¸¦ç€æ™ºèƒ½æ‰‹æœºé—¯è¡å¼‚ä¸–ç•Œ ç¬¬2æœŸ - 2.0&#x2F;5.0 ä»ç¬¬ä¸€å­£å¼€å§‹å°±æ˜¯å•çº¸ï¼Œè¿™ç¬¬äºŒå­£å½“ç„¶è¿˜æ˜¯ã€‚è¯´å®è¯æˆ‘éƒ½ä¸èƒ½ç†è§£è¿™æ˜¯æ€ä¹ˆå‡ºçš„ç¬¬äºŒå­£ã€‚ å‹‡è€…æ­»äº† - 2.0&#x2F;5.0 ?ä¸Šæ¥æˆ‘å°±æ²¡ç»·ä½ï¼Œå°±ç¦»è°±è¿™å•¥å‰§æƒ…ï¼Œæ ¹æœ¬çœ‹ä¸ä¸‹å»ã€‚ ç¬¬äºŒæ¬¡è¢«å¼‚ä¸–ç•Œå¬å”¤ - 2.0&#x2F;5.0 ç¬¬äºŒæ¬¡ï¼Ÿso whatï¼Ÿä¹ æƒ¯äº†ï¼Ÿå›¾å•¥ï¼Ÿå¼‚ä¸–ç•Œå•çº¸ã€‚ è½¬ç”Ÿè´µæ—çš„å¼‚ä¸–ç•Œå†’é™©å½• - 2.5&#x2F;5.0 å¼‚ä¸–ç•Œå•çº¸ï¼Œé¾™å‚²å¤©ï¼Œåˆ¶ä½œè¿˜å¯ä»¥ï¼Œæ„¿æ„çœ‹çš„å¯ä»¥çœ‹ã€‚ 2023å¹´7æœˆæ–°ç•ªæœ¬å­£åº¦æˆ‘çœ‹è¿‡çš„ç•ªå‰§æœ‰24éƒ¨ï¼ˆå…±53éƒ¨å®šæ¡£ï¼Œè§‚çœ‹44.4%ï¼‰ï¼šç”œç‚¹è½¬ç”Ÿã€æ»¡æ€€ç¾æ¢¦çš„å°‘å¹´æ˜¯ç°å®ä¸»ä¹‰è€…ã€å–œæ¬¢çš„äººå¿˜è®°æˆ´çœ¼é•œäº†ã€è½¬ç”Ÿæˆè‡ªåŠ¨è´©å–æœºçš„æˆ‘ä»Šå¤©ä¹Ÿåœ¨è¿·å®«å¾˜å¾Šã€æˆ‘çš„å¹¸ç¦å©šå§»ã€ä¸æ­»å°‘å¥³ æ€äººç¬‘å‰§ã€å’’æœ¯å›æˆ˜ ç¬¬2å­£ã€BanG Dream! Itâ€™s MyGO!!!!!ã€åœ£è€…æ— åŒã€é—´è°æ•™å®¤ ç¬¬2å­£ã€ç§Ÿå€Ÿå¥³å‹ ç¬¬3å­£ã€AIç”µå­åŸºå› ã€èƒ½å¹²çš„çŒ«ä»Šå¤©ä¹Ÿå¿§éƒã€è±èçš„ç‚¼é‡‘å·¥æˆ¿ã€AYAKAã€å…¬å¸é‡Œçš„å°å°å‰è¾ˆã€å…¶å®æˆ‘ä¹ƒæœ€å¼ºï¼Ÿã€è™½ç„¶ç­‰çº§åªæœ‰1çº§ä½†å›ºæœ‰æŠ€èƒ½æ˜¯æœ€å¼ºçš„ã€åº™ä¸å¯è¨€ã€é»‘æš—é›†ä¼šã€åƒµå°¸100ï¼šåœ¨æˆä¸ºåƒµå°¸å‰è¦åšçš„100ä»¶äº‹ã€æ— èŒè½¬ç”Ÿ ç¬¬2å­£ã€æ­»ç¥å°‘çˆ·ä¸é»‘å¥³ä»† ç¬¬2å­£ã€è‹±é›„æ•™å®¤ã€‚ ç¥ä½œ BanG Dream! Itâ€™s MyGO!!!!! - 4.0&#x2F;5.0 é‚¦é‚¦äººï¼Œé‚¦é‚¦é­‚ï¼å•çº¯å› ä¸ºè¿™ä¸ªå­£åº¦å®åœ¨æœ‰ç‚¹æ‹‰ï¼Œèƒ½æŠŠé‚¦é‚¦æ¨ä¸Šæ¥ï¼Œæœ¬æ¥æ˜¯æˆ‘ä»¬å°ä¼—äººçš„å¿«ä¹ã€‚ æ¨è æˆ‘çš„å¹¸ç¦å©šå§» - 4.0&#x2F;5.0 æˆ‘ä¸ªäººå¾ˆå–œæ¬¢ï¼Œæ¯”è¾ƒéœ¸æ€»å® å¦»çš„æ„Ÿè§‰ï¼Œçº¯çˆ±ï¼Œè›®å¥³æ€§å‘çš„ä¸€ä¸ªç•ªã€‚ç”·ä¸»æƒ…å•†å¾ˆé«˜ï¼Œå¥³ä¸»å› ä¸ºè¢«åŸç”Ÿå®¶åº­PUAçš„å¿ƒç†å·²ç»æœ‰é—®é¢˜äº†ä¸èƒ½åƒæ­£å¸¸äººæ€è€ƒã€‚ ä¸æ­»å°‘å¥³ æ€äººç¬‘å‰§ - 3.5&#x2F;5.0 å¾ˆæ— å˜å¤´çš„å¼€å±€ï¼Œä½†æ˜¯ä¹‹åçš„å‰§æƒ…è¿˜å¯ä»¥ï¼Œé™¤äº†éƒ¨åˆ†åœ°æ–¹æ¯”è¾ƒå¥‡æ€ªï¼Œæ¯”å¦‚å¸è¡€é¬¼çˆ¸çˆ¸çŸ¥é“è‡ªå·±çš„å„¿å­æ˜¯æ€äººçŠ¯ç›´æ¥æŠŠå„¿å­ç»™æ´»æ´»æ‰“æ­»ã€‚å‰§æƒ…èŠ‚å¥æŠŠæ§ä¸é”™ï¼Œä¸å¿«ä¸æ…¢ã€‚ä½œç”»ä¼˜è‰¯ã€‚ ç§Ÿå€Ÿå¥³å‹ ç¬¬3å­£ - 3.5&#x2F;5.0 ä¸€ç›´è¿½çš„å¯ä»¥ç»§ç»­çœ‹ï¼Œä¸»æ‰“ä¸€ä¸ªèˆ”ç‹—ç”·ä¸»ã€‚ èƒ½å¹²çš„çŒ«ä»Šå¤©ä¹Ÿå¿§éƒ - 3.5&#x2F;4.0 ä¸€å¼€å§‹çš„è¿é•œå¤ªå¥‡æ€ªäº†ï¼Œå’Œæ— äººæœºæ‹æ‘„ä¸€æ ·çš„æ„Ÿè§‰ï¼Œéå¸¸å¥‡æ€ªã€‚ä¸»é¢˜å¾ˆå¥½å®¶é‡Œåˆä¸€æ”¯ä¸ç”¨èŠ±é’±è¿˜ç»™å½“ä¿å§†çš„çŒ«ã€‚å¾ˆæ—¥å¸¸çš„æ—¥å¸¸ç•ªã€‚ç”»é£å’ŒKç±»ä¼¼ã€‚ åƒµå°¸100ï¼šåœ¨æˆä¸ºåƒµå°¸å‰è¦åšçš„100ä»¶äº‹ - 4.0&#x2F;5.0 å¼€å¤´æœ‰ä¸‘åŒ–å¥³æ€§çš„é—®é¢˜ã€‚ç¨å¾®æœ‰ç‚¹çˆ½ç•ªæ„Ÿè§‰ï¼Œé€‚åˆç¤¾ç•œçœ‹ã€‚é«˜åˆ†æ˜¯å› ä¸ºä½œç”»éå¸¸å¼ºã€‚ æ­»ç¥å°‘çˆ·ä¸é»‘å¥³ä»† ç¬¬2å­£ - 4.0&#x2F;5.0 ç›´æ¥å¼€å§‹åœ¨åºŠä¸Šåƒè›†ä¸€æ ·æ‰­å°±å¥½äº†ï¼Œå‘ç³–ï¼Œç”œã€‚ è¿˜ä¸é”™ è½¬ç”Ÿæˆè‡ªåŠ¨è´©å–æœºçš„æˆ‘ä»Šå¤©ä¹Ÿåœ¨è¿·å®«å¾˜å¾Š - 3.0&#x2F;5.0 æ—¥æœ¬äººçš„XPç»ˆäºå‘å¥‡æ€ªçš„æ–¹å‘å‘å±•äº†ï¼Œå‰§æƒ…æ–¹é¢è¿˜æ˜¯è›®ä¸é”™çš„ï¼Œå¾ˆæ–°é¢–è›®æœ‰æ„æ€ã€‚ è±èçš„ç‚¼é‡‘å·¥æˆ¿ - 3.0&#x2F;5.0 æ¸¸æˆå®£ä¼ ç±»çš„ç•ªï¼Œå‰§æƒ…æ¯”è¾ƒä¸€èˆ¬ï¼Œä¸»è¦æ˜¯ç¦åˆ©å§ï¼Œæ›´å¤šæ˜¯çœ‹è…¿ã€‚ å…¬å¸é‡Œçš„å°å°å‰è¾ˆ - 3.0&#x2F;5.0 P9çš„ç”»é£ä¸é”™ï¼Œå‰§æƒ…æ¯”è¾ƒå¥—è·¯ã€‚ é»‘æš—é›†ä¼š - 3.0&#x2F;5.0 ææ€–åŠ¨æ¼«é¢†åŸŸï¼Œç¨å¾®æœ‰ç‚¹å°ææ€–ã€‚ æ— èŒè½¬ç”Ÿ ç¬¬2å­£ - 3.0&#x2F;5.0 æŠ–Mç”·ä¸»ï¼ŒèŠ‚å¥æœ‰ç‚¹æ…¢è¿™ä¸€å­£ã€‚ å•çº¯æƒ³è°ˆè°ˆ ç”œç‚¹è½¬ç”Ÿ - 2.5&#x2F;5.0 ä¸æ˜¯å¾ˆå¥½è¯„ä»·è¿™ä¸ªä¸œè¥¿ï¼Œæœ¬æ¥æƒ³çœ‹ç”œç‚¹å¸ˆå’Œåˆ¶ä½œçš„ï¼Œå¥½å®¶ä¼™è¿™åŸºæœ¬ä¸Šæ²¡æœ‰å‘€ã€‚åˆæ˜¯é¾™å‚²å¤©å¼‚ä¸–ç•Œè½¬ç”Ÿã€‚ æ»¡æ€€ç¾æ¢¦çš„å°‘å¹´æ˜¯ç°å®ä¸»ä¹‰è€… - 2.5&#x2F;5.0 ä¸€å¼€å§‹ç”·ä¸»æ˜¯èˆ”ç‹—ï¼Œå¥³ä¸»ä¸ç†äººï¼Œçªç„¶æŸå¤©ä¸èˆ”äº†ï¼Œå¥³ä¸»åè€Œæ¥äº†ã€‚emmmmå°±é…±ã€‚ åœ£è€…æ— åŒ - 2.5&#x2F;5.0 è¿˜ä¸é”™çš„å¼‚ä¸–ç•Œå•çº¸ï¼Œç”·ä¸»äººè®¾è‰°è‹¦å¥‹æ–—ã€‚ AIç”µå­åŸºå›  - 3.0&#x2F;5.0 æ²¡æ³•æ¨èçš„åŸå› æ˜¯è¿™ä¸ªå‰§æƒ…æ¯”è¾ƒå¥‡æ€ªï¼Œæƒ³è¦è°ˆè®ºAI&#x2F;æœºå™¨äººä¸äººç±»ä¹‹é—´çš„å…³ç³»ï¼Œä½†æ˜¯ç”¨åŠ›å¾ˆå¥‡æ€ªï¼Œå‰§æƒ…ä¹Ÿå¾ˆå¥‡æ€ªã€‚å…³æ³¨ç‚¹æ˜¯å¥½çš„ã€‚ AYAKA - 3.0&#x2F;5.0 å‰§æƒ…è¿˜å¯ä»¥ï¼Œå°±æ˜¯é€»è¾‘ç¨å¾®å¥‡æ€ªç‚¹ï¼Œç”»é£ä¸é”™ï¼ŒçœŸæ˜¯AYAKAå‘€ã€‚ åº™ä¸å¯è¨€ - 3.0&#x2F;5.0 ç¦åˆ©ç•ªã€‚ç”»é£è¿˜å¯ä»¥ã€‚ ç­”è¾© é—´è°æ•™å®¤ ç¬¬2æœŸ - 2.5&#x2F;5.0 æ²¡æƒ³åˆ°èƒ½å‡ºç¬¬äºŒå­£ï¼Œè™½ç„¶æ¯”ç¬¬ä¸€å­£å¥½ï¼Œä½†æ˜¯ç¬¬ä¸€å­£å®åœ¨æ˜¯ä¸è¡Œï¼Œå½±å“åˆ°æˆ‘å¯¹ç¬¬äºŒå­£çš„è¯„ä»·äº†ã€‚ å…¶å®æˆ‘ä¹ƒæœ€å¼ºï¼Ÿ-2.5&#x2F;5.0 å¼‚ä¸–ç•Œå•çº¸ï¼Œç”·ä¸»æ•°å€¼å¤ªé«˜æµ‹ä¸å‡ºæ¥è¢«å›½ç‹ç‹åæŠ›å¼ƒï¼Œè¢«è¾¹å¢ƒè´µæ—æ”¶å…»ï¼Œä¹‹åå„ç§è‹±è—å®åŠ›å’Œâ€œæ— æ„â€æš´éœ²å®åŠ›ã€‚ è‹±é›„æ•™å®¤ - 2.5&#x2F;5.0 è¿™ç©æ„ï¼Œæœ‰ç‚¹è„‘æº¢è¡€ï¼Œç”·ä¸»æƒ…å•†è¿‡ä½ï¼Œä¸åœåœ¨æ”¶åå®«æ„Ÿè§‰ã€‚ 2023å¹´10æœˆæ–°ç•ªæœ¬å­£åº¦æˆ‘çœ‹è¿‡çš„ç•ªå‰§æœ‰26éƒ¨ï¼ˆå…±76éƒ¨å®šæ¡£ï¼Œè§‚çœ‹35.5%ï¼‰ï¼šç±³å¥‡ä¸è¾¾åˆ©ã€é¸­ä¹ƒæ¡¥è®ºçš„ç¦å¿Œæ¨ç†ã€æˆ‘æ¨æ˜¯åæ´¾å¤§å°å§ã€äº¡éª¸æ¸¸æˆPart.2ã€æ˜Ÿçµæ„Ÿåº”ã€åœ£å¥³é­”åŠ›æ— æ‰€ä¸èƒ½ ç¬¬2å­£ã€BULLBUSTERã€æ¡åˆ°è¢«é€€å©šå¤§å°å§çš„æˆ‘æ•™ä¼šå¥¹åšååçš„äº‹ã€16bitçš„æ„ŸåŠ¨ã€æƒ³å½“å†’é™©è€…å‰å¾€éƒ½å¸‚çš„å¥³å„¿å‡åˆ°äº†Sçº§ã€å¤§å°å§å’Œçœ‹é—¨çŠ¬ã€çŸ³çºªå…ƒ ç¬¬3å­£ Part.2ã€è‘¬é€çš„èŠ™è‰è²ã€ç»éªŒä¸°å¯Œçš„ä½ å’Œç»éªŒä¸ºé›¶çš„æˆ‘äº¤å¾€çš„æ•…äº‹ã€ä¸æ­»ä¸å¹¸ã€å¥³å‹æˆå † ç¬¬2å­£ã€å®¶é‡Œè¹²å¸è¡€å§¬çš„è‹¦é—·ã€é—´è°è¿‡å®¶å®¶ ç¬¬2å­£ã€çŒªè‚å€’æ˜¯çƒ­çƒ­å†åƒå•Šã€æˆ‘ä»¬çš„é›¨è‰²åè®®ã€è¯å±‹å°‘å¥³çš„å‘¢å–ƒã€é¦™æ ¼é‡Œæ‹‰Â·å¼€æ‹“å¼‚å¢ƒï½ç²ªä½œçŒæ‰‹æŒ‘æˆ˜ç¥ä½œï½ã€OVERTAKEã€MF Ghostæé€Ÿè½¦é­‚ã€æš´é£Ÿçš„å·´è¨å¡ã€è¶…è¶…è¶…è¶…è¶…å–œæ¬¢ä½ çš„100ä¸ªå¥³æœ‹å‹ã€è¿›å‡»çš„å·¨äºº æœ€ç»ˆå­£ å®Œç»“ç¯‡ åç¯‡ã€‚ ç¥ä½œ è‘¬é€çš„èŠ™è‰è² (è‘¬é€ã®ãƒ•ãƒªãƒ¼ãƒ¬ãƒ³) - 5.0&#x2F;5.0 çœŸçš„éœ€è¦è§£é‡Šå—ï¼Œå¤§å®¶çœ‹ç•ªæ˜¯ä¸ºäº†ä»€ä¹ˆï¼Œä¸å°±æ˜¯ä¸ºäº†å®ƒå—ã€‚é™¤äº†opæœ‰ç‚¹ä¸å¤§åŒ¹é…ï¼Œå†…å®¹å®Œå…¨æŒ‘ä¸å‡ºæ¥ç‘•ç–µã€‚å‰§æƒ…é¡¶çº§ã€åˆ¶ä½œé¡¶çº§ã€é…ä¹é¡¶çº§ã€‚ä¸»çº¿èŠ™è‰è²åœ¨æ¼«é•¿çš„å²æœˆä¸­çœ‹åˆ°äº†äººç±»ç”Ÿå‘½è™½çŸ­æš‚ä½†æ˜¯æœ‰è‡ªå·±çš„æ„ä¹‰ã€‚äººç±»å¥‹æ–—ä¸€ç”Ÿå³ä½¿åœ¨å½“æ—¶å…¨ä¸–ç•Œéƒ½çŸ¥é“ä½†æ˜¯éšç€æ—¶é—´çš„æ¨ç§»è¿˜æ˜¯ä¼šè¢«äººç±»å¿˜å´çš„äº‹å®ã€‚å¯¿å‘½å’Œç”Ÿå­˜çš„æ„ä¹‰å¸¦ç»™äº†æˆ‘ä»¬æ·±åˆ»çš„æ€è€ƒã€‚èŠ™è‰è²æœ€å–œæ¬¢çš„èŠ±çš„é­”æ³•ï¼Œä¹ŸåŒæ—¶æ˜¯è€å¸ˆæœ€å–œæ¬¢çš„é­”æ³•ï¼Œä¹Ÿæ˜¯è¾›ç¾å°”æœ€å–œæ¬¢çš„é­”æ³•ã€‚ é—´è°è¿‡å®¶å®¶ ç¬¬2å­£ (SPYÃ—FAMILY Season 2) - 4.5&#x2F;5.0 å“‡åº“å“‡åº“ï¼Œå¯çˆ±æ˜¯æ— æ•Œçš„ã€‚ æ¨è ç±³å¥‡ä¸è¾¾åˆ© - 3.5&#x2F;5.0 åŒç”·ä¸»å’Œé¢†å…»çš„çˆ¶æ¯çš„æ•…äº‹ã€‚æ­éœ²äº†å­¤å„¿çš„ä¸€äº›é—®é¢˜ï¼ŒåŸç”Ÿå®¶åº­çš„ç¼ºå¤±ç»™å­©å­ä»å°é€ æˆæ— æ³•ç£¨ç­çš„å¿ƒç†é—®é¢˜éå¸¸ä¸¥é‡ã€‚æˆ‘ä¸ªäººçœ‹çš„ç¨å¾®æœ‰ç‚¹çª’æ¯æ„Ÿå’Œæ— æ¯”çš„å¿ƒç–¼ï¼Œæ‰€ä»¥è¯„åˆ†ç»™ä¸åˆ°å¤ªé«˜ã€‚ é¸­ä¹ƒæ¡¥è®ºçš„ç¦å¿Œæ¨ç† - 3.5&#x2F;5.0 ä¾¦æ¢æ¨ç†ç•ªï¼Œä»è¿™ä¸ªè§’åº¦æ¥è¯´éå¸¸å¥½çœ‹ã€‚ä½†æ˜¯æœ‰ä¸€ä¸ªé—®é¢˜å°±æ˜¯å¼‚èƒ½æ¯”è¾ƒéš¾ç†è§£é€»è¾‘ä¸Šçš„è”ç³»å’Œå®šä½ã€‚ æˆ‘æ¨æ˜¯åæ´¾å¤§å°å§ - 3.5&#x2F;5.0 è½¬ç”Ÿä¹™å¥³æ¸¸æˆï¼Œé¢˜ææŒºå¸¸è§ï¼Œè½»ç™¾ï¼Œå¥³ä¸»æŠ–Mï¼Œæ”»ç•¥æŠ–Sã€‚å‰§æƒ…éå¸¸å¥½çš„åœ°æ–¹å°±æ˜¯æ‰“ç›´çƒï¼Œæ²¡é‚£ä¹ˆå¤šä¹±ä¸ƒå…«ç³Ÿçš„å¥—è·¯ï¼Œçœ‹ç€å¾ˆèˆ’æœã€‚ 16bitçš„æ„ŸåŠ¨ - 3.5&#x2F;5.0 å®é™…ä¸Šæœ‰ç‚¹çº ç»“æ˜¯æ¨èè¿˜æ˜¯è¿˜ä¸é”™ï¼Œå¥³ä¸»æœ‰äº›æ—¶å€™ä¸ç†æ™ºï¼Œè€Œä¸”å¤–æ˜Ÿäººçš„åŠ å…¥æœ‰ç‚¹å¥‡æ€ªã€‚è¯´å®è¯å°±æ˜¯åœ¨èåˆå„ç§å…ƒç´ åˆ°ä¸€èµ·è¿™éƒ¨ç•ªã€‚ çŸ³çºªå…ƒ ç¬¬3å­£ Part.2 - 4.0&#x2F;5.0 ä¾æ—§æ¨èï¼Œç†ç”±å’ŒPart 1ä¸€æ ·ã€‚ ä¸æ­»ä¸å¹¸ - 4.0&#x2F;5.0 æ‰“æ–—åœºé¢éå¸¸å¥½ï¼Œå‰§æƒ…ä¹Ÿéå¸¸ä¸é”™ï¼Œä¸æ‹–æ²“ä¸é¾™å‚²å¤©ã€‚æœ‰ç‚¹ä½ çš„è‹±é›„å­¦é™¢çš„æ„Ÿè§‰ã€‚ è¯å±‹å°‘å¥³çš„å‘¢å–ƒ - 3.5&#x2F;5.0 æ—¥æœ¬äººçœ¼ä¸­çš„ä¸­å›½åå®«ã€‚ç»†èŠ‚å’Œä¸­æ–‡æ–¹é¢æœ‰ä¸€ç§ç»™æ„å¤§åˆ©äººåƒè èæŠ«è¨çš„æ„Ÿè§‰ï¼Œå°±ï¼Œè¿™æ˜¯ä¸­å›½åå®«å—ï¼Œä½ å®¶ä¸­æ–‡æ˜¯è¿™ä¹ˆå‘éŸ³çš„ã€‚æŠ›å¼€è¿™éƒ¨åˆ†ä¸è°ˆï¼Œå‰§æƒ…å®‰æ’æ¯”è¾ƒæœ‰è¶£ï¼ˆå½“ç„¶ä¹Ÿæœ‰æ¼æ´ï¼Œæ„Ÿè§‰å¥³ä¸»æ¥åå®«æ–­æ¡ˆæ¥äº†ï¼‰ï¼Œæ²¡æœ‰ä»€ä¹ˆåå®«çš„å‹¾å¿ƒæ–—è§’ã€‚ç½‘é£åšçš„ï¼Œç”»é£åˆ¶ä½œéƒ½å¾ˆæ£’ã€‚ é¦™æ ¼é‡Œæ‹‰Â·å¼€æ‹“å¼‚å¢ƒï½ç²ªä½œçŒæ‰‹æŒ‘æˆ˜ç¥ä½œï½ - 3.5&#x2F;5.0 MMORPGç±»å‹çš„ç•ªï¼Œå†…å®¹å‰§æƒ…åˆ¶ä½œéƒ½å¾ˆä¸é”™ã€‚é¢˜ææ²¡ä»€ä¹ˆæ–°æ„ä½†æ˜¯æ— è¿‡ã€‚ è¶…è¶…è¶…è¶…è¶…å–œæ¬¢ä½ çš„100ä¸ªå¥³æœ‹å‹ - 3.5&#x2F;5.0 è™½ç„¶æ˜¯åå®«ï¼Œä½†æ˜¯è¿™ä¸ªè®¾å®šè¿‡äºé€†å¤©ï¼Œè¿™ç©æ„100ä¸ªåå®«è¿˜ä¸å…šäº‰çš„å¯è¿˜è¡Œã€‚ è¿˜ä¸é”™ æ˜Ÿçµæ„Ÿåº” - 3.0&#x2F;5.0 èŠ³æ–‡ç¤¾è½»ç™¾æ—¥é•¿é¢˜æï¼Œå› ä¸ºå¼€å¤´æœ‰ç‚¹å°å°¬æˆ‘æ²¡å¤§çœ‹ä¸‹å»ï¼Œä¹‹åæœ‰æœºä¼šè¡¥ä¸€ä¸‹åº”è¯¥ä¼šå¥½ä¸å°‘ã€‚èŠ³æ–‡ç¤¾ä¸ä¼šè®©å¤§å®¶åœ¨èŒè±šå¤±æœ›çš„ã€‚ æ¡åˆ°è¢«é€€å©šå¤§å°å§çš„æˆ‘æ•™ä¼šå¥¹åšååçš„äº‹ - 3.0&#x2F;5.0 ç¨å¾®æœ‰ç‚¹æ— å˜å¤´ã€‚ç”»é£å¾ˆä¸é”™ï¼Œè®¾å®šå¾ˆæœ‰æ„æ€ã€‚ç”·ä¸»å±äºéœ¸é“æ€»è£ï¼Œéå¸¸ä¸é”™ã€‚æœ‰å‘ç³–ã€‚ ç»éªŒä¸°å¯Œçš„ä½ å’Œç»éªŒä¸ºé›¶çš„æˆ‘äº¤å¾€çš„æ•…äº‹ - 3.0&#x2F;5.0 å¥³ä¸»çš„è¡Œä¸ºç¨å¾®æœ‰ç‚¹ä¸æ˜¯å¾ˆå¥½è¯„ä»·è¯´å®è¯ï¼Œä¹Ÿæ²¡ä¹±æï¼Œåœ¨äº¤å¾€æœŸé—´ä¹Ÿå¾ˆä¸“ä¸€ï¼Œä½†è¿˜æ˜¯äº¤å¾€å¾ˆå¤šäººè€Œä¸”æ²¡æœ‰ä»€ä¹ˆåº¦å¯è¨€ã€‚ å•çº¯æƒ³è°ˆè°ˆ äº¡éª¸æ¸¸æˆPart.2 - 2.0&#x2F;5.0 ç»­ä½œï¼Œä½†æ˜¯è¿™æ¬¡æˆ‘çœ‹ä¸å¤§ä¸‹å»äº†ï¼Œæ‰€ä»¥è¿™ä¸ªç”šè‡³ä¸å¦‚ä¸ŠåŠæ®µã€‚ åœ£å¥³é­”åŠ›æ— æ‰€ä¸èƒ½ ç¬¬2å­£ - 2.5&#x2F;5.0 ç»­ä½œï¼Œç¬¬ä¸€å­£çš„æ—¶å€™å¥³ä¸»è¢«å¬å”¤è¿‡æ¥å› ä¸ºè¢«è®¤ä¸ºä¸æ˜¯åœ£å¥³å°±è¢«æ™¾åœ¨ä¸€è¾¹ï¼Œå¹¸äºæœ‰å¥½å¿ƒäººæ¥å—è®©å¥³ä¸»æœ‰äº†å¾…çš„åœ°æ–¹ã€‚ç¬¬äºŒå­£èŠ‚å¥æ›´æ…¢ï¼Œæœ‰ç‚¹emmmã€‚ è¿›å‡»çš„å·¨äºº æœ€ç»ˆå­£ å®Œç»“ç¯‡ åç¯‡ - 1.0&#x2F;5.0 ä¸ºå•¥ä¸æ˜¯ç­”è¾©å‘¢ï¼Œå› ä¸ºå†æ€ä¹ˆæ ·ä¹Ÿæ˜¯ç»™10å¹´çš„æƒ…æ€€ç”»ä¸Šäº†å¥å·ï¼Œè™½ç„¶çœ‹å®Œæ„Ÿè§‰è¢«å–‚äº†ä¸€å£ç­”è¾©ä½†æ˜¯è¿˜å¾—åƒä¸‹å»çš„æ„Ÿè§‰ã€‚ ç­”è¾© æƒ³å½“å†’é™©è€…å‰å¾€éƒ½å¸‚çš„å¥³å„¿å‡åˆ°äº†Sçº§ - 2.0&#x2F;5.0 å•çº¸ï¼Œéšä¾¿çœ‹çœ‹å¯ä»¥ã€‚æ²¡ä»€ä¹ˆå®é™…æ€§å†…å®¹ã€‚ å¤§å°å§å’Œçœ‹é—¨çŠ¬ - 1.0&#x2F;5.0 ï¼Ÿå•¥ç©æ„è¿™æ˜¯ï¼Ÿ çŒªè‚å€’æ˜¯çƒ­çƒ­å†åƒå•Š - 2.0&#x2F;5.0 æ¯çŒªçš„äº§åæŠ¤ç†ã€‚ æˆ‘ä»¬çš„é›¨è‰²åè®® - 0.0&#x2F;5.0 æˆ‘çœ‹ç•ªè¿™ä¹ˆå¤šå¹´ï¼Œç¬¬ä¸€æ¬¡å¯¹ä¸€éƒ¨ç•ªæ²¡æœ‰ä¸€æ»´å¥½æ„Ÿã€‚ æš´é£Ÿçš„å·´è¨å¡ - 2.0&#x2F;5.0 å¼€å¤´è¿˜æœ‰ç‚¹æ„æ€ï¼Œè¶Šå¾€åè¶Šæ²¡æ„æ€ã€‚ å…¶ä»–è¡¥ç•ªè¿™é‡Œåªè°ˆä¸€ä¸‹æˆ‘èƒ½æƒ³å¾—èµ·æ¥çš„7éƒ¨ï¼Œè‰å¯ä¸½ä¸ã€å­¤ç‹¬æ‘‡æ»šï¼ã€çµèƒ½ç™¾åˆ†ç™¾Iã€çµèƒ½ç™¾åˆ†ç™¾IIã€çµèƒ½ç™¾åˆ†ç™¾IIIã€æ‘‡æ›³éœ²è¥ ç¬¬ä¸€å­£ã€æ‘‡æ›³éœ²è¥ ç¬¬äºŒå­£ã€‚è™½ç„¶ä½†æ˜¯ï¼Œè¿™ä¸ƒéƒ¨éƒ½æ˜¯ç¥ä½œã€‚ è‰å¯ä¸½ä¸ (ãƒªã‚³ãƒªã‚¹ãƒ»ãƒªã‚³ã‚¤ãƒ«) - 4.5&#x2F;5.0 07&#x2F;2023 (2022) åŒå¥³ä¸»è½»ç™¾ã€‚å¬é›†å­¤å„¿è®­ç»ƒæˆæ€æ‰‹ï¼Œæš—æ€çŠ¯ç½ªçš„äººï¼Œè®©æ•´ä¸ªç¤¾ä¼šè¡¨é¢ä¸ŠçŠ¯ç½ªç‡ä¸º0ã€‚æœ‰äººæƒ³è¦æ¢å¤æ­£å¸¸çš„ç¤¾ä¼šè€ŒæŠŠLycorisæ…å‡ºæ¥ï¼Œè®©å¤§å®¶æ”»å‡»ä»–ä»¬ã€‚åæœŸçš„æ§½ç‚¹æœ‰ç‚¹å¤§ï¼Œå°¤å…¶æ˜¯å¥³ä¸»å¿ƒè„ä¸è¡Œäº†ç»™å®ƒæ¢å¿ƒè„é‚£æ®µï¼Œå¥³ä¸»æ­»æ´»ä¸ç”¨å®å¼¹ä¸ç®¡æ˜¯è‡ªå·±è¦æ­»è¿˜æ˜¯å…¶ä»–äººè¦æ­»éƒ½ä¸ç”¨å®å¼¹ï¼Œæˆ‘ä¸ç†è§£ã€‚ å­¤ç‹¬æ‘‡æ»šï¼ (ã¼ã£ã¡ãƒ»ã–ãƒ»ã‚ã£ã!) - 4.5&#x2F;5.0 05&#x2F;2023 (2022) ç¤¾æå°å­¤ç‹¬ï¼Œéå¸¸æˆ³ä¸­ç¤¾æäººçš„å†…å¿ƒã€‚ çµèƒ½ç™¾åˆ†ç™¾ (ãƒ¢ãƒ–ã‚µã‚¤ã‚³100) ç¬¬I&#x2F;II&#x2F;IIIå­£- 4.5&#x2F;5.0 06&#x2F;2023 (2016&#x2F;2019&#x2F;2022) çµå¹»å¸ˆå‚…å¤ªå–„è‰¯äº†ï¼Œä¸“èŒä¸­äºŒç—…20å¹´ï¼Œä¸€ä¸ªå‡¡äººèƒ½åœ¨èƒ½åŠ›è€…é¢å‰ä¾ƒä¾ƒè€Œè°ˆæ¯«ä¸ç•æƒ§ã€‚è·¯äººä½œä¸ºåœ°è¡¨æœ€å¼ºåœ¨çµå¹»å¸ˆå‚…çš„å¸®åŠ©ä¸‹ä¿ä½äº†è‡ªå·±çš„é’æ˜¥ã€‚ æ‘‡æ›³éœ²è¥ ( ã‚†ã‚‹ã‚­ãƒ£ãƒ³) ç¬¬1&#x2F;2å­£ - 4.5&#x2F;5.0 08&#x2F;2023 (2018&#x2F;2021) èŠ³æ–‡ç¤¾yydsã€‚ çºªå½•ç‰‡åŠå‰§çºªå½•ç‰‡å’Œå‰§æˆ‘çœ‹çš„ä¸å¤šï¼Œå¾ˆå°‘ï¼Œä»Šå¹´ä¸»è¦çœ‹è¿‡ä¿©ï¼Œä¸€ä¸ªæ˜¯å­¤ç‹¬çš„ç¾é£Ÿå®¶ï¼Œä¸€ä¸ªæ˜¯ä¸­å›½æ•‘æŠ¤ã€‚ä¸­å›½æ•‘æŠ¤æ›´å¤šçš„æ˜¯æ‹“å±•æˆ‘å¯¹äºå›½å†…åŒ»ç–—ç°çŠ¶åŠå¸¸è§ç´§æ€¥ç–¾ç—…çš„è®¤çŸ¥ã€‚ å°è¯´æœ‰ä¸€è¯´ä¸€ä»Šå¹´å®é™…ä¸Šå°±çœ‹è¿‡ä¸€æœ¬å°è¯´ï¼Œä¼šè¯´è¯çš„è‚˜å­çš„ã€Šç¬¬ä¸€åºåˆ—ã€‹ï¼ŒåŒæ—¶å®ƒçš„ç•ªæˆ‘ä¹Ÿçœ‹å®Œäº†ã€‚ç¬¬ä¸€åºåˆ—è¿™æœ¬å°è¯´ä¸»è¦è®²çš„æ˜¯æœ«ä¸–èƒŒæ™¯ä¸‹å®éªŒä½“å’Œä¸»è§’çš„å¤§é€ƒäº¡ï¼Œè™½ç„¶æ˜¯å¾ˆå¸¸è§çš„é¢˜æä½†æ˜¯æ„å¤–çš„å¥½çœ‹ã€‚å¤§æ¦‚åœ¨åæ®µ30%çš„åœ°æ–¹å¼€å§‹å°±ç¨æ˜¾æ— è¶£äº†ï¼Œå±äºä¸»è¦åœ°å›¾åˆ·å®Œäº†æ²¡å¾—åˆ·å¼€è¾Ÿä¸€å—æ–°åœ°å›¾è·‘åˆ°æ¬§æ´²æå·«å¸ˆå»äº†ã€‚ä½†æ˜¯è‚˜å­è¯´çš„è¿™æ®µè¯è¿˜æ˜¯æ¯”è¾ƒä¸é”™çš„ã€‚ åºŸåœŸä¹‹ä¸Šï¼Œäººç±»æ–‡æ˜å¾—ä»¥è‹Ÿå»¶æ®‹å–˜ã€‚ä¸€åº§åº§å£å’æ‹”åœ°è€Œèµ·ï¼Œç§©åºå´ä¸æ–­å´©åã€‚æœ‰äººè¯´ï¼Œå½“ç¾éš¾é™ä¸´æ—¶ï¼Œç²¾ç¥æ„å¿—æ‰æ˜¯äººç±»é¢å¯¹å±é™©çš„ç¬¬ä¸€åºåˆ—æ­¦å™¨ã€‚æœ‰äººè¯´ï¼Œä¸è¦è®©æ—¶ä»£çš„æ‚²å“€ï¼Œæˆä¸ºä½ çš„æ‚²å“€ã€‚æœ‰äººè¯´ï¼Œæˆ‘è¦è®©æˆ‘çš„æ‚²å“€ï¼Œæˆä¸ºè¿™ä¸ªæ—¶ä»£çš„æ‚²å“€ã€‚è¿™æ¬¡æ˜¯ä¸€ä¸ªæ–°çš„æ•…äº‹ã€‚æµ©åŠ«ä½™ç”Ÿï¼Œç»ˆè§å…‰æ˜ã€‚ è¿™æœ¬ä¹¦æˆ‘ç»¼åˆè¯„åˆ†èƒ½å¤Ÿç»™åˆ°4.0&#x2F;5.0ï¼Œå› ä¸ºç»“å°¾å¤„ç†ä¸å¥½è€Œæ‰£1åˆ†ã€‚ æ¸¸æˆä»Šå¹´ä¸»è¦ç©çš„æ¸¸æˆä¹Ÿä¸å¤šï¼Œä»Šå¹´ç¡®å®ç¨å¾®å¿™ä¸€äº›å¨±ä¹çš„æ—¶é—´ä¹Ÿå‡å°‘äº†ï¼Œæˆ‘è¿™é‡Œå°±æŠŠæˆ‘ä»Šå¹´ç©è¿‡çš„æ‰€æœ‰æ¸¸æˆéƒ½ç®€å•è¯´ä¸€ä¸‹å§ã€‚ ç¾¤æ˜Ÿ (Stellaris) - 4.0&#x2F;5.0 ä¸€ä¸ªè®©è‡ªå·±æˆä¸ºç¬¬å››å¤©ç¾ã€å®‡å®™ç”²çº§æˆ˜çŠ¯çš„æ¸¸æˆã€‚æˆ‘å¼€å§‹ç©çš„ç‰ˆæœ¬æ˜¯3.6ï¼Œç°åœ¨æœ€æ–°ç‰ˆæœ¬åº”è¯¥æ˜¯3.10ã€‚ç¾¤æ˜Ÿå±äºRTSç±»æ¸¸æˆï¼Œå’Œæˆ‘ä»å°ç©çš„çº¢è­¦2å±äºä¸€ä¸ªç±»åˆ«ã€‚å‘å±•ç»æµã€ç§‘ç ”ã€æ”¿ç­–ï¼Œç„¶åç‚¹ç§‘æŠ€æ ‘ã€é€ æˆ˜èˆ°ã€å åœ°ç›˜ï¼Œåˆ°å®£ç§°ã€å¼€æˆ˜ã€é™„åº¸ã€ç­å›½ã€‚å±äºé•¿çº¿çš„æ¸¸æˆï¼Œä¹Ÿéå¸¸çš„åå™¬æ—¶é—´ï¼Œç©è¿™ä¸ªæ¸¸æˆè¦å°å¿ƒæ—¶é—´ä¼šæ¶ˆå¤±ï¼ˆPç¤¾æ¸¸æˆç‰¹ç‚¹ï¼‰ã€‚ å®ˆæœ›å…ˆé”‹ 2 (Overwatch 2) - 4.0&#x2F;5.0 æš´é›ªè¿™ä¸ªæ ·å­æˆ‘ä»¬ç©å®¶ä¹Ÿæ²¡åŠæ³•ï¼Œå®ˆæœ›è¿˜æ˜¯ä¾æ—§å¥½ç©çš„ï¼Œ5äººå¯¹æˆ˜ç»™åˆ°äº†Tå’Œä¸»å¥¶æ›´å¤§çš„å‹åŠ›ï¼Œä¹Ÿè¦æ±‚Cåšæ›´å¤šçš„å‡»æ€è€Œéå•çº¯è¾“å‡ºã€‚ç°åœ¨çš„ç‰ˆæœ¬åªè¦Tæˆ–è€…ä¸»å¥¶æ‰äº†å°±å¾—èµ¶ç´§å¾€å›æ‹‰ï¼Œè¦ä¸ç„¶å°±è¦å›¢ç­ã€‚ åŸå¸‚ï¼šå¤©é™…çº¿2 (Cities Skylines II) - 4.5&#x2F;5.0 è¿™æ¸¸æˆï¼Œå±äºæŠŠåŸå¸‚å¤©é™…çº¿1å’Œsimcity5ç»“åˆèµ·æ¥äº†ä¸€éƒ¨åˆ†ï¼Œå†æ”¹è¿›äº†ä¸€éƒ¨åˆ†ã€‚æœ‰äº†æ›´å¤§æ›´çµæ´»çš„åœ°å›¾ï¼Œæ›´ç®€æ˜“çš„å¹³åœ°å’Œä¿®è·¯ï¼Œå»ºç­‘ç‰©æ‰©å±•ç­‰ç­‰ã€‚ä¹Ÿå…¨é¢æ”¹è¿›äº†ç»æµç³»ç»Ÿã€‚è™½ç„¶ç›®å‰æ¥è¯´ä¼˜åŒ–æ˜¯ç­”è¾©æˆ‘3070Tiç©4Kä¸­ç”»è´¨éƒ½ç¨å¾®æœ‰ç‚¹å¡ï¼Œä½†ç”»è´¨å’Œæ•´ä½“æ¸¸ç©æ•ˆæœæ˜¯ç¡®å®æ²¡å¾—è¯´ã€‚å¸Œæœ›åæœŸèƒ½å¤Ÿæ”¹è¿›å»ºç­‘ç‰©æ‰©å±•ä»¥åå»ºç­‘ç‰©çš„éƒ¨åˆ†ä¸èƒ½ç§»åŠ¨æˆ–æ‹†é™¤çš„è¿™ç§é—®é¢˜ï¼ŒåŒ…æ‹¬80mçš„æ¡¥å¢©å¤ªçª„äº†ç°åœ¨é«˜æ¶éƒ½æ²¡æ³•æ‹‰ã€‚ èŠ‚å¥å¤§å¸ˆ - 4.0&#x2F;5.0 è¿™ä¸ªä¸œè¥¿ï¼Œå±äºæƒ…æ€€åŠ åˆ†äº†ï¼Œ11æœˆ7æ—¥å›å½’ã€‚æˆ‘ç›®å‰ä¾¥å¹¸ç©åˆ°äº†å®—å¸ˆæ®µä½ï¼Œé—¯å…³æ‰“åˆ°äº†400å‡ºå¤´ã€‚å±äºéŸ³æ¸¸é‡Œæ¯”è¾ƒå…¥é—¨éš¾åº¦çš„æ¸¸æˆï¼ˆ10çº§æ­Œé™¤å¤–ï¼‰ã€‚ ç”µå­äº§å“ä»Šå¹´åœ¨ç”µå­äº§å“æ–¹é¢çš„æ¶ˆè´¹ç¡®å®éå¸¸å°‘ï¼ˆå¾€å¹´é™¤äº†æ›´æ–°æ¢ä»£ä¹Ÿç¡®å®ä¸å¤šï¼‰ï¼Œè¿™é‡Œæˆ‘å°±æå››æ ·æˆ‘ä»Šå¹´ä¹°çš„ç”µå­äº§å“æ¥ç®€å•èŠèŠã€‚ ç›¯ç›¯æ‹Z50è¡Œè½¦è®°å½•ä»ª (DDPAI Z50 Dashcam) - 3.0&#x2F;5.0 02&#x2F;14&#x2F;2023 - 145.44ç¾å…ƒ(çº¦1030äººæ°‘å¸) ä¸ºå•¥çªç„¶ä¹°ä¸ªè¡Œè½¦è®°å½•ä»ªå‘¢ï¼ŸåŸå› å¾ˆç®€å•ï¼Œæˆ‘åœ¨ç¾å›½å†œå†å¤§å¹´åˆä¸€å¼€è½¦å‡ºé—¨çš„è·¯ä¸Šè¢«ä¸€ä¸ªçš®å¡æ¶æ„è¿½å°¾ï¼Œå½“æ—¶ä»–è¯´çš„æ¯”å”±çš„éƒ½å¥½å¬ä¸€å£ä¸€ä¸ªæŠ±æ­‰ã€ä¸€å£ä¸€ä¸ªå¯¹ä¸èµ·ã€‚æˆ‘å›å»æŠ¥äº†ä¿é™©ä»¥åä»–è·Ÿä¿é™©å…¬å¸è¯´æ˜¯æˆ‘æ•…æ„å€’è½¦æ’åˆ°ä»–ï¼Ÿæˆ‘åœ¨é‚£ç­‰çº¢ç»¿ç¯åœçš„å¥½å¥½çš„ï¼Œçœ‹ä»–åœçš„ç¦»æˆ‘å¤ªè¿‘å¾€å‰æŒªï¼Œä»–ä¹Ÿè·Ÿç€æˆ‘å¾€å‰å°±ç›´æ¥æ’ä¸Šæ¥äº†ã€‚æ‰€ä»¥ä¹°äº†è¿™ä¸ªè¡Œè½¦è®°å½•ä»ªã€‚ è¯´å®è¯å®‰è£…éš¾åº¦ç¨å¾®æœ‰ç‚¹ï¼Œè¦å¼€æœºç”¨è¾…åŠ©çº¿æ ¡å‡†è´´åˆ°è¾…åŠ©è´´ä¸Šï¼Œèµ°çº¿ä¹Ÿè¦è‡ªå·±å¼„ï¼Œæœ€åæˆ‘ç”¨çš„æ˜¯ç‚¹çƒŸå™¨å–ç”µç¨³å®šæ€§ç›®å‰æ¥çœ‹è¿˜å¯ä»¥ã€‚åç½®æ‘„åƒå¤´ç”»è´¨æœ‰ä¸€äº›æ‹‰ï¼Œå‰ç½®åœ¨ç™½å¤©è¿˜å¯ä»¥ï¼Œæ™šä¸Šå™ªç‚¹æœ‰ä¸€äº›å¤šã€‚ APPéå¸¸è„‘æº¢è¡€ï¼Œæµ·å¤–ç‰ˆæ˜¯å•ç‹¬çš„APPå’Œå›½å†…ä¸ä¸€æ ·ï¼Œè¿æ¥ä¸Šç›¸æœºä»¥åä¼šåœ¨æ‰‹æœºç›¸å†Œé‡Œåˆ›å»ºå±…å¤šDDPAIç›¸ç°¿ï¼Œå°±å¾ˆè„‘æº¢è¡€ã€‚ç»¼åˆä¸‹æ¥è€ƒè™‘ä»·æ ¼ç»™äº†3.0ã€‚ AirTag - 4.0&#x2F;5.0 05&#x2F;11&#x2F;2023 - 4ä¸ª91.96ç¾å…ƒ(çº¦650äººæ°‘å¸) - å•ä»·23ç¾å…ƒ(çº¦163äººæ°‘å¸) å…¶å®è¿™ä¸ªä¸œè¥¿ä¸ç”¨æˆ‘å¤šåšä»€ä¹ˆä»‹ç»å¤§å®¶éƒ½æ¸…æ¥šï¼Œè¿™æ¬¡ä¹°ä¸»è¦æ˜¯å› ä¸ºè¦å‡ºå»æ—…æ¸¸å’Œå»é¦™æ¸¯å¸¦ä¸¤ä¸ªæœˆï¼Œä¸ºäº†ä¿è¯è¡Œæçš„å®‰å…¨æ€§ã€‚åœ¨è¡Œæç®±å’ŒåŒ…é‡Œéƒ½æ”¾äº†ä¸€ä¸ªï¼Œå‡†ç¡®åº¦è¿˜æ˜¯å¯ä»¥çš„ï¼Œç›®å‰ç”¨äº†åŠå¹´æ²¡æœ‰ä½ç”µé‡è­¦å‘Šï¼Œè€ƒè™‘åˆ°ä»·æ ¼ä¹Ÿå°±ç»™äº†ä¸€ä¸ª4åˆ†ã€‚ NOCO Genius 1æ±½è½¦è“„ç”µæ± ç»´æŒå™¨ - 4.5&#x2F;5.0 06&#x2F;06&#x2F;2023 - 27.4ç¾å…ƒ(çº¦194äººæ°‘å¸) è¿™ä¸ªä¸œè¥¿å¯èƒ½å¤§å®¶ä¸æ˜¯å¾ˆæ¸…æ¥šï¼Œå› ä¸ºæˆ‘éœ€è¦æŠŠè½¦åœåœ¨ç¾å›½ä¸‰ä¸ªæœˆä¸€åŠ¨ä¸åŠ¨ï¼Œé•¿æ—¶é—´ä¸åŠ¨è½¦è“„ç”µæ± ä¼šè·‘ç©ºç”µã€‚è¿™ä¸ªä¸œè¥¿å¯ä»¥ç›´æ¥æ¥åˆ°è½¦çš„è“„ç”µæ± ä¸Šï¼Œæ£€æµ‹è“„ç”µæ± çš„å‰©ä½™ç”µé‡ï¼Œåœ¨ç”µé‡è¿‡ä½çš„æ—¶å€™å……ç”µï¼Œæ»¡ç”µçš„æ—¶å€™å°±åœä¸‹æ¥ã€‚ä¿æŠ¤ç”µæ± è®©ç”µæ± ä¸ä¼šæ²¡ç”µï¼Œä¹Ÿä¸ä¼šè¿‡å†²ã€‚å‰©ä¸‹äº†å«Jumpstartçš„åŠŸå¤«ï¼ˆè™½ç„¶ä¿é™©å…¬å¸ä¿é™©ä½†æ˜¯å¾ˆéº»çƒ¦ï¼‰ã€‚ å°ç±³ç±³å®¶è¡€å‹è®¡BPX1 - 4.0&#x2F;5.0 07&#x2F;05&#x2F;2023 - çº¦30.76ç¾å…ƒ(217.73äººæ°‘å¸) è¿™ä¸ªä¸œè¥¿æˆ‘ä¹°çš„åŸå› å¾ˆç®€å•ï¼Œå°±æ˜¯å®ƒçš„è¿™ä¸ªåˆ›æ–°è®¾è®¡çš„ç»‘å¸¦ã€‚ä¼ ç»Ÿçš„ç»‘å¸¦éœ€è¦æŠŠæ‰‹ä»ä¸­é—´ç©¿è¿‡å»å¥—åˆ°ä¸Šè‡‚ï¼Œå®ƒè¿™ç§è®¾è®¡å¯ä»¥ç›´æ¥å¤¹åœ¨ä¸Šè‡‚ç„¶åç²˜ä¸Šï¼Œçœäº†ä¸€ä¸ªæ­¥éª¤ã€‚å‡†ç¡®åº¦ä¹Ÿæ˜¯éå¸¸ä¸é”™çš„ï¼Œå±äºå±…å®¶å¿…å¤‡çš„å°å®¶ç”µã€‚å”¯ä¸€çš„é—®é¢˜å°±æ˜¯è™½ç„¶èƒ½å¤Ÿè¿æ¥ç±³å®¶APPï¼Œä½†æ˜¯ä¸èƒ½è‡ªåŠ¨åŒæ­¥åˆ°Apple Healthï¼Œå¦‚æœèƒ½åŒæ­¥çš„è¯å°†æ˜¯ç»æ€ã€‚å¦‚æœå®¶é‡Œæœ‰ç±³å®¶ç½‘å…³ç»™è€äººç”¨å¾ˆå¥½ï¼Œæ¯æ¬¡è€äººæµ‹å®Œè¡€å‹ä¼šè‡ªåŠ¨é€šè¿‡ç½‘å…³ä¸Šä¼ å¹¶ä¸”ç»™ä½ å‘æ¨é€ã€‚ åº”ç”¨ä»Šå¹´æˆ‘æƒ³è¦æåˆ°çš„åº”ç”¨åªæœ‰ä¸€ä¸ªï¼Œè™½ç„¶å®ƒæ¥è¿‘å¹´åº•æ‰å‡ºæ¥ï¼Œä½†æˆ‘è§‰å¾—è¿˜æ˜¯éå¸¸æœ‰æ„æ€å’Œæ–¹ä¾¿çš„ä¸€ä¸ªåº”ç”¨ï¼Œé‚£å°±æ˜¯iOSå†…ç½®çš„ã€Šæ‰‹è®°ã€‹ã€‚è¿™ä¸ªAPPå’Œå…¶ä»–æ—¥è®°ä¸ä¸€æ ·çš„ç‚¹å°±æ˜¯å®‰å…¨å’Œè‡ªåŠ¨åŒ–ï¼Œå®ƒä¼šåœ¨ä½ å‡ºå»æ—…æ¸¸ä¸€å¤©å›åˆ°é…’åº—é‡Œç»™ä½ æ¨é€ä½ ä»Šå¤©æ—…æ¸¸çš„æ™¯ç‚¹å’Œä¹Ÿè®¸ä½ æƒ³è¦è®°å½•çš„å†…å®¹çš„ç…§ç‰‡ï¼Œç»“åˆåœ°ç‚¹ä¸ºä½ å¸¦æ¥ä¸€äº›æ€è·¯ã€‚å®ƒä¹Ÿä¼šåœ¨ä½ æ²¡å‡ºå»ç©æ²¡ç‚¹å­å†™æ—¥è®°çš„æ—¶å€™ç»™ä½ æä¾›ä¸€äº›å¥½çš„ç‚¹å­æ–¹ä¾¿ä½ å†™æ—¥è®°ã€‚ äºŒæ¬¡å…ƒå‘¨è¾¹ä»Šå¹´æ²¡ä¹°ä»€ä¹ˆæ–°çš„æ‰‹åŠã€‚ä½†æ˜¯å»æ—¥æœ¬çš„æ—¶å€™ä¹°äº†è›®å¤šçš„å°æ‘†ä»¶ã€ç«‹ç‰Œå’Œå•ªå”§ï¼Œéƒ½æ”¾åœ¨å›½å†…å®¶é‡Œäº†ã€‚ ç”Ÿæ´»è¡£è¯´å®è¯ä»Šå¹´ä¸»è¦æ˜¯ä½“é‡æ‰äº†æ¥è¿‘20å…¬æ–¤ï¼Œä¹‹å‰çš„è¡£æœåŸºæœ¬ä¸Šéƒ½ç©¿ä¸ä¸Šäº†ï¼Œè¶ç€å»æ—¥æœ¬å’Œå›å›½çš„æ—¶å€™ä¹°äº†ä¸€äº›ä¼˜è¡£åº“å’ŒGUå®¶çš„è¡£æœï¼ˆä¾¿å®œï¼‰ã€‚ é£Ÿç¾å›½æ˜¯ç¾é£Ÿè’æ¼ çš„äº‹å®å¤§å®¶éƒ½çŸ¥é“ï¼Œè™½ç„¶æ¯”è‹±å›½è‚¯å®šæ˜¯å¥½ä¸å°‘ï¼Œä½†é—®é¢˜åœ¨äºæœ‰åƒçš„æˆ‘åƒä¸èµ·å‘€â€¦æ‰€ä»¥é—®é¢˜åœ¨æˆ‘ï¼Œä¸æ˜¯ä»–ä»¬è´µï¼Œæ˜¯æˆ‘ç©·ã€‚ä¸‹é¢å°±æ˜¯æˆ‘æœ¬å¹´åœ¨ç¾å›½ã€æ—¥æœ¬ã€é¦™æ¸¯ã€æ·±åœ³ç­‰åœ°æ–¹åƒåˆ°çš„æˆ‘è§‰å¾—è¿˜èƒ½æ‹¿ä¸Šå°é¢ç»™å¤§å®¶åˆ†äº«ä¸€ä¸‹çš„ç¾é£Ÿï¼ŒåŒ…å«ç²¤èœã€ç¾å¼å¿«é¤ã€æ—¥æ–™ã€æ„å¤§åˆ©èœã€ç”œå“ã€å’ŒCUHKçš„é£Ÿå ‚ã€‚å½“ç„¶æœ€åä¹Ÿæœ‰æˆ‘è‡ªå·±ä»Šå¹´éƒ¨åˆ†è‡ªå·±åšçš„èœï¼Œåœ¨ç¾å›½å¤§å¤šæ•°æ—¶å€™è‡ªå·±åšæ­£ç»èœè¿˜æ˜¯ç…ç‰›æ’æˆ–è€…çƒ¤ä¸œè¥¿è¿™ç§æ¯”è¾ƒå¿«å’Œç®€å•çš„èœã€‚ é¦–å…ˆæ˜¯ç”œå“å’Œå¿«é¤ä»¬ï¼ŒæŠŠä»–ä»¬æ”¾åˆ°ä¸€èµ·å•çº¯ä¸ºäº†èŠ‚çœç©ºé—´ã€‚å·¦è¾¹ä¸€åˆ—æ˜¯ç”œå“ï¼Œå³ä¾§ä¸¤åˆ—æœ€ä¸Šé¢ä¸¤ä¸ªæ˜¯é¦™æ¸¯çš„éº¦å½“åŠ³æ—©é¤ã€ç¾å›½KFCçš„é™å®šç‚¸é¸¡æ±‰å ¡ã€ç¾å›½Dominosçš„æŠ«è¨ï¼ˆè„†åº•ï¼‰ã€ç‹—è›‹çš„æ±‰å ¡åº—ã€In-N-Outï¼ˆæˆ‘çš„é£Ÿå ‚ï¼‰æˆ‘æœ€å¸¸åƒçš„Protein Double Doubleã€æ‹‰æ–¯ç»´åŠ æ–¯åƒåˆ°çš„é¾™è™¾å·ã€æ‹‰æ–¯ç»´åŠ æ–¯å–åˆ°çš„16ç§å…¨çƒå¯ä¹ã€‚ ç„¶åæ˜¯æ¸¯ä¸­æ–‡CUHKçš„é£Ÿå ‚ï¼Œæˆ‘ä¸»è¦åƒçš„å°±æ˜¯å’Œå£°ä¹¦é™¢æ¥¼ä¸‹çš„ï¼ˆå› ä¸ºä¸å‡ºé—¨ï¼Œå¤ªéº»çƒ¦äº†ï¼‰ã€‚ä¸€é¡¿é¥­ä»·æ ¼åœ¨40-60æ¸¯å¸ä¸ç­‰ï¼Œåœ¨å­¦æ ¡ä¸¤ä¸ªæœˆå·²ç»æŠŠæ‰€æœ‰èƒ½ç‚¹çš„éƒ½ç‚¹è¿‡ä¸€éäº†ã€‚ æ¥ä¸‹æ¥æ˜¯æˆ‘åœ¨ç¾å›½ã€é¦™æ¸¯ã€æ·±åœ³åƒåˆ°çš„ç²¤èœä»¬ã€‚æœ€ä¸Šé¢æ˜¯æ—©èŒ¶ï¼ˆæœ€çˆ±ç³¯ç±³é¸¡ã€è™¾é¥ºã€è±‰æ±æ’éª¨ï¼‰ã€æ¥ä¸‹æ¥æ˜¯çƒ§è…Šä»¬ï¼ˆæœ€çˆ±çƒ§é¹…ï¼‰ã€ç„¶åæ˜¯äº‘åé¢ã€ç‰›è‚šé¢ã€è‚‰éª¨èŒ¶ã€‚ ç„¶åæ˜¯æˆ‘ä»¬çš„æ„å¤§åˆ©èœï¼ŒæŠ«è¨ã€è‚‰ä¸¸ã€æ„é¢ã€risottoï¼ˆæ„å¤§åˆ©çƒ©é¥­ï¼‰ã€å’Œä¸€ä¸ªå¥‡æ€ªçš„å¯å¯åŠ å’–å•¡è…Œåˆ¶è¿‡çš„ç…ç‰›æ’ æ¥ä¸‹æ¥æ˜¯æ—¥æ–™ä»¬ï¼Œä¸Šé¢ä¸¤æ’æ˜¯å¯¿å¸ã€æ¥ä¸‹æ¥æ˜¯å¯¿å–œçƒ§ã€çƒ¤è‚‰ã€é³—é±¼é¥­ã€æ—¥å¼æ—©é¤ã€æ‹‰é¢ã€æ²¾é¢ã€‚ æœ€åæ˜¯æ³°é¤ï¼Œæœ€å¤šçš„è¿˜æ˜¯æ³°å¼å’–å–±äº†ã€‚ æœ€åå°±æ˜¯æˆ‘è‡ªå·±åšçš„ä¸€éƒ¨åˆ†é¥­äº†ã€‚ä»Šå¹´åšäº†ä¸¤æ¬¡è‚˜å­ã€ä¸€æ¬¡é¾™è™¾ã€æ— æ•°æ¬¡ç‰›æ’ã€é¸¡ç¿…ã€é±¼ç­‰ã€‚ ä½ä»Šå¹´ä¸€å…±ç¦»å¢ƒç¾å›½å¤§æ¦‚3ä¸ªåŠæœˆï¼Œé™¤äº†é‚£æ®µæ—¶é—´å…¶ä»–æ—¶é—´å’Œå»æ‹‰æ–¯ç»´åŠ æ–¯ç©çš„å‡ å¤©ï¼Œå…¶ä»–æ—¶å€™æˆ‘éƒ½åœ¨è‡ªå·±ç§Ÿçš„æˆ¿å­é‡Œé¢å¾…ç€ã€‚æˆ¿ç§Ÿä¸€ä¸ªæœˆæ˜¯1200ç¾é‡‘ï¼Œ2b2b condoä¸­çš„1b1bï¼Œå·²ç»ä½äº†ä¸€å¹´å¤šäº†ã€‚æš‘å‡çš„æ—¶å€™æ°å¥½æœ‰éš”å£å®éªŒå®¤çš„å¥½å§å¦¹éœ€è¦å°±è½¬ç§Ÿç»™å¥¹äº†ä¸¤ä¸ªåŠæœˆå¤šã€‚ 6æœˆåˆæ”¾å‡ååœ¨å»é¦™æ¸¯çš„è·¯ä¸Šé¡ºè·¯å»äº†ä¸œäº¬ç©ã€‚ä½äº†é“¶åº§åé“ç©†ç‘Ÿé…’åº—4æ™šï¼Œä¸€ä¸ªäººçº¦150ç¾å…ƒï¼ˆ1078äººæ°‘å¸ï¼‰ã€‚è¯´å®è¯è¿™å®¶æ˜¯ç¨å¾®æœ‰ç‚¹å°ï¼Œå¤§æ¦‚èƒ½æ‰“å¼€3ä¸ª28å¯¸çš„è¡Œæç®±æˆ¿é—´å°±æ²¡æœ‰è½è„šçš„åœ°æ–¹äº†ï¼Œä½†è¿™ä¹ˆå°çš„æˆ¿é—´å±…ç„¶ä¸€åº”ä¿±å…¨ï¼Œä½©æœã€‚æœ€åä¸€æ™šæ˜¯ç”¨AMEX Hilton Aspireé€çš„Free Nightæ¢çš„Conrad Tokyoï¼Œä¸€åˆ†é’±æ²¡èŠ±ï¼Œé’»å¡è¿æ°”ä¸é”™å»çš„æ—©ç»™å‡åˆ°äº†æµ·æ™¯å¥—ï¼Œå½“æ™šç°é‡‘ä»·æ ¼æ˜¯å¤§æ¦‚870ç¾å…ƒï¼ˆçº¦6150äººæ°‘å¸ï¼‰ã€‚è¿™å®¶è™½ç„¶è®¾æ–½æ¯”è¾ƒè€ï¼Œä½†æ€»ä½“ä½çš„è¿˜æ˜¯è›®èˆ’æœçš„ï¼Œå¤–åŠ ä¸Šé’»å¡çš„è¡Œæ”¿é…’å»Šä¹Ÿè¿˜å¯ä»¥ã€‚ä¸‹é¢æ˜¯ä¸œäº¬åº·è±å¾·æµ·æ™¯å¥—æˆ¿çš„å†…é¥°å’Œè¡Œæ”¿é…’å»Šçš„æƒ…å†µã€‚ æ¥åˆ°é¦™æ¸¯ä»¥åä½äº†ä¸¤ä¸ªæœˆæ¸¯ä¸­æ–‡çš„å’Œå£°ä¹¦é™¢å®¿èˆï¼Œ8000æ¸¯å¸ä¸¤ä¸ªæœˆï¼ŒåŒäººé—´å•ç‹¬çš„åºŠã€æ¡Œå­ã€æŸœå­ï¼Œæœ‰ç©ºè°ƒï¼Œä¸è®¡ç”µè´¹ã€‚æ¯å±‚å…±ç”¨æµ´å®¤å’Œå«ç”Ÿé—´ï¼Œæ²¡æœ‰LGBTQå®¿èˆï¼Œåªæœ‰binary sexã€‚ å»æ‹‰æ–¯ç»´åŠ æ–¯æœŸé—´ä½çš„æ˜¯Hiltonæ——ä¸‹æŒ‚Double Treeç‰Œå­çš„Tropicanaï¼Œä¸»è¦ä¸ºäº†ç”¨æ‰Aspireé€çš„250åˆ€resort creditã€‚å®šçš„æ™®é€šæˆ¿ä¸‰æ™šä¸€ä¸ªäºº141.3ï¼Œé’»å¡åœ¨ç¾å›½ä¸å¥½ä½¿ï¼Œå°±ç»™å‡åˆ°äº†é«˜çº§æˆ¿ï¼Œå¥½åœ¨ç»™äº†ä¸ªæœ€è¾¹ä¸Šçš„æˆ¿é—´è¿˜å¤§ä¸€äº›æœ‰ä¸€äº›é£æ™¯ã€‚æ—©é¤ä¸ç”¨æƒ³ä¸€ä¸ªäººå°±ç»™13åˆ€creditæ²¡æœ‰å…è´¹æ—©é¤ã€‚æ€»ä½“ä½“éªŒè¿˜å¯ä»¥ï¼Œç»™å°è´¹home keeperä¹Ÿæ²¡æ‹¿ã€‚ è¡Œä»Šå¹´æˆ‘è‡ªå·±çš„è½¦ä¸€å…±å¼€äº†6547 milesï¼ˆ10,520 kmï¼‰ï¼Œæ¶ˆè€—ç‡ƒæ²¹232 gallonï¼ˆ879 Lï¼‰ï¼Œå¹³å‡æ²¹è€—27.66 MPGï¼ˆ8.54 L&#x2F;100KMï¼‰ã€‚ä¸€å…±æ²¹è´¹å¤§æ¦‚æ˜¯1300ç¾å…ƒï¼ˆçº¦9200äººæ°‘å¸ï¼‰ï¼Œçº¦æ¯åŠ ä»‘5.6ç¾å…ƒï¼ˆ10.5äººæ°‘å¸&#x2F;å‡ï¼‰ã€‚ åœ¨æ—¥æœ¬ç©çš„æ—¶å€™éƒ½é åœ°é“å’Œç”µè½¦ï¼Œæ²¡æ‰“è¿‡è½¦ï¼ˆè¦å‘½åœ°è´µï¼‰ï¼Œå¤§æ¦‚ä¸€å…±èŠ±äº†35ç¾å…ƒï¼ˆ248äººæ°‘å¸ï¼‰6å¤©çš„åœ°é“å’Œç”µè½¦ã€‚åœ¨é¦™æ¸¯é™¤äº†ä»æœºåœºåˆ°å®¿èˆå’Œå®¿èˆåˆ°æœºåœºè¡Œæå¤ªå¤šåªèƒ½æ‰“è½¦ï¼ˆå•ç¨‹340æ¸¯å¸-çº¢è‰²å‡ºç§Ÿè½¦ï¼‰ï¼Œå…¶ä»–æ—¶å€™éƒ½é åœ°é“ï¼ˆä¹Ÿè´¼è´µï¼‰ï¼Œååœ°é“å¯èƒ½èŠ±äº†å››ç™¾å¤šæ¸¯å¸å§ï¼ˆå…«è¾¾é€šå……å€¼æ¯”è¾ƒå¤æ‚ä¸å¥½è®°è´¦ï¼‰ã€‚åœ¨æ·±åœ³ç©çš„æ—¶å€™åŸºæœ¬ä¸Šé æ‰“è½¦å’Œåœ°é“ï¼Œå›½å†…æ‰“è½¦å¤ªä¾¿å®œäº†ã€‚ ä»Šå¹´ä¸€å…±é£äº†6æ®µï¼Œå…¨éƒ½æ˜¯ç°é‡‘ç¥¨ä¹°çš„ï¼ˆç§¯åˆ†æ¢ä¸æ˜¯å¾ˆåˆ’ç®—ï¼‰ã€‚æ´›æ‰çŸ¶åˆ°ä¸œäº¬ç¾½ç”°åçš„deltaï¼ŒåŒ—ç¾èˆªå¸æ²¡ä»€ä¹ˆæœŸå¾…ï¼ŒLAXä¹Ÿæ²¡æœ‰PPSçš„ä¼‘æ¯å®¤ã€‚ä¸œäº¬ç¾½ç”°åˆ°é¦™æ¸¯åçš„é¦™æ¸¯å¿«è¿ï¼Œå»‰èˆªèƒ½åˆ°åœ°æ–¹å°±è¡Œï¼Œä¸€ç›´åœ¨è´­ç‰©ï¼Œç¾½ç”°æˆ‘å»çš„æ—¶å€™æ²¡æœ‰PPSçš„ä¼‘æ¯å®¤ã€‚é¦™æ¸¯å›åˆ°é’å²›åä¸œèˆªä¹°çš„ç•™å­¦ç”Ÿç¥¨ï¼Œä¸Šæµ·è™¹æ¡¥è½¬æœºï¼Œè¡Œæè¿‡å¤šã€‚é¦™æ¸¯æœºåœºchaseçš„ä¼‘æ¯å®¤åƒçš„è¿˜ä¸é”™ï¼Œæœ‰ç‚–ç‰›è‚‰å’Œäº‘åé¢ï¼Œè¿˜æœ‰ä¸€äº›å¸¸è§„å†·ç›˜ã€‚è¿™ä¸ªè¿ªå£«å°¼è”åé£æœºè¿˜ä¸ç®—éš¾çœ‹ï¼ŒæŒºå¥½ã€‚åˆ°ä¸Šæµ·å› ä¸ºé¦™æ¸¯é›·æš´é›¨æ™šç‚¹ï¼Œç»™å…è´¹æ”¹ç­¾äº†æœ€è¿‘çš„ä¸€ç­é£æœºï¼Œç»™äº†ç¬¬ä¸€æ’çš„ä½ç½®ï¼Œä¸€ä¸ªäººå ä»¨åº§è¿˜æ˜¯è›®å®½æ•çš„ã€‚ä¸Šæµ·çš„ä¼‘æ¯å®¤åƒçš„å°±å¾ˆå¸¸è§„äº†ï¼Œä¸€äº›ä¸­å¼ç‚’èœå’Œé¢æ¡ã€‚ä¸œèˆªä»ä¸Šæµ·åˆ°é’å²›è¿™ä¹ˆçŸ­è¿˜ç»™äº†ä¸ªæ­é…è›®ä¸é”™çš„ç®€é¤ï¼ŒæŒºå¥½ã€‚è‡ªä»èƒ¶ä¸œæœºåœºå»ºå¥½æˆ‘å°±ä»è¿™é£åˆ°ç¾å›½ï¼Œä¸¤å¹´äº†ç»ˆäºç¬¬ä¸€æ¬¡åœ¨è¿™é‡Œè½ã€‚ ä»é’å²›å›æ´›æ‰çŸ¶çš„æ—¶å€™ä¹°çš„æ–°èˆªå­¦ç”Ÿç¥¨ã€‚é’å²›å›½é™…å‡ºå‘çš„ä¼‘æ¯å®¤é¤é£Ÿè›®ä¸é”™ï¼Œæ—©é¤æœ‰é¢ã€ç‚’èœã€é¢åŒ…ã€å„ç§é¥®æ–™å’Œé›¶é£Ÿã€‚å‰æ®µé’å²›åˆ°æ–°åŠ å¡æ˜¯é…·èˆªï¼Œç»™å…è´¹é¥®æ–™å’Œé¤é£Ÿï¼ˆæ²¡æå‰å®šå°±åªæœ‰veganï¼‰ï¼Œæ–°åŠ å¡è¿™ä¸ªé¥®æ–™åˆ†çº§è›®å¥½ç©ï¼Œå¸¸æ¸©çš„å¯ä¹ä¸å¥½å–ï¼ˆå°±ç®—å¯å£ä¹Ÿä¸å¥½ä½¿ï¼‰ã€‚åœ¨æ–°åŠ å¡åœç•™ä¸€æ™šç¬¬äºŒå¤©é£æ´›æ‰çŸ¶ï¼Œæ¨Ÿå®œçš„ä¼‘æ¯å®¤ä¹Ÿéå¸¸ä¸é”™ï¼Œæˆ‘ä»¬äºšæ´²æœºåœºçš„ä¼‘æ¯å®¤æ˜¯çœŸçš„è›®å¥½åƒçš„ç§ç±»è¿˜å¤šã€‚é£æ´›æ‰çŸ¶ä¸­é€”åœ¨æˆç”°ç»åœä¸€ä¸ªå°æ—¶ï¼Œæå‰ç½‘ä¸Šè®¢å¥½äº†å…ç¨å•†å“èµ¶ç´§å»ä»˜é’±æ‹¿ç€ä¸œè¥¿èµ°äººï¼Œä¼‘æ¯å®¤é¡ºé“çœ‹äº†ä¸€çœ¼æ„Ÿè§‰è›®å°ä½†æ˜¯åƒçš„æŒºå…¨ã€‚æ–°èˆªçš„é¤é£Ÿè¿˜æ˜¯è›®å¥½åƒçš„ï¼Œé¤åç”œç‚¹æ˜¯æ¢¦é¾™ã€‚ ä¿¡ç”¨å¡ä»Šå¹´æ¯•ç«Ÿè¿›äº†524æœ¬æ¥æƒ³å®‰åˆ†å®ˆå·±ä¸€äº›ï¼Œä½†å®åœ¨æ˜¯å¿ä¸ä½ï¼Œä¸€å…±å¼€äº†4å¼ å¡ï¼Œå¦‚æœæƒ³è¦refer linkçš„å¯ä»¥åœ¨ä¸‹é¢è¯„è®ºåŒºé—®æˆ‘è¦æˆ–è€…ç›´æ¥ç»™æˆ‘å‘é‚®ä»¶ï¼Œæˆ‘å°±ä¸ç›´æ¥æ”¾åœ¨è¿™é‡Œäº†ã€‚2æœˆå¼€äº†AMEX Hilton Aspireç§’æ‰¹CL 8Kã€4æœˆCSPè¯´å¯ä»¥ç ´524ç›´æ¥æ¥äº†ä¸€å¼ ç§’æ‰¹CL10Kã€5æœˆçœ‹åˆ°Biltæœ‰å¼€å¡5Xæ­£å¥½æœ‰å¤§é¢æ¶ˆè´¹å°±å¼€äº†ä¹Ÿæ˜¯ç§’æ‰¹CL 2Kã€æœ€åä¸€å¼ æ˜¯10æœˆä»½AMEX Hiltonç³»åˆ—æ”¹ç‰ˆç¬¬ä¸€å¤©å¼€çš„æ— å¹´è´¹ï¼ˆpendingæ‰“ç”µè¯ç§’æ‰¹CL 240Kï¼‰ã€‚ å…¶ä»–ä»20å¹´å¼€å§‹3å¹´äº†ï¼Œæˆ‘ç»ˆäºé˜³äº†ä¸€æ¬¡ï¼Œè¢«äººä¼ æŸ“çš„ï¼Œä¸Šè¯¾çš„æ—¶å€™åé¢æœ‰äººä¸€ç›´å’³å—½ã€‚æ­£å¥½æœ‰ä¸€æ•´ä¸ªå‘¨æœ«åŠ ä¸Šå‘¨äº”å‘¨ä¸€ã€‚å‰ä¸¤å¤©é«˜çƒ§39åº¦ä»¥ä¸Šå¤–åŠ å¿ƒç‡ä¸€ç›´åœ¨120+ï¼Œé€€çƒ§è¯éƒ½é€€ä¸ä¸‹æ¥ï¼Œäººç›´æ¥åºŸäº†ï¼Œç¬¬ä¸‰å¤©å¼€å§‹å—“å­å¾®ç—›å¼€å§‹é€€çƒ§ï¼Œå››äº”å¤©å½»åº•è½¬é˜´ã€‚ ç”Ÿäº§æ”¶å…¥ä»Šå¹´çš„æ”¶å…¥ç»„æˆä»ç„¶ä¸»è¦ä¸ºå­¦æ ¡å…¼èŒæ”¶å…¥å¤§çº¦10Kå¤šï¼Œéƒ¨åˆ†æ‚å·¥ï¼Œé¢å¤–çš„å°±æ˜¯ä¿¡ç”¨å¡è–…çš„ç¾Šæ¯›ï¼ˆç§¯åˆ†ä¸æŠ˜ç°ï¼‰ï¼Œä»¥åŠå‰ä¸ä¹…capital oneçš„referè–…äº†500åˆ€ã€‚ å¤§æ¦‚çš„ä¿¡ç”¨å¡ç‚¹æ•°æ”¶å…¥ï¼Œå…±çº¦3,400ç¾å…ƒï¼ˆ23,800äººæ°‘å¸ï¼‰ï¼šURç‚¹æ•°æ”¶å…¥-100Kï¼ˆçº¦1,600ç¾å…ƒï¼‰ï¼ŒMRç‚¹æ•°æ”¶å…¥-60Kï¼ˆçº¦720ç¾å…ƒï¼‰ï¼ŒHiltonç‚¹æ•°æ”¶å…¥-270Kï¼ˆçº¦1,080ç¾å…ƒï¼‰ å€ºåŠ¡ä»Šå¹´è¿˜äº†ä¸€å¤§éƒ¨åˆ†è½¦è´·ï¼Œç›®å‰è¿˜å‰©çº¦4000å¤šç¾å…ƒï¼Œç›®æ ‡è¿˜æ˜¯æœ¬ç§‘æ¯•ä¸šå‰è¿˜å®Œå…¨éƒ¨è½¦è´·ï¼Œç›®å‰çœ‹æ¥æ˜¯å¯ä»¥å®Œæˆçš„ã€‚ å­¦ä¸šä»Šå¹´çš„æ‰€æœ‰å­¦æ ¡è¯¾ç¨‹æˆç»©å…¨éƒ½æ˜¯Aå’ŒA+ï¼Œæ€»ä½“æ¥è¯´éå¸¸ä¸é”™ï¼Œç¨³ä½äº†GPAçš„ç¨³å®šå¢é•¿ã€‚ä»Šå¹´åŒæ—¶é¢ä¸´ç ”ç©¶ç”Ÿç”³è¯·ï¼Œç›®å‰å·²ç»æäº¤äº†UCLAã€USCã€UCSDã€Purdueã€NWUäº”æ‰€å­¦æ ¡çš„ç”³è¯·ï¼Œæ¥ä¸‹æ¥è¿˜æœ‰ä¸€å †å­¦æ ¡ç­‰ç€æäº¤ã€‚ ç§‘ç ”ä»Šå¹´å‚ä¸äº†ä¸‰ä¸ªç§‘ç ”é¡¹ç›®ï¼Œä¸¤ä¸ªæœ¬æ ¡å’Œä¸€ä¸ªæ¸¯ä¸­æ–‡æš‘ç ”ã€‚æœªæ¥çš„å‘å±•å’Œç ”ç©¶æ–¹å‘ä¹ŸåŸºæœ¬ç¡®å®šäº†ï¼Œç›®å‰æ¥çœ‹æˆ‘ä¼šä¸“æ³¨äºMLå°¤å…¶æ˜¯DLåœ¨Health Careå’ŒPublic Healthæ–¹é¢çš„ç®—æ³•å¼€å‘ï¼Œç€é‡äºå¯ä»¥è¿ç”¨åˆ°æ¶ˆè´¹é¢†åŸŸå’Œä¸´åºŠçš„ç›¸å…³ç®—æ³•å’Œæ¨¡å‹ã€‚ é¡¹ç›®ä»Šå¹´åšçš„é¡¹ç›®ä¸ç®—å°‘ï¼Œæœ‰å‰ç«¯ä¹Ÿæœ‰åç«¯ï¼Œç”šè‡³è¿˜åšäº†ä¸ªiOSçš„appã€‚åŒæ—¶ä¹Ÿé‡æ„äº†æˆ‘çš„zl-saica.comå’Œzla.pubçš„åšå®¢åç«¯åŠæœåŠ¡å™¨ç»­è´¹ã€‚zla.icuä¹Ÿæ˜¯ä»Šå¹´æ›´æ–°çš„æ–°åŸŸåï¼Œè€çš„bmmw.netä½œä¸ºçŸ­ç½‘å€çš„åŠŸèƒ½ä¹Ÿå…¨éƒ½è¿ç§»åˆ°äº†icuç«™ã€‚appç«™ä¹Ÿç”¨äºäº†é¡¹ç›®å±•ç¤ºï¼Œä¸‹ä¸€æ­¥ä¼šåšä¸ºæˆ‘çš„ä¸ªäººåœ¨çº¿ç®€å†ä½¿ç”¨ã€‚ 2024æ„¿æ™¯æœ€å¤§çš„æ„¿æœ›å½“ç„¶å°±æ˜¯è¢«æœ€å¥½çš„MSé¡¹ç›®å½•å–ï¼Œå¼€å§‹æˆ‘çš„MSç”Ÿæ´»ã€‚è‚¯å®šä¼šæ¢ä¸€ä¸ªæ–°çš„åŸå¸‚ç”Ÿæ´»ï¼Œå¸Œæœ›æ–°çš„åŸå¸‚å’Œæ–°å®¶èƒ½å¤Ÿç¬¦åˆæˆ‘çš„æœŸå¾…å§ã€‚å¦‚æœèƒ½åœ¨æ¯•ä¸šåçš„æš‘å‡æœ‰ä¸€ä¸ªæ¯”è¾ƒå¥½çš„è¡”æ¥researché¡¹ç›®å°±ä¼šæ›´å¥½äº†ã€‚å¦å¤–çš„æ„¿æœ›ï¼Œæˆ‘æœ€äº²å¯†çš„æœ‹å‹ä»¬å½“ç„¶å¾ˆæ¸…æ¥šäº†ï¼Œå¼€å¯å…¨æ–°çš„æˆ‘å¸Œæœ›çš„ç”Ÿæ´»ï¼å¤–åŠ èº«ä½“å¥åº·ã€‚ OKRO1: å¥åº·ç®¡ç† KR1: æ—©ç¡æ—©èµ·23:30-7:30ã€‚ KR2: ç»´æŒç°æœ‰é¥®é£Ÿç»“æ„ï¼Œé«˜è›‹ç™½è´¨ä½è„‚è‚ªã€‚ KR3: å¢åŠ è¿åŠ¨ï¼Œè‡³å°‘æ¯å‘¨1-2æ¬¡ï¼ˆæœ€å¥½2æ¬¡å§â€¦ï¼‰ï¼Œæ¯æ¬¡15minä»¥ä¸Šã€‚ KR2: ä½“é‡ç»´æŒåœ¨115-125æ–¤èŒƒå›´å†…ï¼Œä½“è„‚ç‡ç»´æŒåœ¨15%ä»¥å†…ã€‚ O2: å†…å®¹åˆ›ä½œ KR1: åšå®¢æ¯æœˆè‡³å°‘æ›´æ–°1ç¯‡æ–°æ–‡ç« ï¼Œå…¨å¹´è‡³å°‘20ç¯‡æ–‡ç« ã€‚ KR2: æ‰‹è®°æ¯å‘¨è‡³å°‘2-3ç¯‡ï¼Œæ—…æ¸¸è‡³å°‘1å¤©ä¸€ç¯‡ã€‚ O3: å­¦æœ¯ç›®æ ‡ KR1: è‡³å°‘å‘1ç¯‡æ–‡ç« ã€‚ KR2: å®ŒæˆHonor Programæ¯•ä¸šè¦æ±‚ã€‚ KR3: MSç¬¬ä¸€ä¸ªå­¦æœŸç»´æŒGPAåœ¨3.7ä»¥ä¸Šã€‚ KR4: åœ¨MSç¬¬ä¸€ä¸ªå­¦æœŸæ‰¾åˆ°åšresearchçš„ç»„ã€‚ O4: è´¢åŠ¡ç®¡ç† KR1: ç»§ç»­ç»´æŒè®°è´¦çš„ä¹ æƒ¯ã€‚ KR2: é˜…è¯»3æœ¬å…³äºè´¢åŠ¡è§„åˆ’å’ŒæŠ•èµ„çš„ä¹¦ç±ã€‚ KR3: æ¯æœˆèŠ±é”€æŒ‰ç…§é¢„ç®—ï¼Œåœ¨6æœˆè¿˜å®Œè½¦è´· KR4: è¿˜å®Œè½¦è´·åæ¯æœˆå­˜200ç¾å…ƒï¼Œå…¶ä¸­100åˆ°æ ‡æ™®500å®šæŠ•ã€100åˆ°saving accountã€‚ O5: ä¸ªäººæˆé•¿ KR1: ç»§ç»­å®Œæˆè‡³å°‘10ä¸ªCourseraæˆ–è€…ç±»ä¼¼çš„åœ¨çº¿è¯¾ç¨‹ã€‚ KR2: å­¦ä¹ æ—¥è¯­ï¼Œå®Œæˆ50éŸ³å›¾å’Œduolingoçš„è¯¾ç¨‹ã€‚ KR3: é˜…è¯»è‡³å°‘3æœ¬å’Œä¸“ä¸šé¢†åŸŸä¸ç›¸å…³çš„ä¸“ä¸šä¹¦ç±ã€‚ ğŸ¥³2024æ–°å¹´å¿«ä¹ï¼","categories":[{"name":"éšæƒ³","slug":"éšæƒ³","permalink":"https://www.zl-asica.com/categories/%E9%9A%8F%E6%83%B3/"}],"tags":[{"name":"Summary","slug":"Summary","permalink":"https://www.zl-asica.com/tags/Summary/"}]},{"title":"Deep Learningæ·±åº¦å­¦ä¹ -å­¦ä¹ ç¬”è®°","slug":"deep-learning-notes","date":"2023-11-17T18:18:00.000Z","updated":"2024-02-13T06:37:05.553Z","comments":true,"path":"2023/deep-learning-notes/","permalink":"https://www.zl-asica.com/2023/deep-learning-notes/","excerpt":"This notesâ€™ content are all based on https://www.coursera.org/specializations/deep-learning","text":"This notesâ€™ content are all based on https://www.coursera.org/specializations/deep-learning Latex may have some issues when displaying. 1. Neural Networks and Deep Learning1.1 Introduction to Deep Learning1.1.1 Supervised Learning with Deep Learning Structured Data: Charts. Unstructured Data: Audio, Image, Text. 1.1.2 Scale drives deep learning progress The larger the amount of data, the better the performance of the larger neural network compare to smaller one or supervised learning. Sigmoid change to ReLU will make gradient descent much more faster. Since the gradient will not go to 0 really fast. 1.2 Basics of Neural Network Programming1.2.1 Binary Classification Input: XâˆˆRnxX \\in R^{nx} Output: 0, 1 1.2.2 Logistic Regression Given xx, want y^=P(y=1âˆ£x)\\hat{y} = P(y=1|x) Input: xâˆˆRnxx \\in R^{n_x} Parameters: wâˆˆRnx,bâˆˆRw \\in R^{n_x}, b \\in R Output y^=Ïƒ(wTx+b)\\hat{y} = \\sigma(w^Tx + b) Ïƒ(z)=11+eâˆ’z\\sigma(z)=\\dfrac{1}{1+e^{-z}} If zz large, Ïƒ(z)â‰ˆ11+0â‰ˆ1\\sigma(z)\\approx\\dfrac{1}{1+0}\\approx1 If zz large negative number, Ïƒ(z)â‰ˆ11+Bignumâ‰ˆ0\\sigma(z)\\approx\\dfrac{1}{1+Bignum}\\approx0 Loss (error) function: y^=Ïƒ(wTx+b)\\hat{y} = \\sigma(w^Tx + b), where Ïƒ(z)=11+eâˆ’z\\sigma(z)=\\dfrac{1}{1+e^{-z}} z(i)=wTx(i)+bz^{(i)}=w^Tx^{(i)}+b Want y(i)â‰ˆy^(i)y^{(i)} \\approx \\hat{y}^{(i)} L(y,y^)=âˆ’[ylogâ¡(y^)+(1âˆ’y)logâ¡(1âˆ’y^)]L(y, \\hat{y}) = -[y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})] If y=1:L(y^,y)=âˆ’logâ¡y^y=1: L(\\hat{y}, y)=-\\log{\\hat{y}} &lt;- want logâ¡y^\\log{\\hat{y}} as large as possible, want y^\\hat{y} large If y=0:L(y^,y)=âˆ’logâ¡(1âˆ’y^)y=0: L(\\hat{y}, y)=-\\log{(1-\\hat{y})} &lt;- want logâ¡(1âˆ’y^)\\log{(1-\\hat{y})} as large as possible, want y^\\hat{y} small Cost function J(w,b)=1mâˆ‘âˆ—i=1mL(y^(i),y(i))=âˆ’1mâˆ‘âˆ—i=1mL[y(i)logâ¡(y^(i))+(1âˆ’y(i))logâ¡(1âˆ’y^(i))]J(w, b)=\\dfrac{1}{m}\\sum\\limits*{i=1}^{m}L(\\hat{y}^{(i)},y^{(i)})=-\\dfrac{1}{m}\\sum\\limits*{i=1}^{m}L[y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)})] 1.2.3 Gradient Descent Repeat w:=wâˆ’Î±dJ(w)dww:=w-\\alpha\\dfrac{dJ(w)}{dw}; b:=bâˆ’Î±âˆ‚J(w,b)âˆ‚bb:=b-\\alpha\\dfrac{\\partial J(w,b)}{\\partial b} Î±\\alpha: Learning rate Right side of minimum, dJ(w)dw&gt;0\\dfrac{dJ(w)}{dw} &gt; 0; Left side of minimum, dJ(w)dw&lt;0\\dfrac{dJ(w)}{dw} &lt; 0 Logistic Regression Gradient Descent x1,x2,w1,w2,bx_1,x_2,w_1,w_2,b z=w1x1+w2x2+bz=w_1x_1+w_2x_2+b -->a=Ïƒ(z)a=\\sigma(z) -->L=(a,y)L=(a,y) da=dL(a,y)da=âˆ’ya+1âˆ’y1âˆ’ada=\\dfrac{dL(a,y)}{da}=-\\dfrac{y}{a}+\\dfrac{1-y}{1-a} dL(y,a)da=dda(âˆ’ylogâ¡(a)âˆ’(1âˆ’y)logâ¡(1âˆ’a))\\dfrac{dL(y,a)}{da} = \\dfrac{d}{da}(-y\\log(a) - (1-y)\\log(1-a)) dda(âˆ’ylogâ¡(a))=âˆ’ya\\dfrac{d}{da} (-y\\log(a)) = -\\dfrac{y}{a} dda(âˆ’(1âˆ’y)logâ¡(1âˆ’a))=âˆ’1âˆ’y1âˆ’aÃ—(âˆ’1)=1âˆ’y1âˆ’a\\dfrac{d}{da} (-(1-y)\\log(1-a)) = -\\dfrac{1-y}{1-a} \\times (-1) = \\dfrac{1-y}{1-a} =âˆ’ya+1âˆ’y1âˆ’a=âˆ’yaâˆ’yâˆ’11âˆ’a=-\\dfrac{y}{a} + \\dfrac{1-y}{1-a} = -\\dfrac{y}{a} - \\dfrac{y-1}{1-a} dz=dLdz=dL(a,y)dz=aâˆ’ydz=\\dfrac{dL}{dz}=\\dfrac{dL(a,y)}{dz}=a-y =dLdaâ‹…dadz=\\dfrac{dL}{da}\\cdot\\dfrac{da}{dz} (dadz=a(1âˆ’a)\\dfrac{da}{dz}=a(1-a)) dLdw1=&quot;dw1&quot;=x1â‹…dz\\dfrac{dL}{dw_1}=&quot;dw_1&quot;=x_1\\cdot dz dLdw2=&quot;dw2&quot;=x2â‹…dz\\dfrac{dL}{dw_2}=&quot;dw_2&quot;=x_2\\cdot dz db=dzdb=dz Gradient Descent on mm examples J(w,b)=1mâˆ‘_i=1mL(a(i),y(i))J(w, b)=\\dfrac{1}{m}\\sum\\limits\\_{i=1}^{m}L(a^{(i)},y^{(i)}) âˆ‚âˆ‚wâˆ—1J(w,b)=1mâˆ‘âˆ—i=1mâˆ‚âˆ‚w1L(a(i),y(i))\\dfrac{\\partial}{\\partial w*1}J(w,b)=\\dfrac{1}{m}\\sum\\limits*{i=1}^{m}\\dfrac{\\partial}{\\partial w_1}L(a^{(i)},y^{(i)}) J=0;dw1=0;dw2=0;db=0J=0;dw_1=0;dw_2=0;db=0 for i=1i=1 to mm z(i)=wTx(i)+bz^{(i)}=w^Tx^{(i)}+b a(i)=Ïƒ(z(i))a^{(i)}=\\sigma (z^{(i)}) J+=âˆ’[y(i)loga(i)+(1âˆ’y(i))log(1âˆ’a(i))]J+=-[y^{(i)}loga^{(i)}+(1-y^{(i)})log(1-a^{(i)})] dz(i)=a(i)âˆ’y(i)dz^{(i)}=a^{(i)}-y^{(i)} dw1+=x1(i)dz(i)dw_1+=x_1^{(i)}dz^{(i)} (for n = 2) dw2+=x2(i)dz(i)dw_2+=x_2^{(i)}dz^{(i)} (for n = 2) db+=dz(i)db+=dz^{(i)} J/=m;dw1/=m;dw2/=m;db/=mJ/=m;dw_1/=m;dw_2/=m;db/=m dw1=âˆ‚Jâˆ‚w1;dw2=âˆ‚Jâˆ‚w2dw_1=\\dfrac{\\partial J}{\\partial w_1}; dw_2=\\dfrac{\\partial J}{\\partial w_2} w1:=w1âˆ’Î±dw1w_1:=w_1-\\alpha dw_1 w2:=w2âˆ’Î±dw2w_2:=w_2-\\alpha dw_2 b:=bâˆ’Î±dbb:=b-\\alpha db 1.2.4 Computational Graph J(a,b,c)=3(a+bc)J(a,b,c)=3(a+bc) u=bcu=bc v=a+uv=a+u J=3vJ=3v Left to right computation Derivatives with a Computation Graph dJdv=3\\dfrac{dJ}{dv}=3 dJda=3\\dfrac{dJ}{da}=3 dvda=1\\dfrac{dv}{da}=1 Chain Rule: dJda=dJdvâ‹…dvda\\dfrac{dJ}{da}=\\dfrac{dJ}{dv}\\cdot\\dfrac{dv}{da} dJdu=3;dudb=2;dJdb=6\\dfrac{dJ}{du}=3; \\dfrac{du}{db}=2; \\dfrac{dJ}{db}=6 dudc=3;dJdc=9\\dfrac{du}{dc}=3; \\dfrac{dJ}{dc}=9 1.2.5 Vectorization avoid explicit for-loops. J=0;dw=np.zeros((nx,1));db=0J=0;dw=np.zeros((n_x,1));db=0 for i=1i=1 to mm z(i)=wTx(i)+bz^{(i)}=w^Tx^{(i)}+b a(i)=Ïƒ(z(i))a^{(i)}=\\sigma (z^{(i)}) J+=âˆ’[y(i)loga(i)+(1âˆ’y(i))log(1âˆ’a(i))]J+=-[y^{(i)}loga^{(i)}+(1-y^{(i)})log(1-a^{(i)})] dz(i)=a(i)âˆ’y(i)dz^{(i)}=a^{(i)}-y^{(i)} dw+=x(i)dz(i)dw+=x^{(i)}dz^{(i)} db+=dz(i)db+=dz^{(i)} J/=m;dw/=m;db/=mJ/=m;dw/=m;db/=m Z=np.dot(w.T,x)+bZ=np.dot(w.T,x)+b ; b(1,1)-->Broodcasting Vectorization Logistic Regression dz(1)=a(1)âˆ’y(1);dz(2)=a(2)âˆ’y(2)...dz^{(1)}=a^{(1)}-y^{(1)}; dz^{(2)}=a^{(2)}-y^{(2)}... dz=[dz(1),dz(2),...,dz(m)]dz=[dz^{(1)}, dz^{(2)},...,dz^{(m)}] 1Ã—m1\\times m A=[a(1),a(2),...,a(m)]A=[a^{(1)}, a^{(2)}, ..., a^{(m)}] Y=[y(1),y(2),...,y(m)]Y=[y^{(1)}, y^{(2)}, ..., y^{(m)}] dz=Aâˆ’Y=[a(1)âˆ’y(1),a(2)âˆ’y(2),...]dz=A-Y=[a^{(1)}-y^{(1)}, a^{(2)}-y^{(2)}, ...] Get rid of dbdb and dwdw in for loop db=1mâˆ‘_i=1mdz(i)=1mnp.sum(dz)db=\\dfrac{1}{m}\\sum\\limits\\_{i=1}^{m}dz^{(i)}=\\dfrac{1}{m} np.sum(dz) dw=1mâ‹…Xâ‹…dzT=1m[x(1)...][dz(1)...]=1mâ‹…[x(1)dz(1)+...+x(m)dz(m)]dw=\\dfrac{1}{m}\\cdot X\\cdot dz^T=\\dfrac{1}{m}[x^{(1)}...][dz^{(1)}...]=\\dfrac{1}{m}\\cdot[x^{(1)}dz^{(1)}+...+x^{(m)}dz^{(m)}] nÃ—1n\\times 1 New Form of Logistic Regression Z=wtX+b=np.dot(w.T,X)+bZ=w^tX+b=np.dot(w.T, X)+b A=Ïƒ(Z)A=\\sigma (Z) dz=Aâˆ’Ydz=A-Y dw=1mâ‹…Xâ‹…dZTdw=\\dfrac{1}{m}\\cdot X \\cdot dZ^T db=1mnp.sum(dz)db=\\dfrac{1}{m}np.sum(dz) w:=wâˆ’Î±dww:=w-\\alpha dw b:=bâˆ’Î±dbb:=b-\\alpha db Broadcasting(same as bsxfun in Matlab&#x2F;Octave) (m,n)(m,n)+-\\*/(1,n)(1,n)->(m,n)(m,n) 1->m will be all the same number. (m,n)(m,n)+-\\*/(m,1)(m,1)->(m,n)(m,n) 1->n will be all the same number Donâ€™t use a=np.random.randn(5)a = np.random.randn(5) a.shape=(5,)a.shape = (5,) â€œrank 1 arrayâ€ Use a=np.random.randn(5,1)a = np.random.randn(5,1) or a=np.random.randn(1,5)a = np.random.randn(1,5) Check by assert(a.shape==(5,1))assert(a.shape == (5,1)) Fix rank 1 array by a=a.reshape((5,1))a = a.reshape((5,1)) Logistic Regression Cost Function Lost p(yâˆ£x)=y^y(1âˆ’y^)(1âˆ’y)p(y|x)=\\hat{y}^y(1-\\hat{y})^{(1-y)} If y=1y=1: p(yâˆ£x)=y^p(y|x)=\\hat{y} If y=0y=0: p(yâˆ£x)=(1âˆ’y^)p(y|x)=(1-\\hat{y}) logâ¡p(yâˆ£x)=logâ¡y^y(1âˆ’y^)(1âˆ’y)=ylogâ¡y^+(1âˆ’y)logâ¡(1âˆ’y^)=âˆ’L(y^,y)\\log p(y|x)=\\log \\hat{y}^y(1-\\hat{y})^{(1-y)}=y\\log \\hat{y}+(1-y)\\log(1-\\hat{y})=-L(\\hat{y},y) Cost logâ¡p(labels in training set)=logâ¡Î _i=1mp(y(i),x(i))\\log p(labels\\space in\\space training\\space set)=\\log \\Pi\\_{i=1}^{m}p(y^{(i)},x^{(i)}) logâ¡p(labels in training set)=âˆ‘âˆ—i=1mlogâ¡p(y(i),x(i))=âˆ’âˆ‘âˆ—i=1mL(y^(i),y(i))\\log p(labels\\space in\\space training\\space set)=\\sum\\limits*{i=1}^m\\log p(y^{(i)},x^{(i)})=-\\sum\\limits*{i=1}^mL(\\hat{y}^{(i)},y^{(i)}) Use maximum likelihood estimation(MLE) Cost(minmize): J(w,b)=1mâˆ‘_i=1mL(y^(i),y(i))J(w,b)=\\dfrac{1}{m}\\sum\\limits\\_{i=1}^mL(\\hat{y}^{(i)},y^{(i)}) 1.3 Shallow Neural Networks1.3.1 Neural Network Representation Input layer, hidden layer, output layer a[0]=xa^{[0]}=x -> a[1]=[[a1[1],a2[1],a3[1],a4[1]]]a^{[1]}=[[a^{[1]}_1,a^{[1]}_2,a^{[1]}_3,a^{[1]}_4]] -> a[2]a^{[2]} Layers count by # of hidden layer+# of output layer. x1,x2,x3x_1,x_2,x_3 -> 4 hidden nodes4\\space hidden\\space nodes -> Output layerOutput\\space layer First hidden node: z[1]_1=w[1]T_1+b[1]_1,a[1]_1=Ïƒ(z[1]_1)z^{[1]}\\_1=w^{[1]T}\\_1+b^{[1]}\\_1, a^{[1]}\\_1=\\sigma(z^{[1]}\\_1) Seconde hidden node: z[1]_2=w[1]T_2+b[1]_2,a[1]_2=Ïƒ(z[1]_2)z^{[1]}\\_2=w^{[1]T}\\_2+b^{[1]}\\_2, a^{[1]}\\_2=\\sigma(z^{[1]}\\_2) Third hidden node: z[1]_3=w[1]T_3+b[1]_3,a[1]_3=Ïƒ(z[1]_3)z^{[1]}\\_3=w^{[1]T}\\_3+b^{[1]}\\_3, a^{[1]}\\_3=\\sigma(z^{[1]}\\_3) Forth hidden node: z[1]_4=w[1]T_4+b[1]_4,a[1]_4=Ïƒ(z[1]_4)z^{[1]}\\_4=w^{[1]T}\\_4+b^{[1]}\\_4, a^{[1]}\\_4=\\sigma(z^{[1]}\\_4) Vectorization w[1]=[âˆ’w[1]T_1âˆ’âˆ’w[1]T_2âˆ’âˆ’w[1]T_3âˆ’âˆ’w[1]T_4âˆ’](4,3)matrixw^{[1]}=\\begin{gathered}\\begin{bmatrix}-w^{[1]T}\\_1- \\\\ -w^{[1]T}\\_2- \\\\ -w^{[1]T}\\_3- \\\\ -w^{[1]T}\\_4- \\end{bmatrix}\\end{gathered} (4,3)matrix z[1]=[âˆ’w[1]T_1âˆ’âˆ’w[1]T_2âˆ’âˆ’w[1]T_3âˆ’âˆ’w[1]T_4âˆ’]â‹…[x1x2x3]+[b[1]_1b[1]_2b[1]_3b[1]_4]=[w[1]T_1â‹…x+b[1]_1w[1]T_2â‹…x+b[1]_2w[1]T_3â‹…x++b[1]_3w[1]T_4â‹…x+b[1]_4]=[z[1]_1z[1]_2z[1]_3z[1]_4]z^{[1]}=\\begin{gathered}\\begin{bmatrix}-w^{[1]T}\\_1- \\\\ -w^{[1]T}\\_2- \\\\ -w^{[1]T}\\_3- \\\\ -w^{[1]T}\\_4- \\end{bmatrix}\\end{gathered}\\cdot \\begin{gathered}\\begin{bmatrix}x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\end{gathered} + \\begin{gathered}\\begin{bmatrix}b^{[1]}\\_1 \\\\ b^{[1]}\\_2 \\\\b^{[1]}\\_3 \\\\ b^{[1]}\\_4 \\end{bmatrix}\\end{gathered} =\\begin{gathered}\\begin{bmatrix}w^{[1]T}\\_1\\cdot x+b^{[1]}\\_1 \\\\ w^{[1]T}\\_2\\cdot x+b^{[1]}\\_2 \\\\ w^{[1]T}\\_3\\cdot x++b^{[1]}\\_3 \\\\ w^{[1]T}\\_4\\cdot x+b^{[1]}\\_4 \\end{bmatrix}\\end{gathered}=\\begin{gathered}\\begin{bmatrix}z^{[1]}\\_1 \\\\ z^{[1]}\\_2 \\\\z^{[1]}\\_3 \\\\ z^{[1]}\\_4 \\end{bmatrix}\\end{gathered} a[1]=[a[1]_1a[1]_2a[1]_3a[1]_4]=Ïƒ(z[1])a^{[1]}=\\begin{gathered}\\begin{bmatrix}a^{[1]}\\_1 \\\\ a^{[1]}\\_2 \\\\a^{[1]}\\_3 \\\\ a^{[1]}\\_4 \\end{bmatrix}\\end{gathered}=\\sigma(z^{[1]}) z[2]=W[2]â‹…a[1]+b[2]z^{[2]}=W^{[2]}\\cdot a^{[1]}+b^{[2]} (1,1),(1,4),(4,1),(1,1)(1, 1),(1, 4),(4, 1),(1, 1) a[2]=Ïƒ(z[2])a^{[2]}=\\sigma(z^{[2]}) (1,1),(1,1)(1,1),(1,1) a[2](i)a^{[2](i)}: layer 22; example ii for i&#x3D;1 to m: z[1](i)=W[1]â‹…x(i)+b[1]z^{[1](i)}=W^{[1]}\\cdot x(i)+b^{[1]} a[1](i)=Ïƒ(z[1](i))a^{[1](i)}=\\sigma(z^{[1](i)}) z[2](i)=W[2]â‹…a[1](i)+b[2]z^{[2](i)}=W^{[2]}\\cdot a^{[1](i)}+b^{[2]} a[2](i)=Ïƒ(z[2](i))a^{[2](i)}=\\sigma(z^{[2](i)}) Vectorizing of the above for loop X=[âˆ£âˆ£âˆ£âˆ£x(1),x(2),...,x(m)âˆ£âˆ£âˆ£âˆ£](nx,m)matrixX=\\begin{gathered}\\begin{bmatrix}| &amp; | &amp; | &amp; | \\\\ x^{(1)}, &amp; x^{(2)}, &amp; ..., &amp; x^{(m)} \\\\ | &amp; | &amp; | &amp; |\\end{bmatrix}\\end{gathered} (n_x,m)matrix n is different hidden units Z[1]=W[1]â‹…X+b[1]Z^{[1]}=W^{[1]}\\cdot X+b^{[1]} A[1]=Ïƒ(Z[1])A^{[1]}=\\sigma(Z^{[1]}) Z[2]=W[2]â‹…A[1]+b[2]Z^{[2]}=W^{[2]}\\cdot A^{[1]}+b^{[2]} A[2]=Ïƒ(Z[2])A^{[2]}=\\sigma(Z^{[2]}) hrizontally: training examples; vertically: hidden units 1.3.2 Activation Functions g[i]g^{[i]}: activation function of layer ii Sigmoid: a=11+e[âˆ’z]a=\\dfrac{1}{1+e^{[-z]}} Tanh: a=ezâˆ’e[âˆ’z]ez+e[âˆ’z]a=\\dfrac{e^z-e^{[-z]}}{e^z+e^{[-z]}} ReLU: a=max(0,z)a=max(0,z) Leaky ReLu: a=max(0.01z,z)a=max(0.01z, z) Rules to choose activation function Output is between {0, 1}, choose sigmoid. Default choose ReLu. Why need non-liner activation function Use linear hidden layer will be useless to have multiple hidden layers. It will become a=wâ€²x+bâ€²a=w&#x27;x+b&#x27;. Linear may sometime use at output layer but with non-linear at hidden layers. 1.3.3 Forward and Backward Propogation Derivative of activation function Sigmoid: gâ€²(z)=ddzg(z)=11+e[âˆ’z](1âˆ’11+e[âˆ’z])=g(z)(1âˆ’g(z))=a(1âˆ’a)g&#x27;(z)=\\dfrac{d}{dz}g(z)=\\dfrac{1}{1+e^{[-z]}}(1-\\dfrac{1}{1+e^{[-z]}})=g(z)(1-g(z))=a(1-a) Tanh: gâ€²(z)=ddzg(z)=1âˆ’(tanh(z))2g&#x27;(z)=\\dfrac{d}{dz}g(z)=1-(tanh(z))^2 ReLU: gâ€²(z)={0if z&lt;01if zâ‰¥0\\usepackageundefined\\usepackageif z=0g&#x27;(z)=\\left\\{\\begin{array}{lr}0&amp;if \\space z&lt;0 \\\\1&amp;if \\space z\\geq0\\\\\\usepackage{undefined}&amp;\\usepackage{if \\space z=0}\\end{array}\\right. Leaky ReLU: gâ€²(z)={0.01if z&lt;01if zâ‰¥0g&#x27;(z)=\\left\\{\\begin{array}{lr}0.01&amp;if \\space z&lt;0 \\\\1&amp;if \\space z\\geq0\\end{array}\\right. Gradient descent for neural networks Parameters: w[1](n[1],n[2]),b[1](n[2],1),w[2](n[2],n[1]),b[2](n[2],1)w^{[1]}(n^{[1]},n^{[2]}), b^{[1]}(n^{[2]},1),w^{[2]}(n^{[2]},n^{[1]}), b^{[2]}(n^{[2]},1) nx=n[0],n[1],n[2]=1n_x=n^{[0]},n^{[1]},n^{[2]}=1 Cost function: J(w[1],b[1],w[2],b[2])=1mâˆ‘_i=1nL(y^,y)J(w^{[1]}, b^{[1]},w^{[2]}, b^{[2]})=\\dfrac{1}{m}\\sum\\limits\\_{i=1}^nL(\\hat{y},y) Forward propagation: Z[1]=W[1]â‹…X+b[1]Z^{[1]}=W^{[1]}\\cdot X+b^{[1]} A[1]=g[1](Z[1])A^{[1]}=g^{[1]}(Z^{[1]}) Z[2]=W[2]â‹…A[1]+b[2]Z^{[2]}=W^{[2]}\\cdot A^{[1]}+b^{[2]} A[2]=g[2](Z[2])=Ïƒ(Z[2])A^{[2]}=g^{[2]}(Z^{[2]})=\\sigma(Z^{[2]}) Back Propogation: dZ[2]=A[2]âˆ’YdZ^{[2]}=A^{[2]}-Y Y=[y(1),y(2),...,y(m)]Y=[y^{(1)},y^{(2)},...,y^{(m)}] dW[2]=1mdZ[2]A[1]TdW^{[2]}=\\dfrac{1}{m}dZ^{[2]}A^{[1]T} db[2]=1mnp.sum(dZ[2],axis=1,keepdims=True)db^{[2]}=\\dfrac{1}{m}np.sum(dZ^{[2]},axis=1,keepdims=True) dZ[1]=W[2]TdZ[2]\\*gâ€²[1](Z1)dZ^{[1]}=W^{[2]T}dZ^{[2]}\\*g&#x27;^{[1]}(Z^{1}) (n[1],m)âˆ’&gt;elementâˆ’wise productâˆ’&gt;(n[1],m)(n^{[1]},m)-&gt;element-wise\\space product-&gt;(n^{[1]},m) dW[1]=1mdZ[1]XTdW^{[1]}=\\dfrac{1}{m}dZ^{[1]}X^{T} db[1]=1mnp.sum(dZ[1],axis=1,keepdims=True)db^{[1]}=\\dfrac{1}{m}np.sum(dZ^{[1]},axis=1,keepdims=True) Random Initialization x1,x2âˆ’&gt;a1[1],a2[1]âˆ’&gt;a1[2]âˆ’&gt;y^x_1,x_2-&gt;a_1^{[1]},a_2^{[1]}-&gt;a_1^{[2]}-&gt;\\hat{y} w[1]=np.random.randn((2,2))\\*0.01w^{[1]}=np.random.randn((2,2))\\*0.01 b[1]=np.zeros((2,1))b^{[1]}=np.zeros((2,1)) w[2]=np.random.randn((1,2))\\*0.01w^{[2]}=np.random.randn((1,2))\\*0.01 b[2]=0b^{[2]}=0 1.4 Deep Neural Networks1.4.1 Deep L-Layer Neural Network Deep neural network notation L=4L=4 (#layers) n[l]=# units in layer ln^{[l]}= \\#\\space units\\space in\\space layer\\space l n[1]=5,n[2]=5,n[3]=3,n[4]=n[l]=1n^{[1]}=5,n^{[2]}=5,n^{[3]}=3,n^{[4]}=n^{[l]}=1 n[0]=nx=3n^{[0]}=n_x=3 a[l]=activations in layer la^{[l]}=activations\\space in\\space layer\\space l a[l]=g[l](z[l]), w[l]=weights for z[l], b[l]=bias for z[l]a^{[l]}=g^{[l]}(z^{[l]}),\\space w^{[l]}=weights\\space for\\space z^{[l]},\\space b^{[l]}=bias\\space for\\space z^{[l]} x=a[0], y^=alx=a^{[0]},\\space \\hat{y}=a^{l} 1.4.2 Forward Propagation in a Deep Network General: Z[l]=w[l]A[lâˆ’1]+b[l],A[l]=g[l](Z[l])Z^{[l]}=w^{[l]}A^{[l-1]}+b^{[l]}, A^{[l]}=g^{[l]}(Z^{[l]}) x:z[1]=w[1]a[0]+b[1],a[1]=g[1](z[1])x: z^{[1]}=w^{[1]}a^{[0]}+b^{[1]}, a^{[1]}=g^{[1]}(z^{[1]}) a[0]=Xa^{[0]}=X z[2]=w[2]a[1]+b[2],a[1]=g[2](z[2])z^{[2]}=w^{[2]}a^{[1]}+b^{[2]}, a^{[1]}=g^{[2]}(z^{[2]}) â€¦ z[4]=w[4]a[3]+b[4],a[4]=g[4](z[4])=y^z^{[4]}=w^{[4]}a^{[3]}+b^{[4]}, a^{[4]}=g^{[4]}(z^{[4]})=\\hat{y} Vectorizing: Z[1]=w[1]A[0]+b[1],A[1]=g[1](Z[1])Z^{[1]}=w^{[1]}A^{[0]}+b^{[1]}, A^{[1]}=g^{[1]}(Z^{[1]}) A[0]=XA^{[0]}=X Z[2]=w[2]A[1]+b[2],A[2]=g[2](Z[2])Z^{[2]}=w^{[2]}A^{[1]}+b^{[2]}, A^{[2]}=g^{[2]}(Z^{[2]}) Y^=g(Z[4])=A[4]\\hat{Y}=g(Z^{[4]})=A^{[4]} Matrix dimensions z[1]=w[1]â‹…x+b[1]z^{[1]}=w^{[1]}\\cdot x+b^{[1]} z[1]=(3,1),w[1]=(3,2),x=(2,1),b[1]=(3,1)z^{[1]}=(3,1),w^{[1]}=(3,2),x=(2,1),b^{[1]}=(3,1) z[1]=(n[1],1),w[1]=(n[1],n[0]),x=(n[0],1),b[1]=(n[1],1)z^{[1]}=(n^{[1]},1),w^{[1]}=(n^{[1]},n^{[0]}),x=(n^{[0]},1),b^{[1]}=(n^{[1]},1) w[l]/dw[l]=(n[l],n[lâˆ’1]),b[l]/db[l]=(n[l],1)w^{[l]}/dw^{[l]}=(n^{[l]},n^{[l-1]}),b^{[l]}/db^{[l]}=(n^{[l]},1) z[l],a[l]=(n[l],1),Z[l]/dZ[l],A[l]/dA[l]=(n[l],1)z^{[l]},a^{[l]}=(n^{[l]},1),Z^{[l]}/dZ^{[l]},A^{[l]}/dA^{[l]}=(n^{[l]},1) l=0,A[0]=X=(n[0],m)l=0, A^{[0]}=X=(n^{[0]},m) Why deep representation? Earier layers learn simple features; later deeper layers put together to detect more complex things. Circuit theory and deep learning: Informally: There are functions you can compute with a â€œsmallâ€ L-layer deep neural network that shallower networks require exponentially more hidden units to compute. 1.4.3 Building Blocks of Deep Neural Networks Forward and backward functions Layer l:w[l],b[l]l:w^{[l]},b^{[l]} Forward: Input a[lâˆ’1]a^{[l-1]}, output a[l]a^{[l]} z[l]:w[l]a[lâˆ’1]+b[l]z^{[l]}:w^{[l]}a^{[l-1]}+b^{[l]} cache z[l]cache\\space z^{[l]} a[l]:g[l](z[l])a^{[l]}:g^{[l]}(z^{[l]}) Backward: Input da[l],cache(z[l])da^{[l]}, cache(z^{[l]}), output da[lâˆ’1],dw[l],db[l]da^{[l-1]},dw^{[l]},db^{[l]} One iteration of gradient descent of neural network How to implement? Forward propagation for layer ll Input a[lâˆ’1]a^{[l-1]}, output a[l],cache (z[l])a^{[l]},cache\\space (z^{[l]}) z[l]=w[l]a[lâˆ’1]+b[l]z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]} a[l]=g[l](z[l])a^{[l]}=g^{[l]}(z^{[l]}) Vectoried Z[l]=W[l]A[lâˆ’1]+b[l]Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]} A[l]=g[l](Z[l])A^{[l]}=g^{[l]}(Z^{[l]}) Backward propagation for layer ll Input da[l],cache(z[l])da^{[l]}, cache(z^{[l]}), output da[lâˆ’1],dw[l],db[l]da^{[l-1]},dw^{[l]},db^{[l]} dz[l]=da[l]\\*gâ€²[l](z[l])dz^{[l]}=da^{[l]}\\*g&#x27;^{[l]}(z^{[l]}) dw[l]=dz[l]â‹…a[lâˆ’1]dw^{[l]}=dz^{[l]}\\cdot a^{[l-1]} db[l]=dz[l]db^{[l]}=dz^{[l]} da[lâˆ’1]=w[l]Tâ‹…dz[l]da^{[l-1]}=w^{[l]T}\\cdot dz^{[l]} Vectorized: dZ[l]=dA[l]\\*gâ€²[l](Z[l])dZ^{[l]}=dA^{[l]}\\*g&#x27;^{[l]}(Z^{[l]}) dW[l]=1mdZ[l]A[lâˆ’1]TdW^{[l]}=\\dfrac{1}{m}dZ^{[l]}A^{[l-1]T} db[l]=1mnp.sum(dZ[l],axis=1,keepdims=True)db^{[l]}=\\dfrac{1}{m}np.sum(dZ^{[l]},axis=1,keepdims=True) dA[lâˆ’1]=W[l]Tâ‹…dZ[l]dA^{[l-1]}=W^{[l]T}\\cdot dZ^{[l]} 1.4.4 Parameters vs. Hyperparameters Parameters: W[1],b[1],W[2],b[2],...W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]},... Hyperparameters (will affect&#x2F;control&#x2F;determine parameters): learning rate Î±\\alpha # iterations # of hidden units n[1],n[2],...n^{[1]},n^{[2]},... # of hidden layers Choice of activation function Later: momemtum, minibatch size, regularization parameters,â€¦ II. Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization2.1 Practical Aspects of Deep Learning2.1.1 Train &#x2F; Dev &#x2F; Test sets Big data may need only 1% or even less dev&#x2F;test sets. Mismatched: Make sure dev&#x2F;test come from same distribution Not having a test set might be okay. (Only dev set.) 2.1.2 Bias &#x2F; Variance Assume optimal (Bayes) error: â‰ˆ0%\\approx0\\% High bias (underfitting): The prediction cannot classify different elemets as we want. Training set error 15%15\\%, Dev set error 16%16\\%. Training set error 15%15\\%, Dev set error 30%30\\%. â€œjust rightâ€: The prediction perfectly classify different elemets as we want. Training set error 0.5%0.5\\%, Dev set error 1%1\\%. High variance (overfitting): The prediction 100% classify different elemets. Training set error 1%1\\%, Dev set error 11%11\\%. Training set error 15%15\\%, Dev set error 30%30\\%. 2.1.3 Basic Recipe for Machine Learning2.1.3.1 Basic Recipe High bias(training data performance) Bigger network Train longer (NN architecture search) High variance (dev set performance) More data Regulairzation (NN architecture search) 2.1.3.2 Regularization Logistic regression. minâ¡_w,bJ(w,b)\\min\\limits\\_{w,b}J(w,b) wâˆˆRnx,bâˆˆRw\\in\\mathbb{R}^{n_x}, b\\in\\mathbb{R} Î»=regularization parameter\\lambda=regularization\\space parameter J(w,b)=1mâˆ‘_i=1mL(y^(i),y(i))+Î»2mâˆ£âˆ£w2âˆ£âˆ£_2J(w,b)=\\dfrac{1}{m}\\sum\\limits\\_{i=1}^mL(\\hat{y}^{(i)},y^{(i)})+\\dfrac{\\lambda}{2m}||w^2||\\_2 L2 regularization âˆ£âˆ£w2âˆ£âˆ£2=âˆ‘j=1nxwj2=wTw||w^2||_2=\\sum\\limits_{j=1}^{n_x}w_j^2=w^Tw L1 regularization Î»2mâˆ‘_j=1nxâˆ£wjâˆ£=Î»2mâˆ£âˆ£wâˆ£âˆ£_1\\dfrac{\\lambda}{2m}\\sum\\limits\\_{j=1}^{n_x}|w_j|=\\dfrac{\\lambda}{2m}||w||\\_1 ww will be spouse(for L1) (will have lots of 0 in it, only help a little bit) Neural network J(w[1],b[1],...,w[l],b[l])=1mâˆ‘âˆ—i=1mL(y^(i),y(i))+Î»2mâˆ‘âˆ—l=1lâˆ£âˆ£w2âˆ£âˆ£_FJ(w^{[1]},b^{[1]},...,w^{[l]},b^{[l]})=\\dfrac{1}{m}\\sum\\limits*{i=1}^{m}L(\\hat{y}^{(i)},y^{(i)})+\\dfrac{\\lambda}{2m}\\sum\\limits*{l=1}^{l}||w^2||\\_F âˆ£âˆ£w[l]âˆ£âˆ£F2=âˆ‘i=1n[lâˆ’1]âˆ‘âˆ—j=1n[l](wâˆ—ij[l])2||w^{[l]}||_F^2=\\sum\\limits_{i=1}^{n^{[l-1]}}\\sum\\limits*{j=1}^{n^{[l]}}(w*{ij}^{[l]})^2 w:(w[l],w[lâˆ’1])w: (w^{[l]},w^{[l-1]}) Frobenius norm: Square root of square sum of all elements in a matrix. dw[l]=(from backprop)+Î»mw[l]dw^{[l]}=(from\\space backprop)+\\dfrac{\\lambda}{m}w^{[l]} w[l]:=w[l]âˆ’Î±dw[l]w^{[l]}:=w^{[l]}-\\alpha dw^{[l]} (keep the same) Weight decay w[l]:=w[l]âˆ’Î±[(from backprop)+Î»mw[l]]w^{[l]}:=w^{[l]}-\\alpha[(from\\space backprop)+\\dfrac{\\lambda}{m}w^{[l]}] â€‹ =w[l]âˆ’Î±Î»mw[l]âˆ’Î±(from backprop)=w^{[l]}-\\dfrac{\\alpha\\lambda}{m}w^{[l]}-\\alpha(from\\space backprop) â€‹ =(1âˆ’Î±Î»m)w[l]âˆ’Î±(from backprop)=(1-\\dfrac{\\alpha\\lambda}{m})w^{[l]}-\\alpha(from\\space backprop) How does regularization prevent overfitting: Î»\\lambda bigger w[l]w^{[l]} smaller z[l]z^{[l]} smaller, which will make the activation function nearly linear(take tanh as an example). This will cause the network really hard to draw boundary with curve. Dropout regularization Implementing dropout(â€œInverted dropoutâ€) Illustrate with layer l=3l=3 keepâˆ’prob=0.8keep-prob=0.8 (means 0.2 chance get dropout&#x2F;be 0 out) d3=np.random.rand(a3.shape[0],a3.shape[1])&lt;keepâˆ’probd3 = np.random.rand(a3.shape[0],a3.shape[1]) &lt; keep-prob #This will set d3 to be a same shape matrix as a3 with True (1), False (0) value. a3=np.multiply(a3,d3)a3 = np.multiply(a3, d3) #a3\\*=d3; This will let some neruons been dropout a3/=keepâˆ’proba3/=keep-prob #inverted dropout, keep the total avtivation the same before and after dropout. Why work: Canâ€™t rely on any one feature, so have to spread out weights.(shrink weights) First make sure the J is decreasing during iteration, then turn on dropout. Data augmentation Image: crop, flop, twistâ€¦ Early stopping Mid-size âˆ£âˆ£wâˆ£âˆ£_F2||w||\\_F^2 May caused optimize cost function and not overfir at the same time. Orthogonalization Only consider optimize cost function or consider not overfit at one time. 2.1.3.3 Setting up your optimization problem Normalizing training sets x=[x1x2]x=\\begin{gathered}\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}\\end{gathered} Subtract mean: Î¼=1mâˆ‘_i=1mx(i)\\mu=\\dfrac{1}{m}\\sum\\limits\\_{i=1}^{m}x^{(i)} x:=xâˆ’Î¼x:=x-\\mu Normalize variance: Ïƒ2=1mâˆ‘_i=1mx(i)âˆ—âˆ—2\\sigma^2=\\dfrac{1}{m}\\sum\\limits\\_{i=1}^{m}x^{(i)}**2 \"**\" element-wise x/=Ïƒ2x/=\\sigma^2 Use same Î¼,Ïƒ2\\mu,\\sigma^2 to normalize test set. Why normalize inputs? When inputs in very different scales will help a lot for performance and gradient descent&#x2F;learning rate. Vanishing&#x2F;exploding gradients w[l]&gt;Iw^{[l]}&gt;I Just slightly, will make the gradient increase really fast (exploding). w[l]&lt;Iw^{[l]}&lt;I Just slightly, will make the gradient decrease really slow (varnishing). Weight initalization (Single neuron) large nn (number of input features) â€“&gt; smaller wiw_i Variance(w:)=1nVariance(w:)=\\dfrac{1}{n} (sigmoid/tanh) ReLU: 2n\\dfrac{2}{n} (variance can be a hyperparameter, DO NOT DO THAT) w[l]=np.random.randn(shapeOfMatrix)\\*np.sqrt(1n[lâˆ’1])w^{[l]}=np.random.randn(shapeOfMatrix)\\*np.sqrt(\\dfrac{1}{n^{[l-1]}}) ReLU: 2n[lâˆ’1]\\dfrac{2}{n^{[l-1]}} Xavier initialization: 1n[lâˆ’1])\\sqrt{\\dfrac{1}{n^{[l-1]}})} Sometime 2n[lâˆ’1]+n[l])\\sqrt{\\dfrac{2}{n^{[l-1]}+n^{[l]}})} Numerical approximation of gradients f(Î¸+Ïµ)âˆ’f(Î¸âˆ’Ïµ)2Ïµ\\dfrac{f(\\theta+\\epsilon)-f(\\theta-\\epsilon)}{2\\epsilon} Gradient checking (Grad check) Take W[1],b[1],...,W[L],b[L]W^{[1]},b^{[1]},...,W^{[L]},b^{[L]} and reshape into a big vector Î¸\\theta. Take dW[1],db[1],...,dW[L],db[L]dW^{[1]},db^{[1]},...,dW^{[L]},db^{[L]} and reshape into a big vector dÎ¸d\\theta. for each i: dÎ¸_approx[i]=J(Î¸1,Î¸2,...,Î¸i+Ïµ,...)âˆ’J(Î¸1,Î¸2,...,Î¸iâˆ’Ïµ,...)2Ïµâ‰ˆdÎ¸[i]=âˆ‚Jâˆ‚Î¸id\\theta\\_{approx}[i]=\\dfrac{J(\\theta_1,\\theta_2,...,\\theta_i+\\epsilon,...)-J(\\theta_1,\\theta_2,...,\\theta_i-\\epsilon,...)}{2\\epsilon}\\approx d\\theta[i]=\\dfrac{\\partial J}{\\partial \\theta_i} Check Euclidean distance âˆ£âˆ£dÎ¸âˆ—approxâˆ’dÎ¸âˆ£âˆ£_2âˆ£âˆ£dÎ¸âˆ—approxâˆ£âˆ£_2+âˆ£âˆ£dÎ¸âˆ£âˆ£_2\\dfrac{||d\\theta*{approx}-d\\theta||\\_2}{||d\\theta*{approx}||\\_2+||d\\theta||\\_2} (âˆ£âˆ£.âˆ£âˆ£_2||.||\\_2 is Euclidean norm, sqare root of the sum of all elementsâ€™ power of 2) take Ïµ=10âˆ’7\\epsilon=10^{-7}, if above Euclidean distance is â‰ˆ10âˆ’7\\approx10^{-7} or smaller, is great. If is 10âˆ’510^{-5} or bigger may need to check. If is 10âˆ’310^{-3} or bigger may need to worry, maybe a bug. Check which i approx is difference between the real value. notes: Donâ€™t use in training - only to debug. If algorithm fails grad check, look at components to try to identify bug. Remember regularization. (include the Î»2m\\dfrac{\\lambda}{2m}) Doesnâ€™t work with dropout. (since is random, implement without dropout) Run at random initialization; perhaps again after some training. (not work when w,bâ‰ˆ0w,b\\approx0) 2.2 Optimization Algorithms2.2.1 Mini-batch gradient descent Batch vs. mini-batch gradient descent Normal batch may have large amount of data like millions of elements. set m=5,000,000m=5,000,000 X=[x(1),x(2),x(3),...,x(1000),x(1001),...,x(2000),...,x(m)](nx,m)X=[x^{(1)},x^{(2)},x^{(3)},...,x^{(1000)},x^{(1001)},...,x^{(2000)},...,x^{(m)}] (n_x,m) Y=[y(1),y(2),y(3),...,y(m)](1,m)Y=[y^{(1)},y^{(2)},y^{(3)},...,y^{(m)}] (1,m) Mini-batches make 1,000 xx each. Mini-batch number t:X{t},Y{t}t:X^{\\{t\\}},Y^{\\{t\\}} x(i)x^{(i)} ith in trainning set, z[l]z^{[l]} layer in network X{t}X^{\\{t\\}} batch in mini-batch X=[X{1},X{2},...,X{5000}]X = [X^{\\{1\\}},X^{\\{2\\}},...,X^{\\{5000\\}}] Y=[Y{1},Y{2},Y{3},...,Y(5,000)]Y=[Y^{\\{1\\}},Y^{\\{2\\}},Y^{\\{3\\}},...,Y^{(5,000)}] Mini-batch gradient descent 1 step of gradient descent using X{t},Y{t}X^{\\{t\\}},Y^{\\{t\\}} (1000) 1 epoch: single pass through training set. for t=1,...,5000for\\space t=1,...,5000 Forward prop on X{t}X^{\\{t\\}} Z[1]=W[1]X{t}+b[1]Z^{[1]}=W^{[1]}X^{\\{t\\}}+b^{[1]} A[1]=g[1](Z[1])A^{[1]}=g^{[1]}(Z^{[1]}) â€¦ A[l]=g[l](Z[l])A^{[l]}=g^{[l]}(Z^{[l]}) Compute cost J{t}=11000âˆ‘âˆ—i=1lL(y^(i),y(i))+Î»2â‹…1000âˆ‘âˆ—l=1lâˆ£âˆ£w[l]âˆ£âˆ£_F2J^{\\{t\\}}=\\dfrac{1}{1000}\\sum\\limits*{i=1}^{l}L(\\hat{y}^{(i)},y^{(i)})+\\dfrac{\\lambda}{2\\cdot1000}\\sum\\limits*{l=1}^{l}||w^{[l]}||\\_F^2 y^(i),y(i)\\hat{y}^{(i)},y^{(i)} --> from X{t},Y{t}X^{\\{t\\}},Y^{\\{t\\}} Backprop to compute gradient cost J{t} (using (X{t},Y{t}))J^{\\{t\\}}\\space (using\\space (X^{\\{t\\}},Y^{\\{t\\}})) w[l]:=w[l]âˆ’Î±dw[l],b[l]:=b[l]âˆ’Î±db[l]w^{[l]}:=w^{[l]}-\\alpha dw^{[l]}, b^{[l]}:=b^{[l]}-\\alpha db^{[l]} Understanding mini-batch gradient descent If mini-batch size&#x3D;m:batch gradient descent (Too long per iteration).â€“(X{t},Y{t})=(X,Y)(X^{\\{t\\}},Y^{\\{t\\}})=(X,Y) If mini-batch size&#x3D;1:Stochatic gradient descent (noisy, not converge, loos speedup from vectorization).â€“ Every example is it own mini-batch. In practice: select in-between 1 and m. Get lots of vectorization Make progress without needing to wait entire training set. Choosing mini-batch size No need for small training set (m&lt;2000m&lt;2000) Typical mini-batch size: 64, 128, 256, 512. (Use power of 2) Make sure minibatch fir in CPU&#x2F;GPU memory. 2.2.2 Exponentially weighted averages Vâˆ—t=Î²Vâˆ—tâˆ’1+(1âˆ’Î²)Î¸tV*t = \\beta V*{t-1} + (1 - \\beta) \\theta_t VtV_t is the weighted average at time tt. Î¸t\\theta_t is the actual observed value at time tt. Î²\\beta is the decay rate (usually between 0 and 1). V_tâˆ’1V\\_{t-1} is the weighted average at the previous time step. Impact of Decay Rate Î²\\beta: The value of Î²\\beta significantly affects the smoothness of the weighted average curve: A larger Î²\\beta makes the curve smoother, as it gives more weight to past observations, thereby reducing the impact of recent changes on the weighted average. A smaller Î²\\beta makes the curve more responsive to recent changes, as it gives more weight to recent observations. Interpretation of (1âˆ’Ïµ)1Ïµsome constant=1e\\dfrac{(1-\\epsilon)^{\\frac{1}{\\epsilon}}}{\\text{some constant}} = \\dfrac{1}{e} Defining Ïµ\\epsilon as 1âˆ’Î²1 - \\beta provides insight into how the influence of past data gradually diminishes as Î²\\beta approaches 1 (i.e., Ïµ\\epsilon approaches 0). As Ïµ\\epsilon approaches 0, (1âˆ’Ïµ)1Ïµ(1-\\epsilon)^{\\frac{1}{\\epsilon}} approaches 1e\\dfrac{1}{e}, indicating that even though past data is given more weight (high Î²\\beta), its actual impact on the current value is decreasing. Implementation v_Î¸:=0v\\_{\\theta}:=0 Repear for each day: Get the next Î¸t\\theta_t vâˆ—Î¸:=Î²vâˆ—Î¸+(1âˆ’Î²)Î¸tv*\\theta:=\\beta v*\\theta+(1-\\beta)\\theta_t Bias correction in exponentially weighted averages Bias correction is applied to counteract the initial bias in exponentially weighted averages, especially when the number of data points is small or at the start of the calculation. vt1âˆ’Î²t\\dfrac{v_t}{1-\\beta^t} Here, vtv_t is the uncorrected exponentially weighted average at time tt, and Î²\\beta is the decay rate. It ensures that the moving averages are not underestimated, particularly when Î²\\beta is high and in the early stages of the iteration. With iteration goes on, the affect of this correction will become smaller since Î²t\\beta^t is closer to 1. Gradient descent with momentum On iteration t: Compute dw,dbdw, db on current mini-batch (whole batch if not using mini-batch) vâˆ—dw=Î²vâˆ—dw+(1âˆ’Î²)dwv*{dw}=\\beta v*{dw}+(1-\\beta)dw vâˆ—db=Î²vâˆ—db+(1âˆ’Î²)dbv*{db}=\\beta v*{db}+(1-\\beta)db w:=wâˆ’Î±vâˆ—dw,b:=bâˆ’Î±vâˆ—dbw:=w-\\alpha v*{dw}, b:=b-\\alpha v*{db} initiate vâˆ—dw and vâˆ—db=0v*{dw}\\space and\\space v*{db} = 0 Smooth out gradient descent The momentum term vv effectively provides a smoothing effect since it is an average of past gradients. This means that extreme gradient changes in a single iteration are averaged out, reducing the volatility of the update steps. This smoothing effect is particularly useful on loss function surfaces that are not flat or have many local minima. Consider set Î²\\beta as 0.90.9 (common, about the average last 10 gradients), it gives more weight to v_dwv\\_{dw}, consider dwdw as the acceleration. With betabeta decreasing, velocity increasing slower and acceleration increasing faster. 2.2.3 RMSprop and Adam optimization RMSprop (Root Mean Square Propagation) On iteration t: Compute dw,dbdw, db on current mini-batch sâˆ—dw=Î²2sâˆ—dw+(1âˆ’Î²2)dw2s*{dw}=\\beta_2 s*{dw}+(1-\\beta_2)dw^2 Hope to be relative small. sâˆ—db=Î²2sâˆ—db+(1âˆ’Î²2)db2s*{db}=\\beta_2 s*{db}+(1-\\beta_2)db^2 Hope to be relative large. w:=wâˆ’Î±dwsâˆ—dw+Ïµ,b:=bâˆ’Î±dbsâˆ—db+Ïµw:=w-\\alpha\\dfrac{dw}{\\sqrt{s*{dw}}+\\epsilon}, b:=b-\\alpha\\dfrac{db}{\\sqrt{s*{db}}+\\epsilon} Ïµ\\epsilon is a realative small number(10âˆ’810^{-8}) ot prevent nominaotr being 0. Slow down in vertical direction, fast in horizontal direction. Adam (Adaptive moment estimation) optimization algorithm vâˆ—dw=0,sâˆ—dw=0.vâˆ—db=0,sâˆ—dw=0v*{dw}=0, s*{dw}=0. v*{db}=0, s*{dw}=0 On iteration t: Compute dw,bddw, bd using current mini-batch vâˆ—dw=Î²1vâˆ—dw+(1âˆ’Î²âˆ—1)dw,vâˆ—db=Î²âˆ—1vâˆ—db+(1âˆ’Î²1)dbv*{dw}=\\beta_1v*{dw}+(1-\\beta*1)dw,v*{db}=\\beta*1v*{db}+(1-\\beta_1)db sâˆ—dw=Î²2sâˆ—dw+(1âˆ’Î²âˆ—2)dw2,sâˆ—db=Î²âˆ—2sâˆ—db+(1âˆ’Î²2)dbs*{dw}=\\beta_2s*{dw}+(1-\\beta*2)dw^2,s*{db}=\\beta*2s*{db}+(1-\\beta_2)db vâˆ—dwcorrected=vâˆ—dw1âˆ’Î²âˆ—1t,vâˆ—dbcorrected=v_db1âˆ’Î²1tv*{dw}^{corrected}=\\dfrac{v*{dw}}{1-\\beta*1^t}, v*{db}^{corrected}=\\dfrac{v\\_{db}}{1-\\beta_1^t} sâˆ—dwcorrected=sâˆ—dw1âˆ’Î²âˆ—2t,sâˆ—dbcorrected=s_db1âˆ’Î²sts*{dw}^{corrected}=\\dfrac{s*{dw}}{1-\\beta*2^t}, s*{db}^{corrected}=\\dfrac{s\\_{db}}{1-\\beta_s^t} w:=wâˆ’Î±vâˆ—dwcorrectedsâˆ—dwcorrected+Ïµ,b:=bâˆ’Î±vâˆ—dbcorrectedsâˆ—dbcorrected+Ïµw:=w-\\alpha\\dfrac{v*{dw}^{corrected}}{\\sqrt{s*{dw}^{corrected}}+\\epsilon}, b:=b-\\alpha\\dfrac{v*{db}^{corrected}}{\\sqrt{s*{db}^{corrected}}+\\epsilon} Hyperparameters choice: Î±\\alpha: needs to be tune Î²1\\beta_1: 0.9 (dwdw) First moment Î²2\\beta_2: 0.999 (dw2dw^2) Second moment Ïµ:10âˆ’8\\epsilon: 10^{-8} Not affect performance Learning rate decay 1 epoch &#x3D; 1 pass through the data Î±=11+decayâˆ’rate\\*epochâˆ’numÎ±0\\alpha=\\dfrac{1}{1+decay-rate\\*epoch-num}\\alpha_0 Other methods Î±=0.95epochâˆ’numâ‹…Î±0\\alpha=0.95^{epoch-num}\\cdot \\alpha_0 ---- exponentially decay Î±=kepochâˆ’numâ‹…Î±0\\alpha=\\dfrac{k}{\\sqrt{epoch-num}}\\cdot\\alpha_0 or ktâ‹…Î±0\\dfrac{k}{\\sqrt{t}}\\cdot\\alpha_0 ---- discrete staircase Manual decay (small number of model) The problem of local optima Unlikely to stuck in a bad local optima, since there are too many dimensions and all algorithms in deep learning. saddle point â€”- gradient &#x3D; 0 Problem of plateaus: Make learning slow 2.3.1 Tuning process Hyperparameters Î±\\alpha: learning rate (1st) Î²\\beta: momentum (2nd) Î²1,Î²2,Ïµ\\beta_1, \\beta_2, \\epsilon # of layers (3rd) # of hidden units (2nd) learning rate decay (3rd) mini-batch size (2nd) Try random values: Donâ€™t use a grid Coarse to fine: Trying coarse random first, then fine in working well range. 2.3.2 Using an appropriate scale to pick hyperparameters Learning rate: Î±=0.0001,...,1\\alpha = 0.0001,...,1 r=âˆ’4\\*np.random.rand()r=-4\\*np.random.rand() ---- râˆˆ[âˆ’4,0]r\\in[-4,0] râˆˆ[a,b]r\\in[a,b] a=logâˆ—100.0001=âˆ’4,b=logâˆ—101=0a=log*{10}0.0001 = -4, b=log*{10}1 = 0 Î±=10r\\alpha=10^r ----- Î±âˆˆ[10âˆ’4...100]\\alpha\\in[10^{-4}...10^0] Exponentially Weighted Averages Decay Rate: Î²=0.9(last 10),...,0.999(last 1000)\\beta=0.9(last\\space 10),...,0.999(last\\space1000) 1âˆ’Î²=0.1,...,0.001 râˆˆ[âˆ’3,âˆ’1]1-\\beta=0.1,...,0.001\\space r\\in[-3,-1] Reason for focusing on this instead of single Î²\\beta: Î²\\beta is too close to 1, small changes may have big affects. 1âˆ’Î²=10r1-\\beta=10^r Î²=1âˆ’10r\\beta=1-10^r In practice: Re-test&#x2F;Re-evaluate occasionally. Babysitting one model (donâ€™t have enough training capacity) (Panda): One model at one time. Training many models in parallel (Caviar): Can try many at same time. 2.3.3 Batch Normalization Implementing Batch Norm Batch Norm: make sure hidden units have standardized mean and variance. Given some intermediate value in NN z(1),...,z(m)âˆ’z[l](i)z^{(1)},...,z^{(m)}-z^{[l](i)} (ll for some hidden layers, ii for 1 through mm) Î¼=1mâˆ‘_iz(i)\\mu=\\dfrac{1}{m}\\sum\\limits\\_{i}z^{(i)} (Mean) Ïƒ2=1mâˆ‘i(ziâˆ’Î¼)2\\sigma^2=\\dfrac{1}{m}\\sum\\limits_i(z_i-\\mu)^2 (Variance) z_norm(i)=z(i)âˆ’Î¼Ïƒ2+Ïµz\\_{norm}^{(i)}=\\dfrac{z^{(i)}-\\mu}{\\sqrt{\\sigma^2+\\epsilon}} (Make sure mean=0, variance=1. Ïµ\\epsilon prevent denominator=0) z~(i)=Î³z_norm(i)+Î²\\widetilde{z}^{(i)}=\\gamma z\\_{norm}^{(i)}+\\beta (Î³,Î²\\gamma, \\beta are learnable parameters of model) If Î³=Ïƒ2+Ïµ\\gamma=\\sqrt{\\sigma^2+\\epsilon} Î²=Î¼\\beta=\\mu Then z~(i)=z_norm(i)\\widetilde{z}^{(i)}=z\\_{norm}^{(i)} Use z~[l](i)\\widetilde{z}^{[l](i)} instead of z[l](i)z^{[l](i)} Adding Batch Norm to a network Parameters: w[1],b[1],Î²[1],Î³[1],...,w[l],b[l],Î²[l],Î³[l]w^{[1]},b^{[1]},\\beta^{[1]},\\gamma^{[1]},...,w^{[l]},b^{[l]},\\beta^{[l]},\\gamma^{[l]} May use gradient&#x2F;Adam&#x2F;momentum to tune dÎ²[l]d\\beta^{[l]} Î²[l]=Î²[l]âˆ’Î±dÎ²[l]\\beta^{[l]}=\\beta^{[l]}-\\alpha d\\beta^{[l]} Working with mini-batches: Work the same but on single batches. No need for b[l]b^{[l]}, since variance are all 1. Î² Î³\\beta\\space\\space\\gamma have same dimension with bb. Implementing gradient descent (works with momentum, RMSprop, Adam) for t=1t=1â€¦numMiniBatches Compute forwardProp on X^. In each hidden layer use BN to replace z[l]z^{[l]} with z~[l]\\widetilde{z}^{[l]} Use backprop to compute dw[l],db[l],dÎ²[l],dÎ³[l]dw^{[l]},db^{[l]},d\\beta^{[l]},d\\gamma^{[l]} (no dbdb) Update parameters w[l]:=w[l]âˆ’Î±dw[l]w^{[l]}:=w^{[l]}-\\alpha dw^{[l]} Î²[l]:=Î²[l]âˆ’Î±dÎ²[l]\\beta^{[l]}:=\\beta^{[l]}-\\alpha d\\beta^{[l]} Î³[l]:=Î³[l]âˆ’Î±dÎ³[l]\\gamma^{[l]}:=\\gamma^{[l]}-\\alpha d\\gamma^{[l]} Why does Batch Norm work Covariate Shift: Different test and training data (training on black cats but try to test on other color of cats). Internal Covariate Shift: Between different layers of the network, the distribution of inputs to each layer changes. Recursively it changes the input of the latter layer. May lead to instability and reduced efficiency. Batch norm reduces the problem of input values changes. Make input stable. Let the network learn more independent. Batch norm as regularization In mini-batch, each batch is scaled by the mean&#x2F;variance computed on just that mini-batch. May adds some noise to each hidden layerâ€™s (since is not consider the whole training set) (similar to dropout). This has a slight regularization effect. (Use larger mini-batch size could reduce regularization) Batch Norm at test time Î¼,Ïƒ2\\mu, \\sigma^2: estimate using exponentially weighted average (across mini-batch). Î¼âˆ—global=Î²Î¼âˆ—global+(1âˆ’Î²)Î¼\\mu*{\\text{global}} = \\beta \\mu*{\\text{global}} + (1 - \\beta) \\mu Ïƒ2âˆ—global=Î²Ïƒ2âˆ—global+(1âˆ’Î²)Ïƒ2\\sigma^2*{\\text{global}} = \\beta \\sigma^2*{\\text{global}} + (1 - \\beta) \\sigma^2 During testing, use the global mean and variance estimates for normalization, instead of the statistics from the current test sample or mini-batch. 2.3.4 Multi-class classification Softmax regression CC = # classes = 4 (0,...,3) Output layer: 4 nodes for each class. y^\\hat{y} is (4,1) matrix, sum should be 1. z[L]=w[L]a[Lâˆ’1]+bLz^{[L]}=w^{[L]}a^{[L-1]}+b^{L} (4,1) vector (L represents the output layer) Activation function: t=e(z[L])t=e^{(z^{[L]})} (4,1) vector a[L]=ez[L]âˆ‘âˆ—j=14ti (4,1),a[L]_i=tiâˆ‘âˆ—j=14tia^{[L]}=\\dfrac{e^{z^{[L]}}}{\\sum\\limits*{j=1}^4t_i}\\space\\space(4,1), a^{[L]}\\_i=\\dfrac{t_i}{\\sum\\limits*{j=1}^4t_i} Hardmax: Change beigest to 1, rest all set to 0. Training a softmax classifier If C=2C=2, softmax reduces to logistic regression. Loss function: z[L]z^{[L]}->a[L]=y^a^{[L]}=\\hat{y}->L(y^,y)L(\\hat{y},y) (4,1) Backprop: dz[L]=y^âˆ’ydz^{[L]}=\\hat{y}-y (4,1) dz[L]=âˆ‚Jâˆ‚Z[L]dz^{[L]}=\\dfrac{\\partial{J}}{\\partial{Z^{[L]}}} Deep Learning frameworks TensorFlow III. Structuring Machine Learning Projects3.1 ML Strategy (1)3.1.1 Setting up your goal Orthogonalization Chain of assumptions in ML Fir training set well on cost function: bigger network; Adam Fit dev set well on cost function: Regularization; Bigger training set Fit test set well on cost function: Bigger dev set Perorms well in real world: Change dev set or cost function Single number evaluation metric Precision: In examples recognized, what percentage are actually true. Recall: What percentage of target are correctly recognized in whole test set. F1 Score: Average of precision and recall. 11P+1R\\dfrac{1}{\\dfrac{1}{P}+\\dfrac{1}{R}} (harmonic mean) Dev set + Single number evaluation matric: Speed up iteration Use average error rate instead of single error rate for each classes in estimate many classes at same time. Satisficing and optimizing matrics Consider classifiers with accuracy and running time. maximize accuracy and subject to running time &lt;&#x3D; 100ms Accuracy: optimizing Running time: satisfiying N metic: 1 optimizing, n-1 satisficing Train&#x2F;dev&#x2F;test distributions Come from same distribution. (Use randomly shuffle) Choose a dev set and test set to reflect data you expect to get in the future and consider important to do well on. Size of dev&#x2F;test set For large data set, use 98% training, 1% dev, 1% test Size of test set: Set your test set to be big enough to give high confidence in the overall performance of your system. Sometime use only train+dev, without test set. When to change dev&#x2F;test sets and metrics Filter pornographic images out of error rate: Two Steps How to define a metric to evaluate classifiers. How to do well on this metric. If doing well on your metric + dev&#x2F;test set does not correspond to doing well on your application, change your metric and&#x2F;or dev&#x2F;test set. 3.1.2 Comparing to human-level performance Why human-level performance: Bayes (optimal) error: best possible error. Can never surpass. Humans are quite good at a lot of tasks. So long as ML is worse than humans, you can: Get labeled data from humans. Gain insight from manual error analysis: Why did a person get this right? Better analysis of bias&#x2F;variance. 3.1.3 Analyzing bias and variance Avoidable bias If training error is far from human error (bayes error), focus on bias (avoidable bias). If training error is close to human error but far from dev error, focus on variance. Consider human-level error as a proxy for Bayes error (since is not too far from human-level error to Bayes error). Understanding human-level performance: Based on purpose defined which is the human-level error want to use. If human can perform really well, we can use human-level error as proxy for Bayes error. Surpassing human-level performance Not natural perception Lots of data Improving your model performance The two fundamental assumptions of supervised learning You can fit the training set pretty well. (Avoidable bias) The training set performance generalizes pretty well to the dev&#x2F;test set. (Variance) Reducing (avoidable) bias and variance Avoidable bias: Train bigger model. Train longer&#x2F;better optimization, algorithms (momentum, RMSprop, Adam). NN architecture&#x2F;hyperparameters search (RNN, CNN). Variance: More data. Regularization (L2, dropout, data augmentation). NN architecture&#x2F;hyperparameters search. 3.2 ML Strategy (2)3.2.1 Error analysis Carrying out error analysis Error analysis (count mislabel, minus from the error rate get the ceiling of error rate) Get ~100 mislabeled dev set examples. Count up how many are dogs. Evaluate multiple ideas in parallel (ideas for cat detection) Fix pictures of dogs being recognized as cats Fix great cats ï¼ˆlions, panthers, etc.ï¼‰ being misrecognized Improve performance on blurry images Check the details of mislabeled images (only few minutes&#x2F;hours) Cleaning up incorrectly labeled data DL algorithms are quite robust to random errors in the training set. (random error will not affect the algorithm too much) DL algorithms are less robust to systematic errors. When a high fraction of mistake is due to incorrectly label, should spend time to fix it. Correcting incorrect dev&#x2F;test set examples Apply same process to your dev and test sets to make sure they continue to come from the same distribution. Consider examining examples your algorithm got right as well as ones it got wrong. Train and dev&#x2F;test data may now come from slightly different distributions. Build your first system quickly, then iterate Set up dev&#x2F;test set and metric Build initial system quickly Use Bias&#x2F;Variance analysis &amp; Error analysis to prioritize next steps. Training and testing on different distributions 200,000 from high quality webpages, 10,000 from low quality mobile app (but we care about this). Shuffle before use those data. (not a good option, will cause the influence of what we care small.) Use mobile app as dev&#x2F;test set, and just really small part of training set from app. (This we will make our target to what we want.) Maybe 50% in training, 25% in dev, and 25% test. 3.2.2 Mismatched training and dev&#x2F;test set Training-dev set: Same distribution as training set, but not used for training. Training error - Training-dev error - Dev error Human level - traning set error: avoidable bias Traning error - Training-dev error: Variance Training-dev error - Dev error: Data mismatch Dev error - Test error: degree of overfitting to dev set. Addressing data mismatch Carry out manual error analysis to try to understand difference between training and dev&#x2F;test sets. Make training data more similar; or collect more data similar to dev&#x2F;test sets. Artificial data synthesis: Possible issue (overfitting): Original data is 10000, only have the noise of 1, maybe overfit to this 1. Transfer learning Pre-training&#x2F;Fine-tune From relatively large data to relatively small data. But if the target data is too small may not be suitable for transfer learning. (Depend on the outcome we want, it would be valuable to have more data) When makes sense (transfer from A-&gt; B): Task A and B have the same input x. You have a lot more data for Task A than Task B (want this one). Low level features from A could be helpful for learning B. 3.2.3 Learning from multiple tasks Loss function for multiple tasks Loss: y^(4,1)(i)=1mâˆ‘i=1mâˆ‘_j=14L(y^_j(i),yj(i))\\hat{y}_{(4,1)}^{(i)}=\\dfrac{1}{m}\\sum\\limits_{i=1}^m\\sum\\limits\\_{j=1}^4L(\\hat{y}\\_j^{(i)},y_j^{(i)}) Sum only over valid of j with 0&#x2F;1 label. (some of them may only labeled some feature) Unlike softmax regression: One image can have multiple labels When multi-task learning makes sense Training on a set of tasks that could benefit from having shared lower-level features. Usually: Amount of data you have for each task is quite similar. Can train a big enough neural network to do well on all the tasks. 3.2.4 End-to-end deep learning End-to-end needs lots of data to work well. Breaking small data scenario into different deep learning will be better results. Wether to use end-to-end learning Pros: Let the data speak. Less hand-designing of components needed. Cons: May need large amount of data Excludes potentially useful hand-designed components. Key question: Do you have sufficient data to learn a function of the complexity needed to map x to yï¼Ÿ Use DL to learn individual components. Carefully choose X-&gt;Y mappping depending on what tasks you can got data for. IV. Convolutional Neural Networks4.1 Foundations of Convolutional Neural Networks4.1.1 Convolutional operatin Vertical Edge Detection Used to identify vertical edges in images, which is a crucial step in image analysis and understanding. A small matrix, typically 3x3 or 5x5, is used as a convolution kernel to detect vertical edges. The kernel slides over the image, moving one pixel at a time. At each position, element-wise multiplication is performed between the kernel and the overlapping image area, followed by a sum to produce an output feature map. High values in the output feature map indicate the presence of a vertical edge at that location. $\\begin{bmatrix}1 &amp; 0 &amp; -1 \\1 &amp; 0 &amp; -1 \\1 &amp; 0 &amp; -1\\end{bmatrix}$ Based on this matrix example below, it will detect lighter on the left and darker on the right. Horizontal Edge Detection Brighter on the top and darker on the bottom $\\begin{bmatrix}1 &amp; 1 &amp; 1 \\0 &amp; 0 &amp; 0 \\-1 &amp; -1 &amp; -1\\end{bmatrix}$ TBC Other Common Filters Sobel filter $\\begin{bmatrix}1 &amp; 0 &amp; -1 \\2 &amp; 0 &amp; -2 \\1 &amp; 0 &amp; -1\\end{bmatrix}$ Scharr filter $\\begin{bmatrix}3 &amp; 0 &amp; -3 \\10 &amp; 0 &amp; -10 \\3 &amp; 0 &amp; -3\\end{bmatrix}$ Padding nxn * fxf &#x3D; n-f+1 x n-f+1 Problems of convolution: Shrinking output Through away information from edge. Add a padding(p) of 0 n+2pxn+2p * fxf &#x3D; n+2p-f+1 x n+2p-f+1 Valid convolutions: No padding Same convolutions: Pad so that output size is the same as the input size. (padding is fâˆ’12\\dfrac{f-1}{2}) f is usually odd. Strided convolution Stepping s steps instead of 1. n+2pâˆ’fs+1\\dfrac{n+2p-f}{s}+1 x n+2pâˆ’fs+1\\dfrac{n+2p-f}{s}+1 (If not integer, bound down to the nearest integer.) cross-correlation is the real name of convolution in DL. Convolution over volume Set the filter into the same volume as the input matrix. (e.g. RGB image with 3x3x3 filter) If only look at an individual channel, just make other channel with all 0. If consider vertical and horitental seperately, each output 4x4, the final could stack together get a 4x4x2 volume. nÃ—nÃ—nc\\*fÃ—fÃ—ncn\\times n\\times n_c \\* f\\times f \\times n_c -> nâˆ’f+1Ã—nâˆ’f+1Ã—ncâ€²n-f+1 \\times n-f+1 \\times n_c^{&#x27;} (\\# of filters) One layer of a CNN Each output add a bias and apply non-learner to it. ReLU(Output+b) â€“&gt; Consider stack all outputs after this as volume as the a in a&#x3D;g(z) Consider output as the same as the w in z&#x3D;wa+b. Number of parameters in one layer: If you have 10 filters that are 3x3x 3 in one layer of a neural network, how many parameters does that layer haveï¼Ÿ(Consider 3x3x3 + bias, it will be 280 parameters) Summary of notation (If layer 1 is a convolution layer) f[l]=f^{[l]}= filter size (3x3 filter will be f=3) p[l]=p^{[l]}= padding s[l]=s^{[l]}= stride Input: nH[lâˆ’1]Ã—nW[lâˆ’1]Ã—nC[lâˆ’1]n_H^{[l-1]}\\times n_W^{[l-1]}\\times n_C^{[l-1]} Output: nH[l]Ã—nW[l]Ã—nC[l]n_H^{[l]}\\times n_W^{[l]}\\times n_C^{[l]} nâˆ—H/W[l]=nâˆ—H/W[lâˆ’1]+2p[l]âˆ’f[l]x[l]+1n*{H/W}^{[l]}=\\dfrac{n*{H/W}^{[l-1]}+2p^{[l]}-f^{[l]}}{x^{[l]}}+1 Round down to nearest integer Each filter is: f[l]Ã—f[l]Ã—nC[lâˆ’1]f^{[l]}\\times f^{[l]}\\times n_C^{[l-1]} Activations: a[l]a^{[l]} -&gt; nH[l]Ã—nW[l]Ã—nC[l]n_H^{[l]}\\times n_W^{[l]}\\times n_C^{[l]} Batch gradient descent A[l]A^{[l]} -&gt; mÃ—nH[l]Ã—nW[l]Ã—nC[l]m\\times n_H^{[l]}\\times n_W^{[l]}\\times n_C^{[l]} Weights: f[l]Ã—f[l]Ã—nC[lâˆ’1]Ã—nC[l]f^{[l]}\\times f^{[l]}\\times n_C^{[l-1]}\\times n_C^{[l]} (The last quantity is # filters in layer l) bias: nC[l]n_C^{[l]} - (1,1,1,nC[l])(1,1,1,n_C^{[l]}) A simple example ConvNet Get the final output(7x7x40) and take it as a 1960 vector pass through logistic&#x2F;softmax to get out actual final value. Types of layer in a convolutional network Convolution (CONV) Pooling (POOL) Fully connected (FC) 4.1.2 Pooling layers No parameters to learn. Max pooling Consider input is 4x4 matrix, output a 2x2 matrix. f(filter) &#x3D; 2, s(stride) &#x3D; 2. Just max each 2x2 in the input and put it into one cell in the output matrix. Hyperparameters: f(filter) and s(stride). Average pooling Instead of take the maxium, take the average. Input: nHÃ—nWÃ—nCn_H\\times n_W\\times n_C Output: nHâˆ’fs+1Ã—nWâˆ’fs+1Ã—nC\\dfrac{n_H-f}{s}+1\\times \\dfrac{n_W-f}{s}+1\\times n_C Down to the nearest integer. 4.1.3 CNN example Fully Connected layer After several convolutional and pooling layers, the high-level reasoning in the neural network is done via FC layers. The output of the last pooling or convolutional layer, which is typically a multi-dimensional array, is flattened into a single vector of values. This vector is then fed into one or more FC layers. Role: Integration of Learned Features: FC layers combine all the features learned by previous convolutional layers across the entire image. While convolutional layers are good at identifying features in local areas of the input image, FC layers help in learning global patterns in the data. Dimensionality Reduction: FC layers can be seen as a form of dimensionality reduction, where the high-level, spatially hierarchical features extracted by the convolutional layers are compacted into a form where predictions can be made. Classification or Regression: In classification tasks, the final FC layer typically has as many neurons as the number of classes, with a softmax activation function being applied to the output. For regression tasks, the final FC layerâ€™s output size and activation function are adjusted according to the specific requirements of the task. Operation is similar to neurons in a standard neural network. Example Why convolutions? Parameter sharing: A feature detector (such as a vertical edge detector) thatâ€™s useful in one part of the image is probably useful in another part of the image. Sparsity of connections: In each layer, each output value depends only on a small number of inputs. Training set (x(1),y(1))...(x(m),y(m))(x^{(1)},y^{(1)})...(x^{(m)},y^{(m)}) Cost J=1mâˆ‘_i=1mL(y^(i),y(i))J=\\dfrac{1}{m}\\sum\\limits\\_{i=1}^{m}L(\\hat{y}^{(i)},y^{(i)}) 4.2 Deep Convolutional Models: Case Studies4.2.1 Case studies (LeNet-5, AlexNet, VGG, ResNets) Red notations in the image below are what the network original designed but not suitable for nowadays. 4.2.1.1 LeNet-5 Pioneer in CNNs: One of the earliest Convolutional Neural Networks, primarily used for digit recognition tasks. Architecture: Consists of 7 layers (excluding input). Includes convolutional layers, average pooling layers, and fully connected layers. Activation Functions: Uses sigmoid and tanh activation functions in different layers. (Not using nowadays) Local Receptive Fields: Utilizes 5x5 convolution filters to capture spatial features. Subsampling Layers: Employs average pooling for subsampling. (Using max pool nowadays) 4.2.1.2 AlexNet Multiple GPUs in the paper is outdated for today. LRN is not useful after lots of other researches. Deeper Architecture: Contains 8 learned layers, 5 convolutional layers followed by 3 fully connected layers. ReLU Activation: One of the first CNNs to use ReLU (Rectified Linear Unit) activation function for faster training. Overlapping Pooling: Uses overlapping max pooling, reducing the networkâ€™s size and overfitting. Data Augmentation and Dropout: Employs data augmentation and dropout techniques for better generalization. 4.2.1.3 VGG-16 Simplicity and Depth: Known for its simplicity and depth, with 16 learned layers. Uniform Architecture: Features a very uniform architecture, using 3x3 convolution filters with stride and pad of 1, max pooling, and fully connected layers. Convolutional Layers: Stacks convolutional layers (2-4 layers) before each max pooling layer. Large Number of Parameters: Has a high number of parameters (around 138 million), making it computationally intensive. Transfer Learning: Proved to be an excellent model for transfer learning due to its performance and simplicity. 4.2.1.4 ResNets Residual block Main Path: a[l]a^{[l]} â€“&gt; Linear â€“&gt; ReLU â€“&gt; a[l+1]a^{[l+1]} â€“&gt; Linear â€“&gt; ReLU â€“&gt; a[l+2]a^{[l+2]} z[l+1]=W[l+1]a[l]+b[l+1]z^{[l+1]}=W^{[l+1]}a^{[l]}+b^{[l+1]} a[l+1]=g(z[l+1])a^{[l+1]}=g(z^{[l+1]}) z[l+2]=W[l+2]a[l+1]+b[l+2]z^{[l+2]}=W^{[l+2]}a^{[l+1]}+b^{[l+2]} a[l+2]=g(z[l+2])a^{[l+2]}=g(z^{[l+2]}) Short Cut &#x2F; Skip Connection: a[l]a^{[l]} â€“&gt; ReLU â€“&gt; a[l+2]a^{[l+2]} a[l+2]=g(z[l+2]+a[l])a^{[l+2]}=g(z^{[l+2]}+a^{[l]}) In normal plain network, the trainning error with increasing number of layers in theory will continuesly decrease. But in reality it will decrease but increase after a sweet point. What ResNet performs is decreasing training error with numbers of layers increase and the training error not increasing again. Why do residual networks work? Residual networks introduce a shortcut or skip connection that allows the network to learn identity functions effectively. This is crucial for training very deep networks by avoiding the vanishing gradient problem. In a residual block: XX -> BigNN -> a[l]a^{[l]} -> Residual block -> a[l+2]a^{[l+2]} Input XX is passed through a standard neural network (BigNN) to obtain a[l]a^{[l]}, and then it goes through the residual block to produce a[l+2]a^{[l+2]}. The formulation of a residual block can be represented as:a[l+2]=g(z[l+2]+a[l])=g(w[l+2]a[l+1]+b[l+2]+a[l])a^{[l+2]} = g(z^{[l+2]} + a^{[l]}) = g(w^{[l+2]} a^{[l+1]} + b^{[l+2]} + a^{[l]}) Here, gg is the activation function. z[l+2]z^{[l+2]} is the output of the layer just before the activation function. w[l+2]w^{[l+2]} and b[l+2]b^{[l+2]} are the weight and bias of the layer, respectively. If w[l+2]=0w^{[l+2]} = 0 and b[l+2]=0b^{[l+2]} = 0, then a[l+2]=g(a[l])=a[l]a^{[l+2]} = g(a^{[l]}) = a^{[l]}, effectively allowing the network to learn the identity function. In cases where the dimensions of a[l+2]a^{[l+2]} and a[l]a^{[l]} differ (e.g., a[l]âˆˆR128a^{[l]} \\in \\mathbb{R}^{128} and a[l+2]âˆˆR256a^{[l+2]} \\in \\mathbb{R}^{256}), a linear transformation wsw_s (e.g., wsâˆˆR256Ã—128w_s \\in \\mathbb{R}^{256 \\times 128}) is applied to a[l]a^{[l]} to match the dimensions. This architecture enables training deeper models without performance degradation, which was a significant challenge in deep learning before the development of ResNet. Understand through backdrop(personal notes not from the class content) Consider input as x, the residual block calculation as F(x), identity mapping just drag the x and add it to the residual blockâ€™s calculation which makes the final value y=F(x)+xy=F(x)+x Backprop for this will be as follow Gradient of the Residual Blokcâ€™s Output: âˆ‚yâˆ‚w\\dfrac{\\partial y}{\\partial w} This represents the gradient of the output yy with respect to the weights ww. By chain rule: âˆ‚yâˆ‚w=âˆ‚yâˆ‚F(x)âˆ‚F(x)âˆ‚xâˆ‚xâˆ‚w+âˆ‚yâˆ‚xâˆ‚xâˆ‚w\\dfrac{\\partial y}{\\partial w} = \\dfrac{\\partial y}{\\partial F(x)}\\dfrac{\\partial F(x)}{\\partial x}\\dfrac{\\partial x}{\\partial w} + \\dfrac{\\partial y}{\\partial x}\\dfrac{\\partial x}{\\partial w} Since y=F(x)+xy=F(x)+x, âˆ‚yâˆ‚F(x)\\dfrac{\\partial y}{\\partial F(x)} and âˆ‚yâˆ‚x\\dfrac{\\partial y}{\\partial x} should be 1 So the formula become âˆ‚yâˆ‚w=âˆ‚F(x)âˆ‚xâˆ‚xâˆ‚w+âˆ‚xâˆ‚w\\dfrac{\\partial y}{\\partial w} = \\dfrac{\\partial F(x)}{\\partial x}\\dfrac{\\partial x}{\\partial w} + \\dfrac{\\partial x}{\\partial w} Compare to without the identity mapping xx added. âˆ‚yâˆ‚w=âˆ‚F(x)âˆ‚xâˆ‚xâˆ‚w\\dfrac{\\partial y}{\\partial w} = \\dfrac{\\partial F(x)}{\\partial x}\\dfrac{\\partial x}{\\partial w}, there is a âˆ‚xâˆ‚w\\dfrac{\\partial x}{\\partial w} less. Add this xx to F(x)F(x) makes the network will not get worse results compare to before. 4.2.2 Network in Network and 1 X 1 convolutions 1x1 convolutions Functionality of 1x1 Convolutions: A 1x1 convolution, despite its simplicity, acts as a fully connected layer applied to each pixel separately across depth. Itâ€™s effectively used for channel-wise interactions and dimensionality reduction. Increasing Network Depth: 1x1 convolutions can increase the depth of the network without a significant increase in computational complexity. Dimensionality Reduction: They are often used for reducing the number of channels (depth) before applying expensive 3x3 or 5x5 convolutions, thus reducing the computational cost. Feature Re-calibration: 1x1 convolutions can recalibrate the feature maps channel-wise, enhancing the representational power of the network. Using 1x1 convolutions: Reduce dimension: Consider a 28x28x192 input with CONV 1x1 with 32 filters, the output will be 28x28x32. 4.2.3 Inception network Motivation for inception network Input 28x28x192 Use 1x1x192 with 64 filters, output 28x28x64 Use same dimension 3x3x192, output 28x28x128 Use same dimension 5x5x192, output 28x28x32 use same dimension and s&#x3D;1 Max-Pool, output 28x28x32. Final output 28x28x256. The problem of computational cost (Consider 5x5x192) 5x5x192x28x28x32 is really big, 120M. Bottleneck layer (Using 1x1 convolution): shrink 28x28x192 â€“&gt; CONV, 1x1, 16, 1x1x192 â€“&gt; 28x28x16 (Bottleneck layer) â€“&gt; CONV 5x5, 32, 5x5x16 â€“&gt; 28x28x32 In total only 28x28x16+28x28x32x5x5x16&#x3D;12.4M Inception moule The softmax in the itermediate position is used for regularization which is used avoid overfitting. 4.2.4 MobileNet Depthwise Separable Convolution Depthwise Convolution Computational cost &#x3D; #filter params x #filter positions x #of filters Ppointwise Convolution Computational cost &#x3D; #filter params x #filter positions x # of filters nâˆ—câˆ—nâˆ—câˆ—filtersn*c * n*c * filters Cost of depthwise seprable convolution &#x2F; normal convolution 1nc+1f2\\dfrac{1}{n_c} + \\dfrac{1}{f^2} MobileNet v2 Bottleneck Residual Connection Expansion Depthwise Pointwise (Projection) Similar computational cost as v1 MobileNet V2 improves upon V1 by introducing an inverted residual structure with linear bottlenecks, which enhances the efficiency of feature extraction and information flow through the network. This architectural advancement allows V2 to achieve better performance than V1, despite having similar computational costs. Essentially, V2 optimizes the way features are processed and combined, providing more effective and complex feature representation within the same computational budget as V1. 4.2.5 EfficientNet EfficientNet is a series of deep learning models known for high efficiency and accuracy in image classification tasks. Compound Scaling: It introduces a novel compound scaling method, scaling network depth, width, and resolution uniformly with a set of fixed coefficients. High Efficiency and Accuracy: EfficientNets provide state-of-the-art accuracy for image classification while being more computationally efficient compared to other models. 4.2.6 Inception network Transfer Learning Small training set: Freeze all hidden layers (save to disk), only train the softmax unit. Big training set: Freeze less hidden layers, train some of the hidden layers (or use new hidden units), and also own softmax unit. Lots of data: Use the already trained weights and bias as initalization, re-train based on it, as well as the softmax unit. Data augmentation Common augmentation method: Mirroring, Random Cropping, (Rotation, Shearing, Local warping, â€¦) Color shifting: add&#x2F;minus from RGB. Advanced: PCA &#x2F; PCA color augmentation. Implementing distortions during training: One CPU thread doing augmentation, and other threads or GPU doing the training at same time. State of CV Data needed (little data to lots of data): Object detection &lt; Image recognition &lt; Speach recognition Lots of data - Simpler algotithms (Less hand-engineering) Little data - more hand-engineering (â€œhacksâ€) - Transfer learning Two sources of knowledge Labeled data Hand engineered features&#x2F;network architecture&#x2F;other components Tips for doing well on benchmarks&#x2F;winning competitions Ensembling: Train several networks independently and average their outputs (y^\\hat{y}) 1-2% better. (3-15 networks) Multi-crop at test time: Run classifier on multiple versions of test images and average results. (10-crop: center, four corner, also on mirror image the same 5 crops) Use open source code Use architectures of networks published in the literature. Use open source implementations if possible. Use pretrained models and fine-tune on your dataset. 4.3 Object Detection4.3.1 Object localization Want to detect 4 class: 1-pedestrian, 2-car, 3-mtorcycle, 4-background. Defining the target label y: Need to out put bx,by,bh,bwb_x, b_y, b_h, b_w, class label (1-4). (In total 9 elements in the output vector). y=[pc,bx,by,bh,bw,c1,c2,c3]y=[p_c, b_x, b_y, b_h, b_w, c_1, c_2, c_3] There is an object y=[1,bx,by,bh,bw,c1,c2,c3]y=[1, b_x, b_y, b_h, b_w, c_1, c_2, c_3] No object y=[0,?,?,?,?,?,?,?]y=[0, ?, ?, ?, ?, ?, ?, ?] Donâ€™t care for all of other Lost function: L(y^,y)=(y1^âˆ’y1)2+(y2^âˆ’y2)2+...+(y8^âˆ’y8)2L(\\hat{y}, y)=(\\hat{y_1} - y_1)^2 + (\\hat{y_2} - y_2)^2 + ... + (\\hat{y_8} - y_8)^2 if y1=1y_1=1 L(y^,y)=(y1^âˆ’y1)2L(\\hat{y}, y)=(\\hat{y_1} - y_1)^2 if y1=0y_1=0 4.3.2 Landmark detection Annotate key positions (points-xy coordinate) as landmarks. 4.3.3 Object detection Object detection Starts with closely crops images. A window sliding from the top left to bottom right, once and once. If not find increase the windowâ€™s size and redo the sliding. Run each individual image to the convnet. Turning FC layer into convolutional layers Instead directly to FC, use conv filter. Convolution implementation of sliding windows Instead of do 4 times 14x14x3, new conv fc share the computation, directly using the 2x2x4. Output accurate bounding boxes YOLO algorithm Find the medium point of target and working into the boundary box that contains that point. Intersection over union (IoU) Use to check accuracy. Size of intersection &#x2F; size of reunion (normally â€œCorrectâ€ if loU â‰¥\\geq 0.5) Non-max suppression Leave the maximum accuracy one, supprese all with high IoU. Anchor Boxes Predefine anchor boxes, associate ojects with anchor boxes. If objects more than assigned anchor boxes, not works. Not same shape, not works. Training set y is 3x3x2x8 (which is # of grids x # of anchors x # classes(5(pc.bx,by,bh,bwp_c. b_x, b_y, b_h, b_w) + classes)) Regision Proposals R-CNN: Propose regions. Classify proposed regions one at a time. Output label + bounding box. Fast R-CNN: Propose regions. Use convolution implementation of sliding windows to classify all the proposed regions. Faster R-CNN: Use convolutional network to propose regions. Semantic Segmentation with U-Net Per-pixel class labels Deep Learning for Semantic Segmentation Transpose Convolution Increase the image size. U-Net Architecture Skip Connections: Left one get more details in color or anything like that. Right one is more spatial information to figure out where is the object really is. 4.4 Special Applications: Face Recognition &amp; Neural Style Transfer4.4.1 Face recognition Face verification vs. face recognition verification vs recognition â€”- 1:1 vs 1:K Verification Input image, name&#x2F;ID. Output whether the input image is that of the claimed person. Recognition Has a database of K persons Get an input image Output ID if the image is any of the K persons (or â€œnot recognizedâ€) One-shot learning Learning from one example to recognize the person again. Learning a â€œsimilarityâ€ function d(img1, img2) &#x3D; degree of difference between images If d(img1, img2) â‰¤Ï„\\le \\tau â€œsameâ€ \\textgreater Ï„\\textgreater \\space \\tau â€œDifferentâ€ Siamese network Input two differnet images into two CNN and ge the result of them. Such as input x(1),x(2)x^{(1)}, x^{(2)} seperately into two differnt CNN, and the output will be the encoding of each of them f(x(1)),f(x(2))f(x^{(1)}), f(x^{(2)}) Then compare the distance between them d(x(1),x(2))=âˆ£âˆ£f(x(1))âˆ’f(x(2))âˆ£âˆ£_22d(x^{(1)}, x^{(2)}) = ||f(x^{(1)}) - f(x^{(2)})||\\_2^2 Parameters of NN define an encoding f(x(i))f(x^{(i)}) Learn parameters so that: If x(i),x(j)x^{(i)}, x^{(j)} are the smae person, âˆ£âˆ£f(x(i))âˆ’f(x(j))âˆ£âˆ£2||f(x^{(i)}) - f(x^{(j)})||^2 is small. If x(i),x(j)x^{(i)}, x^{(j)} are the different person, âˆ£âˆ£f(x(i))âˆ’f(x(j))âˆ£âˆ£2||f(x^{(i)}) - f(x^{(j)})||^2 is large.. Triplet Loss Learning objective: (Anchor, Positive), (Anchor, Negative) Want: âˆ£âˆ£f(A)âˆ’f(P)âˆ£âˆ£2+Î±â‰¤âˆ£âˆ£f(A)âˆ’f(N)âˆ£âˆ£2||f(A) - f(P)||^2 + \\alpha \\le ||f(A) - f(N)||^2 Î±\\alpha is the margin (similar to SVM) âˆ£âˆ£f(A)âˆ’f(P)âˆ£âˆ£2âˆ’âˆ£âˆ£f(A)âˆ’f(N)âˆ£âˆ£2+Î±â‰¤0||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + \\alpha \\le 0 Loss function Given 3 images A, P, N: L(A,P,N)=max(âˆ£âˆ£f(A)âˆ’f(P)âˆ£âˆ£2âˆ’âˆ£âˆ£f(A)âˆ’f(N)âˆ£âˆ£2+Î±,0)L(A, P, N) = max(||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + \\alpha, 0) J=âˆ‘_i=1mL(A(i),P(i),N(i))J = \\sum\\limits\\_{i=1}^m L(A^{(i)},P^{(i)},N^{(i)}) If have a training set of 10K pictures of 1k persons. Put those 10K into triplet A, P, N, then put into the loss function. Choosing the triplets A, P, N During training, if A, P, N are chosen randomly, d(A,P)+Î±â‰¤d(A,N)d(A,P) +\\alpha \\le d(A, N) is easily satisfied. Choose triplets thatâ€™re â€œhardâ€ to train on. (such as choose d(A,P)â‰ˆd(A,N)d(A,P) \\approx d(A,N)) Training set using triplet loss to make J smaller. And make distance of d for same person small and different large. Face Verification and Binary Classification y^=Ïƒ(âˆ‘_k=1128wkâˆ£f(x(i))_kâˆ’f(x(j))_kâˆ£+b)\\hat{y} = \\sigma (\\sum\\limits\\_{k=1}^{128}w_k|f(x^{(i)})\\_k-f(x^{(j)})\\_k| + b) Only store the f(x(j))f(x^{(j)}) as pre-compute, save storage and computational resources. Face verification supervised learning. 4.4.2 Neural style transfer What is it? Cost Function J(G)=Î±Jâˆ—content(C,G)+Î²Jâˆ—Style(S,G)J(G) = \\alpha J*{content}(C, G) + \\beta J*{Style}(S, G) Find the generated image G Initiate G randomly G: 100x100x3 Use gradient descent to minimize J(G) G:=Gâˆ’âˆ‚âˆ‚GJ(G)G:=G-\\dfrac{\\partial}{\\partial G}J(G) V. Sequence Models5.1 Recurrent Neural Networks5.1.1 RNN model5.1.2 Backpropagation through time5.1.3 Different types of RNNs5.2 Natural Language Processing &amp; Word Embeddings5.2.1 Word Representation5.2.2 Embedding matrix5.2.3 Word embeddings in TensorFlow5.3 Sequence Models &amp; Attention Mechanism5.3.1 Sequence to sequence model5.3.2 Beam search5.3.3 Attention model","categories":[{"name":"è½¯ä»¶","slug":"è½¯ä»¶","permalink":"https://www.zl-asica.com/categories/%E8%BD%AF%E4%BB%B6/"}],"tags":[{"name":"DL","slug":"DL","permalink":"https://www.zl-asica.com/tags/DL/"}]},{"title":"Pythonç®€å•åŠ å¯†å™¨-åè½¬å­—ç¬¦ä¸²","slug":"python-cipher-reverse-string","date":"2023-01-18T06:55:00.000Z","updated":"2024-02-13T06:37:05.553Z","comments":true,"path":"2023/python-cipher-reverse-string/","permalink":"https://www.zl-asica.com/2023/python-cipher-reverse-string/","excerpt":"1. ä»€ä¹ˆæ˜¯åè½¬å­—ç¬¦ä¸²åè½¬å­—ç¬¦ä¸²å…¶å®å°±æ˜¯æŠŠä¸€ä¸ªstringç±»å‹çš„å­—ç¬¦ä¸²å˜é‡è¿›è¡Œä»å¤´åˆ°å°¾çš„ä¸€ä¸ªåè½¬ï¼Œæ¯”å¦‚â€™hello!â€™ç»è¿‡åè½¬åæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°â€™!ollehâ€™ã€‚å¦‚æœåªæƒ³çœ‹ä»£ç å¯ä»¥ç›´æ¥ç¿»åˆ°!2.5çœ‹ä»£ç æ®µå³å¯","text":"1. ä»€ä¹ˆæ˜¯åè½¬å­—ç¬¦ä¸²åè½¬å­—ç¬¦ä¸²å…¶å®å°±æ˜¯æŠŠä¸€ä¸ªstringç±»å‹çš„å­—ç¬¦ä¸²å˜é‡è¿›è¡Œä»å¤´åˆ°å°¾çš„ä¸€ä¸ªåè½¬ï¼Œæ¯”å¦‚â€™hello!â€™ç»è¿‡åè½¬åæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°â€™!ollehâ€™ã€‚å¦‚æœåªæƒ³çœ‹ä»£ç å¯ä»¥ç›´æ¥ç¿»åˆ°!2.5çœ‹ä»£ç æ®µå³å¯ 2. å¦‚ä½•å®ç°åè½¬å­—ç¬¦ä¸²å®é™…ä¸Šæˆ‘ä»¬åœ¨å®ç°è¿™ä¸ªåŠ å¯†å™¨çš„æ—¶å€™é¦–å…ˆè¦è€ƒè™‘çš„æ˜¯å¦‚ä½•åˆ©ç”¨å­—ç¬¦ä¸²æœ¬èº«çš„ä¸€äº›ç‰¹æ€§å»å®ç°è¿™ä¸ªåŠŸèƒ½ï¼Œä»è€Œä½¿ç”¨æœ€ç®€å•çš„ä»£ç è¾¾åˆ°æœ€ä½³çš„æ•ˆæœã€‚ 2.1 å­—ç¬¦ä¸²çš„åŸºæœ¬ç‰¹æ€§æˆ‘ä»¬çš„Stringæœ¬è´¨ä¸Šå°±æ˜¯ä¸€ä¸ªç±»ä¼¼åˆ—è¡¨(List)çš„å­˜åœ¨ã€‚æˆ‘ä»¬çœ‹ä¸‹é¢çš„å›¾ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªStringç±»å‹çš„å˜é‡ä¸ºtest = &quot;screen&quot;ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸‹å›¾çš„æ–¹å¼å¯¹å®ƒè¿›è¡Œç´¢å¼•ã€‚æ¯ä¸€ä¸ªå•ç‹¬çš„å­—æ¯å°±æ˜¯è¿™ä¸ªâ€åˆ—è¡¨â€é‡Œçš„ä¸€ä¸ªå•ç‹¬çš„å…ƒç´ ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¦‚test[0]çš„æ–¹å¼æ¥è·å–å…ƒç´ â€™sâ€™ï¼Œä¹Ÿå¯ä»¥é€šè¿‡å¦‚test[0:2]çš„æ–¹å¼æ¥è·å–å…ƒç´ â€™scâ€™ï¼Œæ­¤å¤„[0:2]å¯¹åº”çš„æ˜¯ä¸€ä¸ª[0,2)å·¦é—­å³å¼€çš„åŒºé—´ã€‚é‚£å¤§å®¶å¯èƒ½å°±æƒ³åˆ°äº†åˆ—è¡¨æ˜¯å¯ä»¥åè½¬(reverse)çš„ï¼Œé‚£ä¹ˆæˆ‘ä»¬è¿™ä¸ªæ‰€è°“çš„â€™åˆ—è¡¨â€™å¯ä¸å¯ä»¥åšåŒæ ·çš„æ“ä½œå‘¢ï¼Ÿç­”æ¡ˆæ˜¯ä¸è¡Œï¼ŒStringå˜é‡å¹¶æ²¡æœ‰reverseå‡½æ•°ï¼Œæ‰€ä»¥æˆ‘ä»¬å°±éœ€è¦å…ˆè®²å­—ç¬¦ä¸²è½¬æ¢ä¸ºçœŸæ­£çš„åˆ—è¡¨æ‰å¯ä»¥è¿›è¡Œåè½¬ã€‚ 2.2 å­—ç¬¦ä¸²è½¬åˆ—è¡¨å­—ç¬¦ä¸²è½¬æ¢ä¸ºåˆ—è¡¨å…¶å®æœ¬è´¨ä¸Šéå¸¸ç®€å•ã€‚ä¸€èˆ¬æ¥è¯´æˆ‘ä»¬æŠŠæ•°æ®è½¬æ¢ä¸ºåˆ—è¡¨å¯¹è±¡æœ‰ä¸¤ç§æ–¹å¼ï¼Œç¬¬ä¸€ç§æ–¹å¼æ˜¯é€šè¿‡split()å‡½æ•°è¿›è¡ŒStringå¯¹è±¡çš„åˆ‡åˆ†ï¼Œç¬¬äºŒç§æ–¹å¼æ˜¯é€šè¿‡list()å‡½æ•°ç›´æ¥å°†Stringå¯¹è±¡å¼ºåˆ¶ç±»å‹è½¬æ¢ä¸ºåˆ—è¡¨(List)ç±»å‹çš„å˜é‡ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨list()å‡½æ•°å°†Stringå˜é‡è½¬æ¢ä¸ºåˆ—è¡¨å¯¹è±¡å³å¯ã€‚ 12test = &#x27;screen&#x27;test_list = list(test) # test_list = [&#x27;s&#x27;, &#x27;c&#x27;, &#x27;r&#x27;, &#x27;e&#x27;, &#x27;e&#x27;, &#x27;n&#x27;] 2.3 åˆ—è¡¨çš„åè½¬åœ¨æˆ‘ä»¬å°†Stringè½¬æ¢ä¸ºåˆ—è¡¨åï¼Œæˆ‘ä»¬å°±è¦å¼€å§‹åè½¬äº†ã€‚åˆ—è¡¨çš„åè½¬éå¸¸åœ°ç®€å•ï¼Œç›´æ¥å¯¹listç±»å‹çš„å˜é‡ä½¿ç”¨.reverse()å‡½æ•°è¿›è¡Œåˆ—è¡¨çš„åè½¬å³å¯ã€‚ 1test_list.reverse() # test_list = [&#x27;n&#x27;, &#x27;e&#x27;, &#x27;e&#x27;, &#x27;r&#x27;, &#x27;c&#x27;, &#x27;s&#x27;] 2.4 åˆ—è¡¨è½¬å­—ç¬¦ä¸²åè½¬å®Œæˆåæˆ‘ä»¬å°±éœ€è¦æŠŠåˆ—è¡¨é‡æ–°è½¬æ¢ä¸ºå­—ç¬¦ä¸²Stringç±»å‹çš„å¯¹è±¡äº†ã€‚å°†listè½¬æ¢ä¸ºStringå®é™…ä¸Šéå¸¸ç®€å•ï¼Œåªéœ€è¦ä½¿ç”¨&#39;&#39;.join()åœ¨å¼•å·ä¸­é—´å°±æ˜¯ç”¨ä»€ä¹ˆå…ƒç´ æŠŠä½ çš„listä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ è¿æ¥èµ·æ¥å½¢æˆä¸€ä¸ªStringã€‚ä¸¾ä¸ª?ï¼Œå¦‚æœä½ åœ¨å¼•å·é—´ç”¨çš„æ˜¯&#39;,&#39;ï¼Œé‚£ä¹ˆä½ æœ€ååˆ—è¡¨å°±ä¼šé€šè¿‡,è¿æ¥èµ·æ¥ã€‚ 1reverse_test = &#x27;&#x27;.join(test_list) # reverse_test = &#x27;neercs&#x27; 2.5 åè½¬å­—ç¬¦ä¸²çš„æœ€ç»ˆå®ç°è¿™é‡Œæˆ‘ä»¬å°±æŠŠæ‰€æœ‰çš„ä»£ç åˆèµ·æ¥ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå˜é‡cipher = &quot;hello!&quot;ï¼Œæˆ‘ä»¬è¦å¾—åˆ°å®ƒåè½¬åçš„Stringå­—ç¬¦ä¸²ã€‚ 12345678def cipher_reverse_string(cipher: str) -&gt; str: cipher_list = list(cipher) cipher_list.reverse() result = &#x27;&#x27;.join(cipher_list) return resultif __name__ == &#x27;__main__&#x27;: print(cipher_reverse_string(&quot;hello!&quot;)) 3. æ€»ç»“æˆ‘ä»¬è¿™é‡Œä½¿ç”¨äº†Pythonä¸­Stringå’ŒListçš„ä¸€äº›å°ç‰¹æ€§å’Œæ–¹æ³•å®ç°äº†ä¸€ä¸ªéå¸¸ç®€å•çš„åŠ å¯†å™¨(å®Œå…¨æ²¡åŠ å¯†çš„åŠ å¯†å™¨)ï¼Œä½¿ç”¨åˆ°äº†Stringå’ŒListçš„äº’ç›¸è½¬æ¢ã€listçš„åè½¬ã€‚æœ¬è´¨ä¸Šæ¥è¯´æ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„å°ç¨‹åºï¼Œåªéœ€è¦ç†è§£å…¶ä¸­çš„é€»è¾‘å³å¯ã€‚","categories":[{"name":"ä»£ç ","slug":"ä»£ç ","permalink":"https://www.zl-asica.com/categories/%E4%BB%A3%E7%A0%81/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://www.zl-asica.com/tags/Python/"}]},{"title":"ä»…ä½¿ç”¨numpyå®ç°æ··æ·†çŸ©é˜µè®¡ç®—ï¼ˆä¸ä½¿ç”¨sklearnï¼‰","slug":"numpy-only-confusion-matrix","date":"2023-01-16T05:28:00.000Z","updated":"2024-02-13T06:37:05.553Z","comments":true,"path":"2023/numpy-only-confusion-matrix/","permalink":"https://www.zl-asica.com/2023/numpy-only-confusion-matrix/","excerpt":"1 ä»€ä¹ˆæ˜¯æ··æ·†çŸ©é˜µ(confusion matrix)å½“æˆ‘ä»¬æ‹¿åˆ°æ•°æ®åï¼Œç»è¿‡æ•°æ®æ¸…æ´—ã€é¢„å¤„ç†å’Œæ•´ç†ä¹‹åï¼Œæˆ‘ä»¬åšçš„ç¬¬ä¸€æ­¥æ˜¯åˆ©ç”¨è¿™äº›æ•°æ®è®­ç»ƒå‡ºä¸€ä¸ªæ¨¡å‹ã€‚æˆ‘ä»¬ç©¶ç«Ÿå¦‚ä½•è¡¡é‡æ¨¡å‹çš„å‡†ç¡®åº¦å’Œæœ‰æ•ˆæ€§ï¼Ÿæ€§èƒ½å’Œæ•ˆç‡åˆå¦‚ä½•ï¼Ÿè¿™å°±æ˜¯æ··æ·†çŸ©é˜µè¢«ç”¨æ¥è§£å†³çš„é—®é¢˜ã€‚æ··æ·†çŸ©é˜µæ˜¯æœºå™¨å­¦ä¹ åˆ†ç±»çš„æ€§èƒ½çš„ä¸€ç§æ–¹å¼ã€‚","text":"1 ä»€ä¹ˆæ˜¯æ··æ·†çŸ©é˜µ(confusion matrix)å½“æˆ‘ä»¬æ‹¿åˆ°æ•°æ®åï¼Œç»è¿‡æ•°æ®æ¸…æ´—ã€é¢„å¤„ç†å’Œæ•´ç†ä¹‹åï¼Œæˆ‘ä»¬åšçš„ç¬¬ä¸€æ­¥æ˜¯åˆ©ç”¨è¿™äº›æ•°æ®è®­ç»ƒå‡ºä¸€ä¸ªæ¨¡å‹ã€‚æˆ‘ä»¬ç©¶ç«Ÿå¦‚ä½•è¡¡é‡æ¨¡å‹çš„å‡†ç¡®åº¦å’Œæœ‰æ•ˆæ€§ï¼Ÿæ€§èƒ½å’Œæ•ˆç‡åˆå¦‚ä½•ï¼Ÿè¿™å°±æ˜¯æ··æ·†çŸ©é˜µè¢«ç”¨æ¥è§£å†³çš„é—®é¢˜ã€‚æ··æ·†çŸ©é˜µæ˜¯æœºå™¨å­¦ä¹ åˆ†ç±»çš„æ€§èƒ½çš„ä¸€ç§æ–¹å¼ã€‚ 2 æ··æ·†çŸ©é˜µæœ‰ä»€ä¹ˆç”¨æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªçŸ©é˜µç›´è§‚åœ°çœ‹åˆ°æ¯ä¸ªæ•°æ®çš„çœŸå®å€¼å’Œæˆ‘ä»¬æ¨¡å‹çš„é¢„æµ‹å€¼çš„å…³ç³»ã€‚ 3 å¦‚ä½•å®ç°3.1 ä½¿ç”¨sklearn.metricsçš„confusion_matrixæœ¬æ–¹æ³•é€‚ç”¨äº†sklearn.metricsä¸‹çš„confusion_matrixå‡½æ•°ç›´æ¥è¿›è¡Œç”Ÿæˆ 12345from sklearn.metrics import confusion_matrixy_true = [2, 0, 2, 2, 0, 1]y_pred = [0, 0, 2, 2, 0, 2]confusion_matrix(y_true, y_pred) 3.2 ä»…ä½¿ç”¨numpyæœ¬æ–¹æ³•ä»…ä½¿ç”¨äº†numpyï¼Œæ²¡æœ‰ä½¿ç”¨ä»»ä½•å…¶ä»–çš„åº“ï¼ŒåŒ…æ‹¬sklearnåœ¨å†…å®ç°æ··æ·†çŸ©é˜µçš„è®¡ç®—ã€‚ 123456789101112131415import numpy as npdef compute_confusion_matrix(true, pred): &#x27;&#x27;&#x27;è¾“å…¥trueå’Œpred å…¶è¾“å‡ºç»“æœä¸ä»¥ä¸‹å†…å®¹ç›¸åŒï¼ˆè®¡ç®—æ—¶é—´ä¹Ÿç›¸ä¼¼ï¼‰ã€‚ &quot;from sklearn.metrics import confusion_matrix&quot; ç„¶è€Œï¼Œè¿™ä¸ªå‡½æ•°é¿å…äº†å¯¹sklearnçš„ä¾èµ–ã€‚&#x27;&#x27;&#x27; K = len(np.unique(true)) # featuresçš„æ•°é‡ result = np.zeros((K, K)) for i in range(len(true)): result[true[i]][pred[i]] += 1 return result æ¥æºï¼šhttps://stackoverflow.com/a/48087308/20626353","categories":[{"name":"è½¯ä»¶","slug":"è½¯ä»¶","permalink":"https://www.zl-asica.com/categories/%E8%BD%AF%E4%BB%B6/"}],"tags":[{"name":"ML","slug":"ML","permalink":"https://www.zl-asica.com/tags/ML/"}]},{"title":"C++è¿›è¡Œç‰¹å®šä½æ•°çš„å››èˆäº”å…¥/å‘ä¸Šå–æ•´/å‘ä¸‹å–æ•´/æˆªæ–­","slug":"cpp-round","date":"2022-03-25T23:26:00.000Z","updated":"2024-02-13T06:37:05.553Z","comments":true,"path":"2022/cpp-round/","permalink":"https://www.zl-asica.com/2022/cpp-round/","excerpt":"1.C++çš„å››èˆäº”å…¥&#x2F;å‘ä¸Šå–æ•´&#x2F;å‘ä¸‹å–æ•´&#x2F;æˆªæ–­æˆ‘ä»¬åœ¨C++ä¸­æœ‰æ—¶ä¼šéœ€è¦ä½¿ç”¨åˆ°æ•°å­¦è¿ç®—ï¼ŒåŒæ ·çš„å¯èƒ½ä¼šä½¿ç”¨åˆ°æ ‡é¢˜æåˆ°çš„å‡ ç§æ–¹å¼è¿›è¡Œè¿ç®—ã€‚å®é™…ä¸ŠC++æœ‰ä¸€ä¸ªåº“å¯ä»¥ç›´æ¥å®ç°è¿™äº›æ–¹æ³•æ“ä½œã€‚","text":"1.C++çš„å››èˆäº”å…¥&#x2F;å‘ä¸Šå–æ•´&#x2F;å‘ä¸‹å–æ•´&#x2F;æˆªæ–­æˆ‘ä»¬åœ¨C++ä¸­æœ‰æ—¶ä¼šéœ€è¦ä½¿ç”¨åˆ°æ•°å­¦è¿ç®—ï¼ŒåŒæ ·çš„å¯èƒ½ä¼šä½¿ç”¨åˆ°æ ‡é¢˜æåˆ°çš„å‡ ç§æ–¹å¼è¿›è¡Œè¿ç®—ã€‚å®é™…ä¸ŠC++æœ‰ä¸€ä¸ªåº“å¯ä»¥ç›´æ¥å®ç°è¿™äº›æ–¹æ³•æ“ä½œã€‚ 123456789101112131415161718192021222324#include &lt;cmath&gt;int main()&#123; double a = 43.5555; // å››èˆäº”å…¥ a = std::round(a); std::cout &lt;&lt; a &lt;&lt; std::endl; // 44.0000 // å‘ä¸‹å–æ•´ a = std::floor(a); std::cout &lt;&lt; a &lt;&lt; std::endl; // 43.0000 // å‘ä¸Šå–æ•´ a = std::ceil(a); std::cout &lt;&lt; a &lt;&lt; std::endl; // 44.0000 // æˆªæ–­ a = std::trunc(a); std::cout &lt;&lt; a &lt;&lt; std::endl; // 43.0000 return 0;&#125; é‚£ä¹ˆè¿™å°±æ˜¯æœ€ç®€å•çš„å››ç§ç®—æ³•åœ¨C++ä¸­çš„ä½¿ç”¨äº†ã€‚ 2.C++ä¸­è·å–ç‰¹å®šä½æ•°çš„æ–¹æ³•é‚£ä¹ˆæœ‰æ—¶å€™æˆ‘ä»¬è¿›è¡Œè¿™æ ·çš„æ•°å­¦è¿ç®—åï¼Œç›®çš„å°±æ˜¯ä¸ºäº†å¾—åˆ°ä¸€ä¸ªæ›´æ–¹ä¾¿çœ‹çš„æ•°ï¼Œé‚£å½“ç„¶è¦å–ç‰¹å®šä½æ•°çš„å°æ•°äº†ã€‚å†™æ³•è§„åˆ™å¦‚ä¸‹(ä»¥å››èˆäº”å…¥æ–¹æ³•std::roundåšæ¼”ç¤º)ã€‚ 1a = std::round(a * ä¿ç•™åˆ°å‡ åˆ†ä½) / ä¿ç•™åˆ°å‡ åˆ†ä½; // ä¿ç•™åˆ°ååˆ†ä½ = ä¿ç•™ä¸¤ä½å°æ•° æˆ‘ä»¬æ¥ä¸€ä¸ªä¿å­˜ä¸¤ä½å°æ•°(ååˆ†ä½)çš„å®ä¾‹æ¥çœ‹ä¸€ä¸‹ã€‚ 1234567891011#include &lt;iostream&gt;#include &lt;cmath&gt;int main()&#123; double a = 43.5555; a = std::round(a * 10) / 10; std::cout &lt;&lt; a &lt;&lt; std::endl; return 0;&#125; é‚£ä¹ˆè¿™æ ·å°±å¯ä»¥éå¸¸è½»æ¾çš„åœ¨C++ä¸­è·å–åˆ°æˆ‘ä»¬éœ€è¦çš„å°æ•°ä½æ•°äº†ã€‚ 3.C++ä¸­å¯¹å®šä½æ•°çš„doubleè¿›è¡Œè¾“å‡ºçš„æ–¹æ³•æˆ‘ä»¬ä¸Šé¢è¯´åˆ°äº†å¦‚ä½•è·å–åˆ°ç‰¹å®šçš„ä½æ•°ï¼Œä½†æ˜¯è¿™æ ·è·å–åˆ°çš„è¿˜æ˜¯doubleå€¼ï¼Œå¦‚æœæƒ³è¦å’Œstd::stringä¸€èµ·æ“ä½œæ˜¯ä¼šå‡ºç°é—®é¢˜çš„ã€‚å¦‚æœæˆ‘ä»¬æƒ³è¦æŠŠè¿™ä¸ªdoubleå’Œå…¶ä»–çš„stringæ”¾åˆ°ä¸€èµ·çš„è¯ï¼Œæˆ‘ä»¬å°±éœ€è¦æŒ‰ç…§å¦‚ä¸‹ä»£ç è¿›è¡Œæ“ä½œã€‚ 123456789101112131415#include &lt;iostream&gt;#include &lt;cmath&gt;#include &lt;string&gt;int main()&#123; double a = 43.5555; a = std::round(a * 10) / 10; // å…ˆè½¬æ¢ä¸ºstd::stringç±»å‹ std::string tempre = std::to_string(a); // å¯¹std::stringç±»å‹è¿›è¡Œ10ä½å°æ•°çš„æ ¼å¼åŒ–ï¼ˆä¸‹é¢è¯­å¥ä¼šè¾“å‡º10ä½å°æ•°ï¼‰ std::cout &lt;&lt; tempre.substr(0, tempre.find(&quot;.&quot;) + 2) &lt;&lt; std::endl; return 0;&#125; è¿™å°±æ˜¯æœ¬æ–‡çš„å…¨éƒ¨å†…å®¹äº†ã€‚","categories":[{"name":"ä»£ç ","slug":"ä»£ç ","permalink":"https://www.zl-asica.com/categories/%E4%BB%A3%E7%A0%81/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://www.zl-asica.com/tags/C/"}]},{"title":"ZL Asicaçš„Adobe Premier Pro 2022åŸºç¡€æ•™ç¨‹æ€ç»´å¯¼å›¾(mindmap)","slug":"zla-pr-2022-intro-level","date":"2021-12-31T04:12:00.000Z","updated":"2024-02-13T06:37:05.553Z","comments":true,"path":"2021/zla-pr-2022-intro-level/","permalink":"https://www.zl-asica.com/2021/zla-pr-2022-intro-level/","excerpt":"æœ‰éœ€è¦çš„å—ï¼ï¼ï¼æˆ‘ç»ˆäºæŠŠPRæ•™ç¨‹åšå®Œäº†ï¼Œä¸€å…±10æœŸï¼Œä»ä»Šå¤©10:30å¼€å§‹åˆ°9å·10:30æ¯å¤©ä¸€æœŸï¼Œéƒ½æœ‰CCå­—å¹•å¯ä»¥æ‰“å¼€æ–¹ä¾¿çœ‹ç‚¹å‡»è®¢é˜…åˆé›†ç¬¬ä¸€æ—¶é—´æ”¶åˆ°æ¨é€å› ä¸ºæˆ‘å°±æ˜¯ä»0åŸºç¡€å°ç™½å¼€å§‹è‡ªå­¦çš„PRï¼Œæˆ‘æŒ‰ç…§æˆ‘çš„å­¦ä¹ è·¯å¾„åˆ¶ä½œçš„ä¸€ç³»åˆ—æ•™ç¨‹å¯èƒ½ä¼šæ¯”ç”¨äº†å¾ˆå¤šå¹´Prçš„å¤§ä½¬åˆ¶ä½œçš„æ›´é€‚åˆ0åŸºç¡€å°ç™½ï¼Œå¤§ä½¬ä»¬è¯·æ‰‹ä¸‹ç•™æƒ…","text":"æœ‰éœ€è¦çš„å—ï¼ï¼ï¼æˆ‘ç»ˆäºæŠŠPRæ•™ç¨‹åšå®Œäº†ï¼Œä¸€å…±10æœŸï¼Œä»ä»Šå¤©10:30å¼€å§‹åˆ°9å·10:30æ¯å¤©ä¸€æœŸï¼Œéƒ½æœ‰CCå­—å¹•å¯ä»¥æ‰“å¼€æ–¹ä¾¿çœ‹ç‚¹å‡»è®¢é˜…åˆé›†ç¬¬ä¸€æ—¶é—´æ”¶åˆ°æ¨é€å› ä¸ºæˆ‘å°±æ˜¯ä»0åŸºç¡€å°ç™½å¼€å§‹è‡ªå­¦çš„PRï¼Œæˆ‘æŒ‰ç…§æˆ‘çš„å­¦ä¹ è·¯å¾„åˆ¶ä½œçš„ä¸€ç³»åˆ—æ•™ç¨‹å¯èƒ½ä¼šæ¯”ç”¨äº†å¾ˆå¤šå¹´Prçš„å¤§ä½¬åˆ¶ä½œçš„æ›´é€‚åˆ0åŸºç¡€å°ç™½ï¼Œå¤§ä½¬ä»¬è¯·æ‰‹ä¸‹ç•™æƒ… bilibiliè§†é¢‘åˆé›†é“¾æ¥ https://space.bilibili.com/29018759/channel/collectiondetail?sid=94665 ä¸€ã€è¯¾ç¨‹ä¸å‰ªè¾‘åŸºç¡€ä¸æµç¨‹ä»‹ç»1.å‰ªè¾‘è½¯ä»¶ä»‹ç» Premiere Pro Final Cut Pro Vegas Pro è¾¾èŠ¬å¥‡ iMovie 2.å‰ªè¾‘æµç¨‹ä»‹ç» 1.æ•´ç†ç´ ææ–‡ä»¶ 2.æ–°å»ºå·¥ç¨‹æ–‡ä»¶ 3.å¼€å§‹å‰ªè¾‘ 4.é‡æ–°é¢„è§ˆç¡®è®¤æ²¡æœ‰é”™è¯¯ 5.æ¸²æŸ“å¯¼å‡º 6.å†æ¬¡ç¡®è®¤è§†é¢‘æ²¡æœ‰é”™è¯¯ 7.äºŒå‹&#x2F;äº¤ç‰‡ 3.è§†é¢‘åŸºç¡€çŸ¥è¯† åˆ†è¾¨ç‡ å¸§ç‡ ç¼–ç æ ¼å¼ å°è£…æ ¼å¼ ç ç‡&#x2F;æ¯”ç‰¹ç‡ 4.é¡¹ç›®æ–‡ä»¶ç®¡ç†äºŒã€åˆæ­¥æ¥è§¦Premiere Pro1.åˆ›å»ºå·¥ç¨‹æ–‡ä»¶ é¡¹ç›®åç§° ä¿å­˜ä½ç½® 2.é¦–é€‰é¡¹ä¸å¿«æ·é”® é¦–é€‰é¡¹ åª’ä½“ç¼“å­˜ å†…å­˜ å¿«æ·é”®æŸ¥è¯¢ä¸è®¾ç½® 3.å·¥ä½œåŒºæ¦‚è¿° é¢æ¿ å·¥ä½œåŒº è“è‰²é«˜äº® 4.å¯¼å…¥åª’ä½“ç´ æ æ‹–æ‹½å¯¼å…¥ æ–‡ä»¶-å¯¼å…¥(âŒ˜I&#x2F;control+I) åŒå‡»å¯¼å…¥ åª’ä½“æµè§ˆå™¨ 5.åˆ›å»ºåºåˆ— æ–‡ä»¶-æ–°å»º-åºåˆ—(âŒ˜+N&#x2F;control+N) ç›´æ¥æ‹–æ‹½ å³é”®-ä»å‰ªè¾‘æ–°å»ºåºåˆ— ä¸‰ã€Premiere Proé¢æ¿1.é¡¹ç›®é¢æ¿ ç´ æç®± æ˜¾ç¤ºæ–¹å¼ æ’åºä¸æœç´¢ 2.æºç›‘è§†å™¨ä¸èŠ‚ç›®ç›‘è§†å™¨ æºç›‘è§†å™¨ èŠ‚ç›®ç›‘è§†å™¨ä»‹ç» 3.æ—¶é—´è½´é¢æ¿ æ—¶é—´ç‚¹ è½¨é“ å„ç§å°å·¥å…· å››ã€ç´ æçš„å¯¼å…¥å’Œåˆæ­¥è°ƒæ•´1.å¯¹è§†é¢‘è¿›è¡Œå¯¼å…¥åˆ é™¤ç§»åŠ¨æ“ä½œ 1)å°†ç‰‡æ®µæ·»åŠ åˆ°åºåˆ— iä¸oé€‰æ‹©å…¥ç‚¹å‡ºç‚¹ é€‰æ‹©æ‹–å…¥éŸ³é¢‘ORè§†é¢‘ORå…¨éƒ¨ 2)ä»åºåˆ—ä¸­åˆ é™¤ç‰‡æ®µ åˆ é™¤å‰ªè¾‘å¹¶ä¿ç•™å…¶ç©ºé—´ åˆ é™¤ç‰‡æ®µå¹¶è‡ªåŠ¨å…³é—­é—´éš™ å‘å‰é€‰æ‹©è½¨é“å·¥å…·(å‘åé€‰æ‹©è½¨é“å·¥å…·) æ‘ä½shifté€‰ä¸­å¤šä¸ªç‰‡æ®µ æ‘ä½alt(option)å¤åˆ¶ç‰‡æ®µ âŒ˜+Z&#x2F;control+Zæ’¤é”€ 3)æŒ‰é¡ºåºç§»åŠ¨ç‰‡æ®µ ç›´æ¥æ‹–æ‹½è¿›è¡Œç§»åŠ¨ æ‘ä½âŒ˜&#x2F;controlä»¥åè¿›è¡Œæ‹–æ‹½ é“¾æ¥é€‰æ‹©é¡¹ å‰ƒåˆ€å·¥å…· 2.å¯¼å…¥å›¾åƒåŠè°ƒæ•´ç‰‡æ®µå¤§å° 1)å¤„ç†å›¾åƒæ–‡ä»¶ PSD æ™®é€šå›¾åƒ 2)æ›´æ”¹å‰ªè¾‘ç‰‡æ®µçš„å¤§å° å³é”®-è®¾ç½®ä¸ºå¸§å¤§å° æ•ˆæœé¢æ¿(ç¼©æ”¾ ä½ç½®) 3.æ·»åŠ åŠåˆ é™¤è½¨é“äº”ã€å…³é”®å¸§ã€ç®€æ˜“éŸ³é¢‘è°ƒæ•´åŠæ ‡è®°1.å…³é”®å¸§ å¢å‡å…³é”®å¸§ å‰åå…³é”®å¸§ 2.è¿›è¡Œç®€å•çš„éŸ³é¢‘è°ƒæ•´ éŸ³é¢‘å‰ªè¾‘æ··åˆå™¨ é™éŸ³(mute) ç‹¬å¥(solo) å·¦å³å£°é“ 3.æ ‡è®°åŠŸèƒ½ å¿«æ·é”®Må¢åŠ æ ‡è®° å¢åŠ æ ‡è®°åˆ°ç‰¹å®šç´ æORæ•´ä¸ªæ—¶é—´è½´ è°ƒæ•´æ ‡è®°é¢œè‰²ã€åç§°ã€æ³¨é‡Š æ ‡è®°é¢æ¿ å…­ã€æ·»åŠ æ•ˆæœ1.æ•ˆæœé¢æ¿åŠæ•ˆæœæ§ä»¶2.è½¬åœºä¸è¿‡æ¸¡ è§†é¢‘è½¬åœºä¸è¿‡æ¸¡ éŸ³é¢‘è½¬åœºä¸è¿‡æ¸¡ 3.å¸¸ç”¨æ•ˆæœ è£å‰ª é«˜æ–¯æ¨¡ç³Š å˜å½¢ç¨³å®šå™¨ 4.è‡ªå®šä¹‰é¢„è®¾ä¸ƒ.è°ƒæ•´ç‰‡æ®µæ—¶é•¿ã€å¸§ç‡åŠæ’­æ”¾é€Ÿåº¦1.ä¿®å‰ªæ—¶é—´è½´ä¸Šçš„å‰ªè¾‘ç‰‡æ®µæ—¶é•¿ ç™½è‰²å°ä¸‰è§’ ç›´æ¥æ‹–æ‹½ æ³¢çº¹ç¼–è¾‘å·¥å…·(B) æ»šåŠ¨ç¼–è¾‘å·¥å…·(N) å†…æ»‘å·¥å…·(U) å¤–æ»‘å·¥å…·(Y) Q(å¼€å¤´)å’ŒW(ç»“å°¾) 2.ä½¿ç”¨è½¨é“é”å’ŒåŒæ­¥é” æºä¿®è¡¥ æ’å…¥(,) è¦†ç›–(.) è½¨é“ç›®æ ‡å®šä½ è½¨é“é”å®š åŒæ­¥é”å®š 3.æ›´æ”¹ç´ æå¸§ç‡ä»¥é€‚åº”åºåˆ—4.æ›´æ”¹ç‰‡æ®µçš„æ’­æ”¾é€Ÿåº¦ å¸§é‡‡æ · å¸§æ··åˆ å…‰æµæ³• é¢„æ¸²æŸ“ 5.æ¯”ç‡æ‹‰ä¼¸å·¥å…·(R)å…«ã€åŸºç¡€è°ƒè‰²æ•™ç¨‹1. é¢œè‰²å·¥ä½œåŒº2. Lumetri èŒƒå›´é¢æ¿ å°æ‰³æ‰‹ çŸ¢é‡ç¤ºæ³¢å™¨YUV 3. åŸºæœ¬æ ¡æ­£é¢æ¿ LUT ç™½å¹³è¡¡é€‰æ‹© æ‰‹åŠ¨è°ƒæ•´ è‡ªåŠ¨è°ƒæ•´ 4.å…¶ä»–è°ƒæ•´é¢æ¿ æ›²çº¿ è‰²è½® HSL æ™•å½± 5.å¤åˆ¶è°ƒæ•´å¥½çš„é¢œè‰²é¢„è®¾ä¹ã€åŸºç¡€éŸ³é¢‘æ•™ç¨‹1.å½•åˆ¶ç”»å¤–éŸ³2.åˆ©ç”¨å…³é”®å¸§è°ƒæ•´éŸ³é‡3.åŒå£°é“åˆ‡æ¢ä¸ºå•å£°é“4.å°†éŸ³é‡è°ƒæ•´ä¸ºä¸€è‡´éŸ³é‡ å‰ªè¾‘-éŸ³é¢‘é€‰é¡¹-éŸ³é¢‘å¢ç›Š-æ ‡å‡†åŒ–æ‰€æœ‰å³°å€¼ä¸º å…¨é€‰-å³é”®-éŸ³é¢‘å¢ç›Š-æ ‡å‡†åŒ–æ‰€æœ‰å³°å€¼ä¸º 5.éŸ³é¢‘å·¥ä½œåŒº åŸºæœ¬å£°éŸ³çª—å£ é¢„è®¾ 6.Auditionè”åŠ¨ ä½¿ç”¨Auditionç¼–è¾‘éŸ³é¢‘ ä½¿ç”¨Auditionå¯¹éŸ³é¢‘åšç®€å•çš„é™å™ªå¤„ç† åã€è§†é¢‘å¯¼å‡º1.Premiere Proè‡ªå·±å¯¼å‡º âŒ˜+M&#x2F;control+M é¢„è®¾ ä¿å­˜ä½ç½® 2.Adobe Media Encoderå¯¼å‡º è®¾ç½®é¢„è®¾&#x2F;å¯¼å‡ºé¢„è®¾&#x2F;å¯¼å…¥é¢„è®¾ æ¸²æŸ“é˜Ÿåˆ— æ˜¾å¡ç¡¬ä»¶åŠ é€Ÿ&#x2F;CPUè½¯ä»¶ç¼–ç ","categories":[{"name":"éšæƒ³","slug":"éšæƒ³","permalink":"https://www.zl-asica.com/categories/%E9%9A%8F%E6%83%B3/"}],"tags":[{"name":"PR","slug":"PR","permalink":"https://www.zl-asica.com/tags/PR/"}]},{"title":"è¥¿è”æ±‡æ¬¾Western Unionè–…ç¾Šæ¯›-å‘å›½å†…è½¬è´¦100åˆ€å³å¯é€â€‰$20äºšé©¬é€ŠGC(0æˆæœ¬0æ‰‹ç»­è´¹)","slug":"western-union-20-amazon-gc","date":"2021-11-09T07:03:00.000Z","updated":"2024-02-13T06:37:05.553Z","comments":true,"path":"2021/western-union-20-amazon-gc/","permalink":"https://www.zl-asica.com/2021/western-union-20-amazon-gc/","excerpt":"1.è¥¿è”æ±‡æ¬¾ä»‹ç»(Western Union)è¥¿è”æ±‡æ¬¾ï¼ˆWestern Union, NYSEï¼šWUï¼‰äº1851å¹´åœ¨çº½çº¦æˆç«‹ï¼Œç¾åœ¨ç¸½éƒ¨åœ¨ç¾åœ‹ç§‘ç¾…æ‹‰å¤šå·ä¸¹ä½›å¸‚ï¼ˆDenverï¼‰ã€‚æ˜¯ä¸–ç•Œä¸Šé¢†å…ˆçš„ç‰¹å¿«æ±‡æ¬¾å…¬å¸ï¼Œè¿„ä»Šå·²æœ‰150å¹´çš„å†å²ï¼Œå®ƒæ‹¥æœ‰å…¨çƒæœ€å¤§æœ€å…ˆè¿›çš„ç”µå­æ±‡å…‘é‡‘èç½‘ç»œï¼Œä»£ç†ç½‘ç‚¹éå¸ƒå…¨çƒè¿‘200ä¸ªå›½å®¶å’Œåœ°åŒºã€‚ è¥¿è”å…¬å¸æ˜¯ç¾å›½è´¢å¯Œäº”ç™¾å¼ºä¹‹ä¸€çš„ç¬¬ä¸€æ•°æ®å…¬å¸ï¼ˆFDCï¼‰çš„å­å…¬å¸ã€‚","text":"1.è¥¿è”æ±‡æ¬¾ä»‹ç»(Western Union)è¥¿è”æ±‡æ¬¾ï¼ˆWestern Union, NYSEï¼šWUï¼‰äº1851å¹´åœ¨çº½çº¦æˆç«‹ï¼Œç¾åœ¨ç¸½éƒ¨åœ¨ç¾åœ‹ç§‘ç¾…æ‹‰å¤šå·ä¸¹ä½›å¸‚ï¼ˆDenverï¼‰ã€‚æ˜¯ä¸–ç•Œä¸Šé¢†å…ˆçš„ç‰¹å¿«æ±‡æ¬¾å…¬å¸ï¼Œè¿„ä»Šå·²æœ‰150å¹´çš„å†å²ï¼Œå®ƒæ‹¥æœ‰å…¨çƒæœ€å¤§æœ€å…ˆè¿›çš„ç”µå­æ±‡å…‘é‡‘èç½‘ç»œï¼Œä»£ç†ç½‘ç‚¹éå¸ƒå…¨çƒè¿‘200ä¸ªå›½å®¶å’Œåœ°åŒºã€‚ è¥¿è”å…¬å¸æ˜¯ç¾å›½è´¢å¯Œäº”ç™¾å¼ºä¹‹ä¸€çš„ç¬¬ä¸€æ•°æ®å…¬å¸ï¼ˆFDCï¼‰çš„å­å…¬å¸ã€‚ 2.è–…ç¾Šæ¯› ä¸€å®šè¦ä½¿ç”¨ä¸‹é¢çš„é“¾æ¥è¿›è¡Œæ“ä½œï¼Œä¸è¦è‡ªè¡Œå»è¥¿è”æ³¨å†Œï¼Œå¦åˆ™ä½ è®²è–…ä¸åˆ°ç¾Šæ¯›ï¼ï¼ï¼ä½ éœ€è¦æœ‰ç¾å›½å¼€æˆ·çš„å€Ÿè®°å¡è´¦æˆ·&#x2F;ä¸”ä¹‹å‰æ²¡æœ‰æ³¨å†Œè¿‡è¥¿è” 1.é¦–å…ˆç‚¹å‡»è¿™ä¸ªé“¾æ¥ï¼Œä½ ä¼šæ˜¾ç¤ºä¸‹é¢è¿™ä¸ªé¡µé¢ï¼Œç‚¹å‡»I Acceptï¼Œå¡«å†™ä½ çš„ä¸ªäººä¿¡æ¯åŠé‚®ç®±æ³¨å†Œã€‚ 2.ç„¶åä½ çš„é‚®ç®±ä¼šæ”¶åˆ°éªŒè¯ç ï¼Œåƒè¿™æ ·ï¼ŒæŠŠå®ƒå¡«è¿‡å»ã€‚ 3.ä¸‹ä¸€æ­¥ä¾¿æ˜¯æ±‡æ¬¾å›å›½å†…äº†ï¼Œé‚£ä¹ˆæˆ‘ä»¬æ‰€è¯´çš„0æ‰‹ç»­è´¹çš„æ–¹å¼æ˜¯ç¾å›½è¿™è¾¹çš„è´¦æˆ·ç›´æ¥è½¬åˆ°å›½å†…çš„æ”¯ä»˜å®é‡Œï¼Œç›®å‰æˆ‘æµ‹è¯•çš„éƒ½æ˜¯å®æ—¶åˆ°è´¦ï¼Œç™»é™†ä½ çš„BOAã€chaseã€Discoverç­‰ä¸»æµé“¶è¡Œçš„onlinebankingï¼Œå°±å¯ä»¥ç›´æ¥é€‰æ‹©ä½¿ç”¨checking accountæˆ–è€…saving accountä»˜æ¬¾äº†ï¼Œæ”¯ä»˜å®æ”¶é’±æ˜¯0æ‰‹ç»­è´¹çš„ã€‚ 4.é¦–å…ˆæˆ‘ä»¬é€‰æ‹©åœ°åŒºä¸ºä¸­å›½(China). 5.ç„¶åæˆ‘ä»¬è¾“å…¥é‡‘é¢(éœ€è¦å¤§äºç­‰äº100åˆ€)ï¼Œè¿™é‡Œå®ƒä¼šæ˜¾ç¤ºè¥¿è”çš„æ±‡ç‡ï¼Œé‚£ä¹ˆè¿™ä¸ªä»·æ ¼åŸºæœ¬ä¸Šå°±æ˜¯é“¶è¡Œé’ä¹°ä»·ç•¥é«˜ä¸€ç‚¹ï¼Œæ¯”æˆ‘ä»¬ç›´æ¥ç”¨è°·æ­Œçœ‹åˆ°çš„å®æ—¶æ±‡ç‡ç•¥ä½ä¸€äº›ï¼Œæ¯”å¦‚æœ¬ç¯‡æ–‡ç« æ’°å†™çš„æ—¶å€™è°·æ­Œæ˜¾ç¤ºæ±‡ç‡ä¸º6.39(6.3315&#x2F;6.39 &#x3D; åŸºæœ¬ä¸Šå°±æ˜¯ä¸€ä¸ª99æŠ˜ï¼ŒæŸå¤±1åˆ€çš„æ±‡ç‡å·®ä»·)ã€‚æˆ‘ä»¬è¿˜éœ€è¦é€‰æ‹©æ”¶æ¬¾æ–¹å¼ä¸ºAlipay(æ”¯ä»˜å®)ï¼Œä»˜æ¬¾æ–¹å¼é€‰æ‹©Bank Account(é“¶è¡Œè´¦æˆ·)ï¼Œè¿™æ ·æˆ‘ä»¬æ‰èƒ½å¤Ÿ0æ‰‹ç»­è´¹ã€‚ 6.ä¸‹ä¸€æ­¥å°±å¯ä»¥å¡«å†™æ”¶æ¬¾äººä¿¡æ¯å•¦ï¼Œå¡«å†™çš„æ—¶å€™æ³¨æ„ä¸€å®šè¦è¯¦ç»†å¡«å†™ï¼Œæ¯”å¦‚æ”¶æ¬¾äººå«åšå¼ ä¸‰ï¼Œæˆ‘ä»¬å°±åœ¨last nameå¡«å†™ZHANGï¼Œfirst nameå¡«å†™SANï¼Œåœ°å€å¡«å†™ä½ è‡ªå·±å›½å†…ä½å€å³å¯ï¼Œæ‰‹æœºå·ä¹Ÿå¡«å›½å†…çš„å°±å¯ä»¥ã€‚æ³¨æ„æ”¯ä»˜å®çš„é’±åŒ…æ‰‹æœºå·å°±æ˜¯ä½ æ”¯ä»˜å®ç»‘å®šçš„æ‰‹æœºå·ï¼Œä¸€å®šè¦æ­£ç¡®å¡«å†™ï¼ï¼ï¼è½¬æ¬¾ç›®çš„é€‰æ‹©ç”Ÿæ´»è´¹å°±å¥½ï¼Œä¸è¦é€‰æ‹©å·¥èµ„ï¼Œé˜²æ­¢è¢«é£æ§ã€‚ 7.ç„¶åå°±æ˜¯ç™»é™†ä½ çš„ç½‘é“¶ç¡®è®¤è´¦å•ä»˜é’±å°±å¯ä»¥äº†ï¼Œè¿‡ç¨‹éå¸¸çš„ç®€å• 8.ä»˜æ¬¾å®Œæˆåå¤§å¤šæ•°äººä¼šç§’åˆ°è´¦ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå¦‚æœä½ æ²¡æœ‰ç”¨è¿‡æ”¯ä»˜å®å®˜æ–¹çš„é—ªé€Ÿæ”¶æ¬¾ç¬¬ä¸€æ¬¡å¯èƒ½ä¼šæ”¾åœ¨æ”¯ä»˜å®é‡Œç„¶åä½ ä¼šæ”¶åˆ°çŸ­ä¿¡ï¼Œè®¾ç½®æ”¶æ¬¾è´¦æˆ·å°±ä¼šè‡ªåŠ¨æ±‡å…¥ä½ çš„é“¶è¡Œå¡é‡Œäº†ã€‚å½“ç„¶è¿˜æœ‰ä¸€ç§æƒ…å†µæ˜¯ä½ çš„ä»˜æ¬¾è¢«refusedäº†ï¼Œä¸éœ€è¦æ‹…å¿ƒï¼Œå³ä½¿refusedä½ è¿˜æ˜¯èƒ½å¤Ÿæ‹¿åˆ°20åˆ€çš„ç¤¼å“å¡ï¼Œç¨ç­‰ä¸€ä¼šå¦‚æœç¡®è®¤æ²¡æœ‰ç¤¼å“å¡å†é‡æ–°è½¬è´¦ä¹Ÿä¸è¿Ÿã€‚ 9.å¤§çº¦ä»˜æ¬¾å®Œ10åˆ†é’Ÿå·¦å³ï¼Œä½ çš„é‚®ç®±å°±ä¼šæ”¶åˆ°è¥¿è”å‘ç»™ä½ çš„ç¤¼å“å¡å…‘æ¢ç äº†ï¼Œè‡ªå·±å…‘æ¢ä½¿ç”¨å°±å¯ä»¥äº†ã€‚ 3.æ€»ç»“è¿™æ¬¡è–…ç¾Šæ¯›çš„æµç¨‹éå¸¸çš„ç®€å•ï¼ŒåŸºæœ¬åªéœ€è¦2-3minçš„æ“ä½œå°±å¯ä»¥è·å¾—20åˆ€çš„amazon GCï¼Œå¤§å®¶æ²¡æœ‰ä½¿ç”¨è¿‡è¥¿è”æ±‡æ¬¾çš„è±†å¯ä»¥æ“ä½œä¸€ä¸‹ã€‚æ³¨æ„ä¸€å®šè¦ä½¿ç”¨æˆ‘æ”¾åœ¨è¿™é‡Œçš„é“¾æ¥è¿›è¡Œæ³¨å†Œï¼Œå¦åˆ™ä½ ä¸ä»…ä¸ä¼šè·å¾—è¿™20åˆ€çš„ç¾Šæ¯›ï¼Œä»¥åçš„æ–°ç”¨æˆ·æ³¨å†Œæ´»åŠ¨ä¹Ÿå’Œä½ æ— å…³äº†ã€‚è¥¿è”æ±‡æ¬¾ç¾Šæ¯›é“¾æ¥(æ³¨å†Œåæ±‡æ¬¾100åˆ€å³å¯è·å¾—20åˆ€GC)","categories":[{"name":"éšæƒ³","slug":"éšæƒ³","permalink":"https://www.zl-asica.com/categories/%E9%9A%8F%E6%83%B3/"}],"tags":[{"name":"ç¾Šæ¯›","slug":"ç¾Šæ¯›","permalink":"https://www.zl-asica.com/tags/%E7%BE%8A%E6%AF%9B/"}]},{"title":"C++å®ç°åŠ¨æ€äºŒç»´æ•°ç»„çš„ä¸¤ç§ç®€æ˜“æ–¹æ³•","slug":"cpp-2d-dynamical-array","date":"2021-10-28T00:21:00.000Z","updated":"2024-02-13T06:37:05.553Z","comments":true,"path":"2021/cpp-2d-dynamical-array/","permalink":"https://www.zl-asica.com/2021/cpp-2d-dynamical-array/","excerpt":"1.C++çš„åŠ¨æ€æ•°ç»„(Dynamically-allocated Single-Dimension arrays)å½“æ— æ³•ä½¿ç”¨é™æ€åˆ†é…çš„æ•°ç»„æ—¶ â€” å› ä¸ºä½ åœ¨ç¼–è¯‘æ—¶ä¸çŸ¥é“åˆé€‚çš„å¤§å°ï¼Œæˆ–è¯¥æ•°ç»„å¤ªå¤§è€Œæ— æ³•åˆç†åœ°æ”¾å…¥è¿è¡Œæ—¶å †æ ˆï¼Œæˆ–è¯¥æ•°ç»„çš„ç”Ÿå‘½å‘¨æœŸæ¯”åˆ›å»ºå®ƒçš„å‡½æ•°æ›´é•¿ã€‚åŠ¨æ€åˆ†é…æ•°ç»„æ¶‰åŠåˆ°ä½¿ç”¨newè¿ç®—ç¬¦ï¼Œç±»ä¼¼äºå…¶ä»–å¯¹è±¡çš„åŠ¨æ€åˆ†é…ï¼Œä¸åŒä¹‹å¤„åœ¨äºæ•°ç»„éœ€è¦æˆ‘ä»¬å¦å¤–æŒ‡å®šå¤§å°ã€‚","text":"1.C++çš„åŠ¨æ€æ•°ç»„(Dynamically-allocated Single-Dimension arrays)å½“æ— æ³•ä½¿ç”¨é™æ€åˆ†é…çš„æ•°ç»„æ—¶ â€” å› ä¸ºä½ åœ¨ç¼–è¯‘æ—¶ä¸çŸ¥é“åˆé€‚çš„å¤§å°ï¼Œæˆ–è¯¥æ•°ç»„å¤ªå¤§è€Œæ— æ³•åˆç†åœ°æ”¾å…¥è¿è¡Œæ—¶å †æ ˆï¼Œæˆ–è¯¥æ•°ç»„çš„ç”Ÿå‘½å‘¨æœŸæ¯”åˆ›å»ºå®ƒçš„å‡½æ•°æ›´é•¿ã€‚åŠ¨æ€åˆ†é…æ•°ç»„æ¶‰åŠåˆ°ä½¿ç”¨newè¿ç®—ç¬¦ï¼Œç±»ä¼¼äºå…¶ä»–å¯¹è±¡çš„åŠ¨æ€åˆ†é…ï¼Œä¸åŒä¹‹å¤„åœ¨äºæ•°ç»„éœ€è¦æˆ‘ä»¬å¦å¤–æŒ‡å®šå¤§å°ã€‚ åŠ¨æ€åˆ†é…æ•°ç»„æ˜¯ä½¿ç”¨newè¡¨è¾¾å¼å®Œæˆçš„ï¼Œè¯¥è¡¨è¾¾å¼ä¼šåŠ¨æ€åˆ†é…è¶³å¤Ÿçš„å†…å­˜æ¥å­˜å‚¨æ•´ä¸ªæ•°ç»„ï¼Œç„¶åè¿”å›æŒ‡å‘æ•°ç»„ç¬¬ä¸€ä¸ªå•å…ƒæ ¼çš„æŒ‡é’ˆã€‚ 1int* a = new int[10]; è¡¨è¾¾å¼ new int[10] åœ¨å †ä¸Šåˆ†é…ä¸€ä¸ªè¶³å¤Ÿå¤§çš„å†…å­˜å—æ¥å­˜å‚¨ 10 ä¸ªæ•´æ•°ï¼Œå¹¶è¿”å›ä¸€ä¸ªæŒ‡å‘ç¬¬ä¸€ä¸ªçš„æŒ‡é’ˆã€‚ç¬¬äºŒä¸ªçš„ä½ç½®ç›´æ¥è·Ÿåœ¨ç¬¬ä¸€ä¸ªä¹‹åï¼Œç¬¬ä¸‰ä¸ªç›´æ¥è·Ÿåœ¨ç¬¬äºŒä¸ªä¹‹åï¼Œä¾æ­¤ç±»æ¨ï¼Œæ‰€æœ‰çš„å•å…ƒæ ¼éƒ½æ˜¯ç›¸åŒçš„ç±»å‹ã€‚æ‰€ä»¥ç»™å®šä¸€ä¸ªæŒ‡å‘ç¬¬ä¸€ä¸ªå•å…ƒæ ¼çš„æŒ‡é’ˆå’Œä¸€ä¸ªç´¢å¼•ï¼Œåœ¨å¹•åï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹è®¡ç®—æ¥è®¡ç®—ä»»ä½•ç»™å®šå•å…ƒæ ¼çš„ç´¢å¼•ï¼š 1å•å…ƒæ ¼ i çš„åœ°å€ = (å•å…ƒæ ¼ 0 çš„åœ°å€) + (sizeof(int) * i) æ³¨æ„ï¼Œè®¡ç®—å…·æœ‰å¤§ç´¢å¼•çš„å•å…ƒæ ¼çš„åœ°å€å¹¶ä¸æ¯”ä½¿ç”¨å°ç´¢å¼•æ›´æ¶ˆè€—æ€§èƒ½ã€‚è¿™ä¸ªè®¡ç®—åªæ˜¯ä¸€ä¸ªä¹˜æ³•å’ŒåŠ æ³•ï¼›å¦‚æœæˆ‘ä»¬å‡è®¾ç»™å®šåœ°å€çš„å†…å­˜è®¿é—®éœ€è¦æ’å®šæ—¶é—´ï¼Œé‚£ä¹ˆè®¿é—®æ•°ç»„ä¸­çš„ä»»ä½•å•å…ƒéƒ½éœ€è¦æ’å®šæ—¶é—´ã€‚ï¼ˆåœ¨å®è·µä¸­ï¼Œå†…å­˜è®¿é—®æ—¶é—´å¯èƒ½ä¼šå› ç¼“å­˜ç­‰å½±å“è€Œæœ‰æ‰€ä¸åŒï¼Œå°½ç®¡ä½ å¯ä»¥åˆç†åœ°è®¤ä¸ºä¸»å†…å­˜è®¿é—®èŠ±è´¹æ’å®šæ—¶é—´ã€‚ï¼‰ æœ‰è¶£çš„æ˜¯ï¼Œè¿”å›çš„æŒ‡é’ˆç±»å‹å¹¶æœªæŒ‡å®šæœ‰å…³æ•°ç»„çš„ä»»ä½•å†…å®¹ã€‚å½“æˆ‘ä»¬åŠ¨æ€åˆ†é…ä¸€ä¸ªintæ•°ç»„æ—¶ï¼Œæˆ‘ä»¬å¾—åˆ°çš„æ˜¯ä¸€ä¸ªintï¼Œå³ä¸€ä¸ªæŒ‡å‘intçš„æŒ‡é’ˆã€‚æ•°ç»„åœ¨ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œå®ç°ä¸ºæŒ‡é’ˆï¼Œä»¥ä»–ä»¬çš„ç¬¬ä¸€ä¸ªå•å…ƒï¼Œå®ƒæ˜¯ç”±æˆ‘ä»¬æ¥äº†è§£æ˜¯å¦ç‰¹å®šintæŒ‡å‘ä¸€ä¸ªå•ä¸€çš„intæˆ–intæ•°ç»„ã€‚ ä¸€æ—¦æœ‰äº†æŒ‡å‘æ•°ç»„çš„æŒ‡é’ˆï¼Œå°±å¯ä»¥åƒè®¿é—®é™æ€åˆ†é…çš„æ•°ç»„ä¸€æ ·è®¿é—®å®ƒçš„æ¯ä¸€ä¸ªæ•°å­—ï¼š 123int* a = new int[10];a[3] = 4;std::cout &lt;&lt; a[3] &lt;&lt; std::endl; a[3]ç›¸å½“äºå‡æƒ³è¡¨è¾¾*qï¼Œå…¶ä¸­qæ˜¯ä¸€ä¸ªæŒ‡å‘intå ç”¨ä¸‰ä¸ªå•ä½çš„ä¸€ä¸ªç‚¹ã€‚æ¢å¥è¯è¯´ï¼Œa[3]ç»™ä½ æä¾›çš„æ˜¯è¿™ä¸ªå•å…ƒæ ¼ä¸­çš„intï¼ˆä»ç†è®ºä¸Šè®²ï¼Œæ˜¯å¯¹å®ƒçš„å¼•ç”¨ï¼‰ï¼Œè€Œä¸æ˜¯æŒ‡å‘è¿™ä¸ªå•å…ƒæ ¼çš„æŒ‡é’ˆã€‚ å½“å®ŒæˆåŠ¨æ€åˆ†é…çš„æ•°ç»„æ—¶ï¼Œéœ€è¦é‡Šæ”¾å®ƒï¼Œå°±åƒæ‚¨å¤„ç†ä»»ä½•å…¶ä»–åŠ¨æ€åˆ†é…çš„å¯¹è±¡ä¸€æ ·ã€‚ä½†æ˜¯ï¼Œé‡è¦çš„æ˜¯è¦æ³¨æ„ä½ è¦ä½¿ç”¨ä¸åŒçš„è¿ç®—ç¬¦delete[]æ¥æ‰§è¡Œæ­¤æ“ä½œã€‚åƒdeleteä¸€æ ·ï¼Œä½ ç»™delete[]ä¸€ä¸ªæŒ‡å‘æ•°ç»„çš„æŒ‡é’ˆï¼Œå®ƒä¼šä¸ºä½ é‡Šæ”¾æ•´ä¸ªæ•°ç»„ï¼ˆä»¥åŠå®ƒæ‰€æœ‰å•å…ƒæ ¼ä¸­çš„æ‰€æœ‰å¯¹è±¡ï¼‰ã€‚ 1delete[] a; ç”±ä½ å†³å®šå“ªäº›æŒ‡é’ˆæŒ‡å‘æ•°ç»„ï¼Œå¹¶åœ¨éœ€è¦æ—¶ä½¿ç”¨delete[]ã€‚åœ¨æŒ‡å‘æ•°ç»„çš„æŒ‡é’ˆä¸Šä½¿ç”¨deleteè€Œä¸æ˜¯delete[ ]ï¼Œæˆ–åœ¨æŒ‡å‘å•ä¸ªå€¼çš„æŒ‡é’ˆä¸Šä½¿ç”¨delete[]è€Œä¸æ˜¯deleteï¼Œä¼šå¯¼è‡´â€œundefined behaviorâ€ã€‚å°±åƒæˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ä½¿ç”¨deleteä¸€æ ·ï¼ŒæŒ‡é’ˆaä¸å—å½±å“ï¼Œå°½ç®¡å®ƒç°åœ¨æŒ‡å‘æœªåˆ†é…çš„å†…å­˜ï¼Œå› æ­¤å®ƒç°åœ¨å°±ä¸ä¼šè¢«å†ä½¿ç”¨ã€‚ 2.C++åŠ¨æ€åˆ†é…äºŒç»´æ•°ç»„-ä½¿ç”¨å¸¸è§„æ–¹å¼(å³ä¸ä½¿ç”¨é™¤iostreamå¤–çš„å†…ç½®åº“)åŠ¨æ€äºŒç»´æ•°ç»„çš„åˆ†é… 123int** test = new int*[rows]; for(int i = 0; i &lt; rows; i++) test[i] = new int[columns]; é‡Šæ”¾åŠ¨æ€åˆ†é…äºŒç»´æ•°ç»„å ç”¨çš„ç©ºé—´ 123for(int i = 0; i &lt; rows; i++) delete []test[i]; delete []test; 3.C++åŠ¨æ€åˆ†é…äºŒç»´æ•°ç»„-ä½¿ç”¨Vector(éœ€è¦é¢å¤–çš„åº“vector)æ³¨æ„ä½¿ç”¨è¿™ç§æ–¹æ³•ä½ éœ€è¦åœ¨æ–‡ä»¶å¤´ 1#include &lt;vector&gt; ä½¿ç”¨vectoråŠ¨æ€åˆ†é…äºŒç»´æ•°ç»„ 123std::vector&lt;std::vector&lt;int&gt;&gt; test(rows); for (i = 0; i &lt; test.size(); i++) test[i].resize(columns); ä½¿ç”¨swap()æ–¹æ³•æ¸…ç©ºvectorå ç”¨çš„å†…å­˜ç©ºé—´ 123// vectorçš„clearæ–¹æ³•æ— æ³•æ¸…ç©ºå…¶å ç”¨çš„å†…å­˜//å¯ä»¥ä½¿ç”¨swapæ¥ä½¿ä¸€ä¸ªç©ºçš„vectoræ¥æ›¿æ¢æ‰åŸæ¥testé‡Œæœ‰çš„å†…å®¹ã€‚vector&lt;T&gt;().swap(test);","categories":[{"name":"ä»£ç ","slug":"ä»£ç ","permalink":"https://www.zl-asica.com/categories/%E4%BB%A3%E7%A0%81/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://www.zl-asica.com/tags/C/"}]},{"title":"Javaåˆ¤æ–­ä¸€ä¸ªæ—¥æœŸå‡ ä¸ªå·¥ä½œæ—¥åçš„æ—¥æœŸï¼ˆå¿½ç•¥èŠ‚å‡æ—¥ï¼‰","slug":"java-get-date-after-several-work-day","date":"2021-10-27T23:09:00.000Z","updated":"2024-02-13T06:37:05.553Z","comments":true,"path":"2021/java-get-date-after-several-work-day/","permalink":"https://www.zl-asica.com/2021/java-get-date-after-several-work-day/","excerpt":"1.å‰è¨€å‰å‡ å¤©å†™ä½œä¸šçš„æ—¶å€™ç¢°åˆ°ä¸€é“é¢˜ç›®ï¼Œé¢˜ç›®å¤§æ„å¦‚ä¸‹ ä¸åŒçš„ç‰©å“é…é€é€Ÿåº¦ä¸åŒï¼Œå¸¸è§„å•†å“éœ€è¦5ä¸ªå·¥ä½œæ—¥ï¼Œé£Ÿå“éœ€è¦1ä¸ªå·¥ä½œæ—¥ï¼Œæ•°ç äº§å“ç«‹å³é€è¾¾ï¼ˆé€šè¿‡é‚®ç®±ï¼‰ã€‚æ ¹æ®ç”¨æˆ·çš„ä¸‹å•æ—¥æœŸå’Œç±»åˆ«åˆ¤æ–­å“ªå¤©èƒ½é€åˆ°ï¼ˆå¿½ç•¥èŠ‚å‡æ—¥ï¼‰ã€‚","text":"1.å‰è¨€å‰å‡ å¤©å†™ä½œä¸šçš„æ—¶å€™ç¢°åˆ°ä¸€é“é¢˜ç›®ï¼Œé¢˜ç›®å¤§æ„å¦‚ä¸‹ ä¸åŒçš„ç‰©å“é…é€é€Ÿåº¦ä¸åŒï¼Œå¸¸è§„å•†å“éœ€è¦5ä¸ªå·¥ä½œæ—¥ï¼Œé£Ÿå“éœ€è¦1ä¸ªå·¥ä½œæ—¥ï¼Œæ•°ç äº§å“ç«‹å³é€è¾¾ï¼ˆé€šè¿‡é‚®ç®±ï¼‰ã€‚æ ¹æ®ç”¨æˆ·çš„ä¸‹å•æ—¥æœŸå’Œç±»åˆ«åˆ¤æ–­å“ªå¤©èƒ½é€åˆ°ï¼ˆå¿½ç•¥èŠ‚å‡æ—¥ï¼‰ã€‚ åœ¨ç½‘ä¸Šæœäº†ä¸€åœˆå‘ç°åŸºæœ¬ä¸Šéƒ½æ˜¯ä½¿ç”¨çš„å„ç§éå¸¸å¤æ‚çš„æ–¹å¼å¹¶ä¸”æœ‰å„ç§å…¼å®¹æ€§é—®é¢˜ï¼ŒåŸºæœ¬ä¸Šä½¿ç”¨çš„éƒ½æ˜¯æ•°ç»„+Calendarçš„å½¢å¼æ¥å†™çš„ã€‚è¿™é‡Œæ ¹æ®æ•™æˆçš„æç¤ºäº†è§£åˆ°äº†ä¸€ç§éå¸¸æ–¹ä¾¿çš„å½¢å¼æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ 2.æ€è·¯1.é¢˜ç›®åˆ†è§£é¦–å…ˆæˆ‘ä»¬éœ€è¦åˆ¤æ–­æ˜¯ä»€ä¹ˆç±»å‹çš„äº§å“ï¼Œç„¶åå†æ ¹æ®äº§å“å¯¹æ—¥æœŸè¿›è¡Œå¤„ç†å¹¶åˆ‡è¿”å›ç»™ç”¨æˆ·ã€‚ 2.æ—¥æœŸæ ¼å¼è¿™é“é¢˜ç›®æ˜¯ç”¨çš„æ ¼å¼ä¸ºyyyy-mm-ddï¼Œæ‰€ä»¥è¿™é“é¢˜æˆ‘ä»¬ä¹Ÿéœ€è¦æ ¹æ®è¿™ä¸ªæ–¹å¼æ¥å†™ã€‚ 3.æ–¹æ³•æ ¹æ®æ•™æˆçš„æç¤ºJavaæœ‰ä¸€ä¸ªéå¸¸é€‚åˆè§£å†³è¿™ä¸ªé—®é¢˜çš„ç±» java.time.LocalDate è¿™é‡Œæ˜¯å®˜æ–¹æ–‡æ¡£https://docs.oracle.com/javase/8/docs/api/java/time/LocalDate.html æœ‰å…´è¶£çš„å¯ä»¥è‡ªè¡ŒæŸ¥çœ‹ã€‚æ ¹æ®æ–‡æ¡£æˆ‘ä»¬å¯ä»¥è·å¾—å‡ ä¸ªæ–¹æ³•æ¥å¸®æˆ‘ä»¬è§£å†³é—®é¢˜ã€‚ LocalDate 12LocalDate a = LocalDate.of(2012, 6, 30);// è¿™é‡Œå°±æ˜¯æŠŠ2021.6.30è½¬æ¢åˆ°äº†java.timeå¯ä»¥è¯†åˆ«çš„æ—¥æœŸæ ¼å¼LocalDateã€‚ plusDays 12a = a.plusDays(1);// æ­¤å¤„çš„aå°±ä¼šå˜æˆ2021.7.1ï¼Œæ—¥æœŸä¼šè‡ªåŠ¨æ›´æ–°åˆ°ä¸‹ä¸€ä¸ªæœˆã€‚ getDayOfWeek 123a.getDayOfWeek();// è¿™é‡Œå¯ä»¥è·å–åˆ°açš„æ—¥æœŸå¯¹åº”çš„æ˜¯æ˜ŸæœŸå‡ ã€‚è¯¥æ–¹æ³•è¿”å›æ˜¯å‘¨ä¸­å“ªå¤©çš„æšä¸¾DayOfWeekã€‚è¿™å°±é¿å…äº†å¯¹intå€¼å«ä¹‰çš„æ··æ·†ã€‚å¦‚æœä½ éœ€è¦è®¿é—®åŸå§‹çš„intå€¼ï¼Œé‚£ä¹ˆæšä¸¾æä¾›äº†intå€¼ã€‚// æ¯”å¦‚è¿™é‡Œæ˜¯2021.7.1ï¼Œä¹Ÿå°±æ˜¯æ˜ŸæœŸå››ã€‚ 4.ä»£ç ä¸‹é¢æ˜¯æˆ‘ä»¬æ­£å¼å®ç°åŠŸèƒ½çš„å‡½æ•° 123456789101112131415161718192021public String purchase(String purchaseDate) &#123; // ä»¥é£Ÿå“ç±»ä¸¾ä¾‹ï¼Œé…é€æ—¶é—´ä¸ºä¸€ä¸ªå·¥ä½œæ—¥ï¼ŒpurchaseDateæ˜¯Stringç±»åˆ«ä»¥æ ¼å¼ä¸ºYYYY-MM-DDçš„æ—¥æœŸã€‚ String[] strOfdate = purchaseDate.split(&quot;-&quot;, 3); // ä»¥-åˆ†å‰²ç”¨æˆ·è¾“å…¥çš„å­—ç¬¦ä¸²ä¿å­˜åˆ°åä¸ºstrOfdateçš„å­—ç¬¦ä¸²æ•°ç»„ä¸­ LocalDate finaldate = LocalDate.of(Integer.parseInt(strOfdate[0]), Integer.parseInt(strOfdate[1]), Integer.parseInt(strOfdate[2])); // è®¾å®šä¸€ä¸ªLocalDateå˜é‡ï¼Œå˜é‡åä¸ºfinaldateï¼Œå°†åˆ†å‰²å¥½çš„æ—¥æœŸèµ‹å€¼ç»™å®ƒ int daycalc = 0; do &#123; finaldate = finaldate.plusDays(1); // æŠŠç”¨æˆ·è¾“å…¥çš„æ—¥æœŸåŠ ä¸€å¤© if ((finaldate.getDayOfWeek() != DayOfWeek.SATURDAY &amp;&amp; finaldate.getDayOfWeek() != DayOfWeek.SUNDAY)) &#123; // åˆ¤æ–­åŠ äº†ä¸€å¤©ä»¥åæ˜¯å¦æ˜¯å‘¨å…­æˆ–è€…å‘¨æ—¥ï¼Œå¦‚æœæ˜¯å°±ç»§ç»­å¾ªç¯ ++daycalc; // å¦‚æœä¸æ˜¯å°±æŠŠdaycalcå‚æ•°åŠ ä¸€ï¼Œè®¡ä½œä¸€ä¸ªå·¥ä½œæ—¥ã€‚ &#125; &#125;while (daycalc &lt; 1); // å½“å·¥ä½œæ—¥ç­‰äº1çš„æ—¶å€™åœæ­¢å¾ªç¯ return finaldate.toString(); // å°†æ—¥æœŸè½¬æ¢ä¸ºStringç±»å‹è¿”å›ç»™ç”¨æˆ·&#125; å¦‚æœæœ‰ä»»ä½•é—®é¢˜æ¬¢è¿åœ¨ä¸‹é¢ç•™è¨€ï¼Œè½¬è½½æ–‡ç« è¯·ä¿ç•™æœ¬æ–‡é“¾æ¥åŠæˆ‘çš„IDZL Asica","categories":[{"name":"ä»£ç ","slug":"ä»£ç ","permalink":"https://www.zl-asica.com/categories/%E4%BB%A3%E7%A0%81/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://www.zl-asica.com/tags/Java/"}]},{"title":"FFmpegçš„å®‰è£…ä¸åŸºç¡€ä½¿ç”¨æ•™ç¨‹","slug":"ffmpeg","date":"2020-11-21T03:01:28.000Z","updated":"2024-02-13T06:37:05.553Z","comments":true,"path":"2020/ffmpeg/","permalink":"https://www.zl-asica.com/2020/ffmpeg/","excerpt":"ä»‹ç»FFmpeg æ˜¯ä¸€ä¸ªå¼€æ”¾æºä»£ç çš„è‡ªç”±è½¯ä»¶ï¼Œå¯ä»¥è¿è¡ŒéŸ³é¢‘å’Œè§†é¢‘å¤šç§æ ¼å¼çš„å½•å½±ã€è½¬æ¢ã€æµåŠŸèƒ½ï¼ŒåŒ…å«äº†libavcodecâ€”â€”è¿™æ˜¯ä¸€ä¸ªç”¨äºå¤šä¸ªé¡¹ç›®ä¸­éŸ³é¢‘å’Œè§†é¢‘çš„è§£ç å™¨åº“ï¼Œä»¥åŠlibavformatâ€”â€”ä¸€ä¸ªéŸ³é¢‘ä¸è§†é¢‘æ ¼å¼è½¬æ¢åº“ã€‚ â€œFFmpegâ€è¿™ä¸ªå•è¯ä¸­çš„â€œFFâ€æŒ‡çš„æ˜¯â€œFast Forwardâ€ã€‚","text":"ä»‹ç»FFmpeg æ˜¯ä¸€ä¸ªå¼€æ”¾æºä»£ç çš„è‡ªç”±è½¯ä»¶ï¼Œå¯ä»¥è¿è¡ŒéŸ³é¢‘å’Œè§†é¢‘å¤šç§æ ¼å¼çš„å½•å½±ã€è½¬æ¢ã€æµåŠŸèƒ½ï¼ŒåŒ…å«äº†libavcodecâ€”â€”è¿™æ˜¯ä¸€ä¸ªç”¨äºå¤šä¸ªé¡¹ç›®ä¸­éŸ³é¢‘å’Œè§†é¢‘çš„è§£ç å™¨åº“ï¼Œä»¥åŠlibavformatâ€”â€”ä¸€ä¸ªéŸ³é¢‘ä¸è§†é¢‘æ ¼å¼è½¬æ¢åº“ã€‚ â€œFFmpegâ€è¿™ä¸ªå•è¯ä¸­çš„â€œFFâ€æŒ‡çš„æ˜¯â€œFast Forwardâ€ã€‚ å®‰è£…æœ¬æ–‡åªå•ç‹¬ä»‹ç»å¦‚ä½•åœ¨Windowså’ŒmacOSä¸‹å®‰è£…FFmpegï¼Œæš‚ä¸è®¨è®ºåœ¨Linuxä¸‹çš„æƒ…å†µã€‚FFmpegçš„å®˜ç½‘ä¸ºhttps://ffmpeg.org/download.html Windows é¦–å…ˆæ‰“å¼€ä¸Šé¢çš„å®˜ç½‘ä¸‹è½½é“¾æ¥ï¼Œæ‰¾åˆ°Windowsæ¨¡å—ä¸‹çš„Windows builds from gyan.dev åœ¨æ–°æ‰“å¼€çš„gyan.devçš„é¡µé¢ä¸­æ‰¾åˆ°Releaseéƒ¨åˆ†ï¼ŒLinksé‡Œç¬¬ä¸€ä¸ªfull(å¦‚çº¢ç®­å¤´æ‰€ç¤º)çš„é“¾æ¥ç›´æ¥ç‚¹å‡»ä¸‹è½½FFmpegçš„æœ€æ–°ç‰ˆå‹ç¼©åŒ…ã€‚ ä¸‹è½½ä¸‹æ¥çš„7zå®‰è£…åŒ…å…ˆè§£å‹ï¼Œç„¶åå°†è§£å‹åçš„æ–‡ä»¶å¤¹æ”¾è‡³ä½ ä¸ä¼šéšæ„åˆ æ‰æˆ–æ”¹åŠ¨ä¸ºæ­¢çš„è·¯å¾„ä¸‹(å¦‚Cç›˜çš„Program Filesä½†ä¸æ˜¯å¿…é¡»æ”¾åˆ°Cç›˜)ã€‚ å¤åˆ¶ffmpegè§£å‹åæ–‡ä»¶å¤¹å†…çš„binæ–‡ä»¶å¤¹è·¯å¾„(å¦‚ä¸‹å›¾æ‰€ç¤º) æ‰“å¼€è®¾ç½®-ç³»ç»Ÿ-å…³äº-é«˜çº§ç³»ç»Ÿè®¾ç½® æ‰“å¼€é«˜çº§ç³»ç»Ÿè®¾ç½®åç‚¹å¼€ç¯å¢ƒå˜é‡ï¼Œæ‰¾åˆ°ç³»ç»Ÿå˜é‡ä¸­çš„Pathå˜é‡åŒå‡»ç‚¹å¼€ã€‚ æ–°æ‰“å¼€çš„é¡µé¢ç‚¹å‡»å³è¾¹çš„æ–°å»º,ç²˜è´´è¿›å»åœ¨ç¬¬å››æ­¥å¤åˆ¶çš„binæ–‡ä»¶å¤¹é“¾æ¥ æ·»åŠ å®Œåä¸€æ­¥ä¸€æ­¥ç¡®å®š-ç¡®å®š-ç¡®å®šã€‚ win+Rï¼Œè¾“å…¥cmdï¼Œå›è½¦ï¼Œæ‰“å¼€cmd è¾“å…¥FFmpegå¹¶å›è½¦æµ‹è¯•æ˜¯å¦å®‰è£…æˆåŠŸï¼Œæ˜¾ç¤ºç±»ä¼¼ä¸‹å›¾å³ä¸ºå®‰è£…æˆåŠŸ macOS command+ç©ºæ ¼æ‰“å¼€èšç„¦æœç´¢ï¼Œè¾“å…¥terminalå¹¶å›è½¦æ‰“å¼€ç»ˆç«¯ è¾“å…¥ä¸‹è¿°å‘½ä»¤å®‰è£…homebrew 1/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)&quot; homebrewå®‰è£…è¿‡ç¨‹ä¸­å¯èƒ½ä¼šéœ€è¦rootæƒé™(ç®¡ç†å‘˜æƒé™)ï¼Œå±Šæ—¶éœ€è¦è¾“å…¥ä½ çš„ç³»ç»Ÿå¯†ç ï¼Œè¾“å…¥æ—¶ä¸ä¼šæ˜¾ç¤ºä½ è¾“å…¥çš„å†…å®¹ï¼Œè¾“å…¥å®Œæˆå›è½¦å³å¯ã€‚3. homebrewå®‰è£…å®Œæˆåè¾“å…¥ä¸‹è¿°å‘½ä»¤å®‰è£…ffmpeg 1brew install ffmpeg å®‰è£…å®Œæˆåè¾“å…¥ffmpegæµ‹è¯•æ˜¯å¦å®‰è£…æˆåŠŸï¼Œæ˜¾ç¤ºç±»ä¼¼ä¸‹å›¾å³ä¸ºå®‰è£…æˆåŠŸ åŸºç¡€ä½¿ç”¨æ ¼å¼è½¬æ¢FFmpegè½¬æ¢æ ¼å¼æœ€ç®€å•æœ€å¸¸ç”¨çš„å‘½ä»¤å¦‚ä¸‹ï¼š 1ffmpeg -i input.xxx output.xxx ä¾‹å¦‚æˆ‘ä»¬æœ‰åŸè§†é¢‘a.movæƒ³è¦è½¬æˆmp4æ ¼å¼å¹¶æ›´æ”¹æ–‡ä»¶åä¸ºbæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤ 1ffmpeg -i a.mov b.mp4 mkvè§£å°ï¼Œç›´æ¥å¤åˆ¶éŸ³é¢‘ä¸è§†é¢‘æµåˆ°mp4ä¸­è¿›è¡Œé‡æ–°å°è£…(æ­¤æ–¹å¼é€‚ç”¨äºflvæ ¼å¼ï¼Œä¾‹å¦‚Bç«™ä¸‹ä¸‹æ¥çš„)ï¼Œç”±äºä¸éœ€è¦é‡æ–°ç¼–ç ï¼Œæ­¤ä»£ç çš„è½¬æ¢é€Ÿåº¦å–å†³äºä½ ç”µè„‘çš„ç¡¬ç›˜é€Ÿåº¦ã€‚ 1ffmpeg -i a.mkv -vcodec copy -acodec copy b.mp4 è§†é¢‘å‹ç¼©FFmpegå‹ç¼©è§†é¢‘åº”ä½¿ç”¨ç±»ä¼¼å¦‚ä¸‹æ ¼å¼çš„å‘½ä»¤ï¼š 12345ffmpeg -i input.mp4 -r 10 -b:a 32k output.mp4 #å¯¹å®ƒé™ä½fpså’ŒéŸ³é¢‘ç ç‡çš„æ–¹æ³•å¤§å¤§å‹ç¼©æ–‡ä»¶å¤§å°ï¼Œè€Œæ¸…æ™°åº¦ä¸å˜ã€‚#æˆ–è€…ffmpeg -i input.mp4 -vcodec libx264 -crf 22 output.mp4 #å°†åŸè§†é¢‘è½¬æ¢æˆH.264æ ¼å¼å¹¶å‹ç¼©ï¼Œåªå‹ç¼©ç ç‡ï¼Œå…¶ä»–ä¸å˜#å†æˆ–è€…ffmpeg -i input.webm -vcodec libx264 -crf 20 -acodec aac output.mp4 #å°†YouTube vp9ç¼–ç è½¬æ¢ä¸ºh264ç¼–ç  å‘½ä»¤é€‰é¡¹ä»‹ç»-r ç ç‡-b:a éŸ³é¢‘ç ç‡-vcodec è§†é¢‘ç¼–ç -crf æ§åˆ¶ä¸å˜ç ç‡(é‡åŒ–æ¯”ä¾‹çš„èŒƒå›´ä¸º0 ~ 51ï¼Œå…¶ä¸­0ä¸ºæ— æŸæ¨¡å¼ï¼Œ23ä¸ºç¼ºçœå€¼ï¼Œ51å¯èƒ½æ˜¯æœ€å·®çš„ï¼Œæ¨èæ—¥å¸¸ä½¿ç”¨18-22ã€‚)-acodec éŸ³é¢‘ç¼–ç å¦‚æœæƒ³è¦åœ¨è½¬ç å‹åˆ¶è§†é¢‘æ—¶ä¿æŒéŸ³é¢‘ä¸å¯¹éŸ³é¢‘è¿›è¡Œå¤„ç†è¯·åœ¨å‘½ä»¤è¡Œé‡ŒåŠ å…¥ä¸‹è¿°å‘½ä»¤ç›´æ¥å¤åˆ¶éŸ³é¢‘æµåˆ°æ–°çš„è§†é¢‘é‡Œå¯ä¿å­˜åŸè§†é¢‘åŒç­‰çš„éŸ³é¢‘æµã€‚ 1-acodec copy è½¬æ¢è§†é¢‘åˆ°gifFFmpegè½¬æ¢è§†é¢‘åˆ°gifå¯ä½¿ç”¨ä¸‹è¿°å‘½ä»¤ 12345#æŠŠè§†é¢‘çš„å‰ 30 å¸§è½¬æ¢æˆä¸€ä¸ª Gifffmpeg -i in.mp4 -vframes 30 -f gif out.gif#å°†è§†é¢‘è½¬æˆ gif å°†è¾“å…¥çš„æ–‡ä»¶ä» (-ss) è®¾å®šçš„æ—¶é—´å¼€å§‹ä»¥ 10 å¸§é¢‘ç‡ï¼Œè¾“å‡ºåˆ° 320x240 å¤§å°çš„ gif ä¸­ï¼Œæ—¶é—´é•¿åº¦ä¸º -t è®¾å®šçš„å‚æ•°ã€‚ffmpeg -ss 00:00:00.000 -i in.mp4 -pix_fmt rgb24 -r 10 -s 320x240 -t 00:00:10.000 out.gif è¿›é˜¶ä½¿ç”¨éŸ³è§†é¢‘ç¼–ç è½¬æ¢-vcodec å¯ä»¥ç”¨æ¥é€‰æ‹©ä½ ç´¢è¦ä½¿ç”¨çš„ç¼–ç å™¨(å¦‚h264&#x2F;hevc&#x2F;mpeg4)ï¼Œä¾‹å¦‚ï¼š 123ffmpeg -i in.mp4 -vcodec h264 out.mp4ffmpeg -i in.mp4 -vcodec hevc out.mp4ffmpeg -i in.mp4 -vcodec mpeg4 out.mp4 é¢å¤–çš„é€‰é¡¹ï¼š-s æŒ‡å®šåˆ†è¾¨ç‡ï¼Œ-b æŒ‡å®šæ¯”ç‰¹ç‡ï¼Œ-r æŒ‡å®šå¸§ç‡ï¼Œ-acodec æŒ‡å®šéŸ³é¢‘ç¼–ç ï¼Œ-ab æŒ‡å®šéŸ³é¢‘æ¯”ç‰¹ç‡ï¼Œ-ac æŒ‡å®šå£°é“æ•°ï¼Œä¾‹å¦‚ï¼š 1ffmpeg -i in.mp4 -s 1920x1080 -b 200k -vcodec h264 -r 60 -acodec libfaac -ab 48k -ac 2 out.mp4 è½¬æ¢å°è£…ä¿ç•™ç¼–ç å’Œå…¶ä»–é€‰é¡¹(å¦‚mkvæˆ–flvè§£å°è£…åé‡æ–°å°è£…ä¸ºmp4)ï¼Œä¾‹å¦‚ï¼š 12ffmpeg -i in.mkv -vcodec copy -acodec copy out.mp4ffmpeg -i in.flv -vcodec copy -acodec copy out.mp4 åˆå¹¶è§†é¢‘æˆ‘ä»¬ç»å¸¸ä¼šéœ€è¦å°†ä¸¤ä¸ªè§†é¢‘åˆå¹¶åˆ°ä¸€èµ·ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œåˆå¹¶ 1ffmpeg -i &quot;concat:1.ts|2.ts&quot; -acodec copy -vcodec copy -absf aac_adtstoasc output.mp4 æ›´æ”¹è§†é¢‘åˆ†è¾¨ç‡æˆ–æ¯”ä¾‹è§†é¢‘åˆ†è¾¨ç‡å¯ä»¥ä½¿ç”¨-sæ¥æŒ‡å®šï¼Œè§†é¢‘æ¯”ä¾‹å¯ä»¥ä½¿ç”¨-aspectæ¥æŒ‡å®šï¼Œä¾‹å¦‚ï¼š 12ffmpeg -i input.mp4 -s 1280x720 -acodec copy output.mp4ffmpeg -i input.mp4 -aspect 16:9 output.mp4 å‰ªè¾‘è§†é¢‘å’Œè£å‰ªè§†é¢‘ç”»é¢ä¸€äº›åŸºç¡€çš„å‰ªè¾‘è§†é¢‘å’Œç”»é¢è£å‰ªä¹Ÿå¯ä»¥é€šè¿‡FFmpegå®ç°-ssè¡¨ç¤ºå¼€å§‹çš„æ—¶é—´ï¼Œ-tè¡¨ç¤ºæ—¶é—´çš„é•¿åº¦ ä¾‹å¦‚ï¼š 12345#ä»30så¼€å§‹æˆªå–10ç§’çš„è§†é¢‘å¹¶å°è£…è¿›h264ï¼Œaacç¼–ç çš„out.mp4é‡Œffmpeg -i in.mp4 -ss 00:00:30 -t 00:00:10 -acodec aac -vcodec h264 -acodec aac out.mp4#å°†1920x1080çš„è§†é¢‘æˆªå–ä¸­é—´çš„1080x1080éƒ¨åˆ†ï¼Œcropçš„å‚æ•°é€‰æ‹©ä¸ºwidth:height:x:yï¼Œwidth:heightä¸ºè£å‰ªåçš„è§†é¢‘åˆ†è¾¨ç‡ï¼Œx:yä¸ºè£å‰ªå‡ºæ¥çš„å·¦ä¸Šè§’çš„ç‚¹çš„åæ ‡ï¼Œæ•…æœ¬è§†é¢‘éœ€è¦ä¸ºxè½´(1920-1080)/2=420ï¼Œyè½´ä¸å˜æ•…ç”¨0å ä½ã€‚ffmpeg -i in.mp4 -vf crop=1080:1080:420:0 -acodec aac out.mp4 æå–(å»é™¤)è§†é¢‘ä¸­çš„è§†é¢‘(æˆ–éŸ³é¢‘)-an ä¸ºå»é™¤éŸ³é¢‘ï¼Œ-vn ä¸ºå»é™¤è§†é¢‘ï¼Œä¾‹å¦‚ï¼š 12345#å»é™¤è§†é¢‘ä¸­çš„éŸ³é¢‘(æå–è§†é¢‘)ffmpeg -i in.mp4 -vcodec copy -an out.mp4#å»é™¤è§†é¢‘ä¸­çš„è§†é¢‘(æå–éŸ³é¢‘)ffmpeg -i in.mp4 -acodec copy -vn out.mp4 åˆå¹¶éŸ³è§†é¢‘æœ¬æ“ä½œç­‰åŒäºå°†çº¯è§†é¢‘(æ— éŸ³é¢‘)çš„è§†é¢‘é‡Œçš„è§†é¢‘æµå’Œå•ç‹¬çš„éŸ³é¢‘æ–‡ä»¶é‡Œçš„éŸ³é¢‘æµè¿›è¡Œåˆå¹¶ï¼Œä¾‹å¦‚ï¼š 1ffmpeg â€“i in.mp4 â€“i in.mp3 â€“vcodec copy â€“acodec copy out.mp4 æ—‹è½¬è§†é¢‘å°†è§†é¢‘æŒ‰ç…§å¼§åº¦åˆ¶è¿›è¡Œæ—‹è½¬ï¼Œä½¿ç”¨-vf rotate&#x3D;å‚æ•°ï¼Œä¾‹å¦‚ï¼š 12#å°†è§†é¢‘æ—‹è½¬90åº¦ffmpeg -i in.mp4 -vf rotate=PI/2 out.mp4 è§†é¢‘(éŸ³é¢‘)å˜é€Ÿè§†é¢‘å˜é€Ÿä½¿ç”¨-filter:v setpts&#x3D;å‚æ•°ï¼ŒéŸ³é¢‘å˜é€Ÿä½¿ç”¨-filter:a atempo&#x3D;å‚æ•°ï¼Œä¾‹å¦‚ï¼š 12345#å°†è§†é¢‘è°ƒæ•´ä¸º0.5å€é€Ÿffmpeg -i in.mp4 -filter:v setpts=0.5*PTS out.mp4#éŸ³é¢‘å˜é€Ÿä¸ºåŸå…ˆçš„ä¸¤å€ffmpeg -i in.mp3 -filter:a atempo=2.0 out.mp3 æ€»ç»“FFmpegæ˜¯ä¸€ä¸ªéå¸¸å‰å®³çš„æ ¼å¼è½¬åŒ–ä¸å‹åˆ¶çš„è½¯ä»¶ï¼Œè™½ç„¶æ²¡æœ‰GUIï¼Œä½†æ˜¯åªè¦æŒæ¡äº†å‡ ä¸ªåŸºæœ¬çš„å‘½ä»¤å°±è¶³ä»¥å®Œæˆç»å¤§å¤šæ•°äººçš„ä½¿ç”¨éœ€æ±‚ï¼ŒWindowsã€macOSã€Linuxå…¨å¹³å°è¯•ç”¨ã€‚è€Œä¸”ç”±äºFFmpegæ˜¯ä¸€ä¸ªå¼€æºè½¯ä»¶ï¼Œæ‰€ä»¥ä½ å¯ä»¥æ ¹æ®ä½ çš„ä¸ªæ€§åŒ–éœ€æ±‚å¯¹è¯¥è½¯ä»¶è¿›è¡Œå®šåˆ¶ã€‚åŒæ ·å¦‚æœä½ æœ‰æ›´å¤šçš„ä½¿ç”¨éœ€æ±‚å¯ä»¥å»æŸ¥é˜…FFmpegçš„å®˜æ–¹æ–‡æ¡£é€‰æ‹©ä½ æ‰€éœ€è¦çš„å‚æ•°ã€‚å¦‚æœå¯¹æœ¬æ–‡ä¸­æœ‰ä»»ä½•å»ºè®®æˆ–è€…é—®é¢˜æ¬¢è¿åœ¨ä¸‹æ–¹è¯„è®ºåŒºç•™è¨€~","categories":[{"name":"è½¯ä»¶","slug":"è½¯ä»¶","permalink":"https://www.zl-asica.com/categories/%E8%BD%AF%E4%BB%B6/"}],"tags":[{"name":"ffmpeg","slug":"ffmpeg","permalink":"https://www.zl-asica.com/tags/ffmpeg/"}]},{"title":"WordPressï¼Œä½ å¥½ï¼","slug":"hello-wordpress","date":"2020-02-29T16:00:00.000Z","updated":"2024-02-13T06:37:05.553Z","comments":true,"path":"2020/hello-wordpress/","permalink":"https://www.zl-asica.com/2020/hello-wordpress/","excerpt":"Hexoï¼Ÿåœ¨æ¥åˆ°WordPressä»¥å‰ï¼Œæˆ‘çš„åšå®¢ä½¿ç”¨çš„æ˜¯Hexoä½œä¸ºé™æ€é¡µé¢ç”Ÿæˆå™¨ï¼Œä¸»é¢˜ä½¿ç”¨çš„æ˜¯nexmoeä¸»é¢˜ï¼Œå¹¶åšäº†ç¨å¾®æ”¹åŠ¨ï¼ŒWordPressç«™ https://www.zla.moeã€‚","text":"Hexoï¼Ÿåœ¨æ¥åˆ°WordPressä»¥å‰ï¼Œæˆ‘çš„åšå®¢ä½¿ç”¨çš„æ˜¯Hexoä½œä¸ºé™æ€é¡µé¢ç”Ÿæˆå™¨ï¼Œä¸»é¢˜ä½¿ç”¨çš„æ˜¯nexmoeä¸»é¢˜ï¼Œå¹¶åšäº†ç¨å¾®æ”¹åŠ¨ï¼ŒWordPressç«™ https://www.zla.moeã€‚ hexoæœ‰å¥½å¤„ä¹Ÿæœ‰åå¤„å¥½å¤„1.é¦–å…ˆä½¿ç”¨hexoçš„å¥½å¤„å°±æ˜¯ä¸ç”¨å­¦PHPï¼ˆWPçš„åŸºæœ¬ä½¿ç”¨ä¹Ÿæ˜¯ä¸éœ€è¦çš„ï¼Œæ­£å¸¸çš„htmlå’Œcssè¶³å¤Ÿäº†ï¼‰ 2.å…¶æ¬¡å°±æ˜¯çº¯é™æ€ç½‘ç«™å¯¹æœåŠ¡å™¨èµ„æºçš„å ç”¨ï¼Œéå¸¸çš„å°ï¼Œè€ŒWPä½¿ç”¨çš„PHPå’Œæ•°æ®åº“ä¼šå¯¹æœåŠ¡å™¨äº§ç”Ÿä¸€å®šçš„å‹åŠ›ã€‚ 3.WPç¨³å®šæ€§ä¹Ÿä¸å¦‚çº¯é™æ€ç½‘ç«™ï¼Œå³ä½¿åœ¨æœ‰å®šæœŸå¤‡ä»½çš„æƒ…å†µä¸‹ä¹Ÿæ— æ³•ä¿è¯å…¨éƒ¨æ•°æ®ä¸ä¸¢å¤±ï¼Œè€Œhexoç”±äºéƒ½æ˜¯ä»æœ¬åœ°ä¸Šä¼ ä¸Šå»çš„ï¼Œæ‰€ä»¥æœ¬åœ°è‡³å°‘æœ‰ä¸€ä»½å¤‡ä»½ï¼Œæ•°æ®ä¸¢å¤±çš„å¯èƒ½æ€§æä½ã€‚ 4.hexoåŸç”Ÿæ”¯æŒmarkdownè¯­æ³•ï¼Œwordpresséœ€è¦å®‰è£…æ’ä»¶åæ‰èƒ½å¤Ÿä½¿ç”¨ï¼ˆæ‰€æœ‰æ–‡ç« éƒ½ä¼šä»¥è¿™æ ·çš„å½¢å¼åœ¨ä¸¤ä¸ªç«™ç‚¹å‘å¸ƒï¼‰ã€‚ åå¤„1.é¦–å…ˆå°±æ˜¯æ–‡ç« ç®¡ç†ï¼Œç›¸å¯¹äºWPéå¸¸çš„å¤æ‚ï¼Œå¾ˆä¸èˆ’æœã€‚ 2.è·Ÿåœ¨æ–‡ç« åé¢çš„å°±æ˜¯è¯„è®ºçš„ç®¡ç†äº†ï¼Œç”±äºç½‘ç«™æ­å»ºåœ¨å¢ƒå†…éœ€è¦å¯¹è¯„è®ºæœ‰ä¸€å®šçš„é™åˆ¶ï¼Œè€Œhexoæˆ‘ä½¿ç”¨çš„æ˜¯valineï¼Œç®¡ç†èµ·æ¥è¿˜ç®—æ˜¯æ¯”è¾ƒèˆ’æœçš„ï¼Œä½†æ˜¯è¿˜æ˜¯å’ŒWPçš„æ²¡æœ‰å¯æ¯”æ€§ã€‚ 3.å¦‚æœæƒ³è¦è‡ªå·±å®šåˆ¶ä¸»é¢˜çš„è¯ï¼Œå°±éœ€è¦å­¦ä¹ ä¸€ä¸‹ejså’Œstyleï¼Œè‡ªå·±å®šåˆ¶ä¸€ä¸ªä¸»é¢˜å‡ºæ¥ã€‚å½“ç„¶åœ¨ejsä¸­æ˜¯å¯ä»¥ç›´æ¥ä½¿ç”¨htmlä»£ç çš„ï¼Œä½†æ˜¯æœ‰ä¸€ä¸ªé—®é¢˜å°±æ˜¯ä½ çš„htmlä»£ç æ˜¯å¦èƒ½å¤Ÿè®©ä¸»é¢˜å®Œç¾çš„é€‚é…ï¼Œè¿™æ˜¯ä¸ªå¾ˆå¤§çš„é—®é¢˜ã€‚åœ¨wordpressé‡Œç”±äºä¸»é¢˜éå¸¸å¤šï¼ŒåŸºæœ¬ä¸Šå¯ä»¥æ‰¾åˆ°æ¯”è¾ƒé€‚åˆè‡ªå·±çš„ä¸»é¢˜ã€‚å¦‚è‹¥é¢œè‰²æˆ–æŸäº›ç»†èŠ‚æ ·å¼è§‰å¾—ä¸é€‚åˆï¼Œå¯ä»¥ä½¿ç”¨csså¯¹ä¸»é¢˜è¿›è¡Œä¸€å®šçš„ä¿®é¥°ï¼Œä½¿å…¶æ›´åŠ ç¾åŒ–ã€‚ 4.SEOä¼˜åŒ–éå¸¸çš„å¤æ‚ï¼Œåœ¨WPä¸­å¯ä»¥ä½¿ç”¨å„ç§æ’ä»¶æ¥å¸®åŠ©ä½ åšä¼˜åŒ–ï¼Œä¹Ÿå¯ä»¥è‡ªå®šä¹‰ä¸€äº›metaä¹‹ç±»çš„ 5.so on~(æ¬¢è¿åœ¨è¯„è®ºåŒºå¢åŠ ä¸€ä¸‹) æŠ•å…¥Wordpressçš„æ€€æŠ±wordpresså§ï¼Œç°åœ¨æˆ‘ä½¿ç”¨çš„ä¸»é¢˜ä¸ºSakuraï¼Œä½œè€…æ˜¯Mashiroï¼Œå¯ä»¥ç‚¹å‰é¢çš„è¶…é“¾æ¥å»çœ‹çœ‹ç›¸å…³çš„ä¸»é¢˜ä½œè€…çš„åšå®¢å’Œä¸»é¢˜ä»‹ç»é¡µé¢ã€‚ä¹‹åæˆ‘ä¼šå¯¹WPçš„ä¸»é¢˜è¿›è¡Œä¸€å®šçš„å®šåˆ¶ï¼ŒåŸºæœ¬ä¸Šå°±æ˜¯åŠ ä¸€äº›å°ç©æ„æ”¹æ”¹æ ·å¼ï¼Œè¿˜æ•¬è¯·æœŸå¾…å’¯~ å¦‚æœä½ å¯¹hexoå’Œwpæœ‰ä¸€äº›å’Œæˆ‘ä¸ä¸€æ ·çš„è§è§£è¿˜æ¬¢è¿åœ¨ä¸‹æ–¹è¯„è®ºåŒºç•™è¨€å“¦~~~","categories":[{"name":"éšæƒ³","slug":"éšæƒ³","permalink":"https://www.zl-asica.com/categories/%E9%9A%8F%E6%83%B3/"}],"tags":[{"name":"WordPress","slug":"WordPress","permalink":"https://www.zl-asica.com/tags/WordPress/"}]},{"title":"æ ‘è“æ´¾å®‰è£…qBittorrent-noxå®ç°ç§å­æœåŠ¡å™¨è‡ªåŠ¨ä¸‹è½½ä¸Šä¼ ","slug":"raspberry-pie-qbittorrent-nox","date":"2019-11-19T01:21:54.000Z","updated":"2024-02-13T06:37:05.553Z","comments":true,"path":"2019/raspberry-pie-qbittorrent-nox/","permalink":"https://www.zl-asica.com/2019/raspberry-pie-qbittorrent-nox/","excerpt":"å‰è¨€&nbsp;&nbsp;æˆ‘ä¹°å›å®¶æ ‘è“æ´¾ 3B+ä»¥åï¼Œå°±ä¸€ç›´æ²¡æœ‰æŠŠå®ƒå……åˆ†çš„åˆ©ç”¨èµ·æ¥ï¼Œä»…ä»…åšäº†ä¸€ä¸ªæ— çº¿æ‰“å°æœºè¿˜æœ‰ä¸€ä¸ªå†…ç½‘è®¿é—®çš„ç½‘ç«™ã€‚è¿™æ®µæ—¶é—´æ­£å¥½æ¥è§¦åˆ°äº†ç§å­ï¼Œæ„Ÿè§‰æ‹¿æ¥åšä¸€ä¸ªç§å­çš„æœåŠ¡å™¨è¿˜æ˜¯ä¸é”™çš„ã€‚","text":"å‰è¨€&nbsp;&nbsp;æˆ‘ä¹°å›å®¶æ ‘è“æ´¾ 3B+ä»¥åï¼Œå°±ä¸€ç›´æ²¡æœ‰æŠŠå®ƒå……åˆ†çš„åˆ©ç”¨èµ·æ¥ï¼Œä»…ä»…åšäº†ä¸€ä¸ªæ— çº¿æ‰“å°æœºè¿˜æœ‰ä¸€ä¸ªå†…ç½‘è®¿é—®çš„ç½‘ç«™ã€‚è¿™æ®µæ—¶é—´æ­£å¥½æ¥è§¦åˆ°äº†ç§å­ï¼Œæ„Ÿè§‰æ‹¿æ¥åšä¸€ä¸ªç§å­çš„æœåŠ¡å™¨è¿˜æ˜¯ä¸é”™çš„ã€‚ å¹³å°ä»‹ç»&nbsp;&nbsp;æ ‘è“æ´¾ 3b+ã€ä¸€å¼ æ’åœ¨æ ‘è“æ´¾ä¸Šä½œä¸ºç³»ç»Ÿç›˜çš„é‡‘å£«é¡¿ 64GB çš„ tf å¡ã€å›ºå®šçš„å…¬ç½‘ ipã€é•¿æ—¶é—´é€šç”µä»¥åŠèƒ½å¤Ÿç¨³å®šçš„ç½‘é€Ÿã€‚&nbsp;&nbsp;ç³»ç»Ÿç‰ˆæœ¬ï¼šRaspbian GNU&#x2F;Linux 10 (buster)&nbsp;&nbsp;è½¯ä»¶ï¼šqBittorrent-nox&nbsp;&nbsp;ç½‘ç»œç¯å¢ƒï¼šå±±ä¸œè”é€š å‡†å¤‡å·¥ä½œ&nbsp;&nbsp;é¦–å…ˆä½ éœ€è¦å›ºå®šä½ çš„å…¬ç½‘ IPï¼Œè¿™æ˜¯ä¸ºäº†ç¨³å®šä½ çš„ç§å­ä¸‹è½½å’Œä¸Šä¼ çš„é€Ÿåº¦ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ–¹ä¾¿ä½ åœ¨å¤–ç½‘å¯¹ä½ çš„æ ‘è“æ´¾çš„ç§å­ä¸‹è½½ä¸Šä¼ è¿›è¡Œç®¡ç†ã€‚å¦‚æœä½ ä¸çŸ¥é“å…¬ç½‘ ip ä¸ºä½•ç‰©äº¦æˆ–æ˜¯ä¸çŸ¥é“å¦‚ä½•å›ºå®šå…¬ç½‘ ip è¯·è‡ªè¡ŒæŸ¥é˜…ï¼Œæœ¬æ–‡å°†ä¸å¤šèµ˜è¿°ã€‚&nbsp;&nbsp;è¦åœ¨ Linux ä¸Šä½¿ç”¨ qBittorrent Web UIï¼Œä½ æ— éœ€å®‰è£…å®Œæ•´çš„ qBittorent æ¡Œé¢åº”ç”¨ç¨‹åºï¼Œæœ‰ä¸€ä¸ªåŸºäºç»ˆç«¯çš„ qBittorrent åº”ç”¨ç¨‹åºå¯ç”¨ï¼Œå®ƒè¢«ç§°ä¸º qBittorrent-Noxã€‚&nbsp;&nbsp;æ³¨æ„ï¼šWeb UI åŠŸèƒ½ä¸ä»…é™äº qBittorrent-mox åº”ç”¨ç¨‹åºï¼Œæ­¤åŠŸèƒ½è¿˜å¯ä»¥ä¸ä¼ ç»Ÿçš„ qBittorent Linux æ¡Œé¢åº”ç”¨ç¨‹åºä¸€èµ·ä½¿ç”¨ï¼Œè¯¥åº”ç”¨ç¨‹åºå¯é€šè¿‡ Flapak å®‰è£…ã€‚ å®‰è£…è¿‡ç¨‹å®‰è£… qBittorrent-nox&nbsp;&nbsp;åœ¨æ ‘è“æ´¾ä¸Šè·å– qBittorrent-nox éå¸¸ç®€å•ï¼Œå› ä¸ºå®ƒä½äºâ€œMainâ€è½¯ä»¶å­˜å‚¨åº“ä¸­ï¼Œä½†æ˜¯ï¼Œç”±äºæ“ä½œç³»ç»Ÿçš„è½¯ä»¶æ›´æ–°æ–¹å¼ï¼Œmain ä¸­çš„ç‰ˆæœ¬å¯èƒ½ä¼šç•¥å¾®è¿‡æ—¶ï¼Œè¦å®‰è£…å®ƒï¼Œè¯·åœ¨ä¸‹é¢è¾“å…¥ apt-get å‘½ä»¤ï¼š 1sudo apt-get install qbittorrent-nox ä½¿ç”¨æ–¹æ³•å¼€å¯ qBittorrent-nox æœåŠ¡å¹¶æ˜ å°„ 8070 ç«¯å£&nbsp;&nbsp;è¯·ç›´æ¥è¾“å…¥ä»¥ä¸‹å‘½ä»¤å¹¶å›è½¦ã€‚å…¶ä¸­ 8070 ä¸ºæ˜ å°„çš„ç«¯å£ï¼Œ-d ä¸ºåå°è¿è¡Œ 1qbittorrent-nox --webui-port=8070 -d è‡³äºä¸ºä½•ä½¿ç”¨ 8070 ç«¯å£è€Œä¸ä½¿ç”¨é»˜è®¤çš„ 8080 ç«¯å£åŸå› å¾ˆç®€å•ï¼Œå›½å†…è¿è¥å•†å°ç¦äº†å®¶åº­å®½å¸¦å¤–ç½‘çš„ 8080 ç«¯å£ï¼Œæ•…æœ¬æ•™ç¨‹ä½¿ç”¨ 8070 ä½œä¸ºäº†æ˜ å°„çš„ç«¯å£ä»¥ä¾¿èƒ½å¤Ÿåœ¨å¤–ç½‘è®¿é—®ï¼Œå¯ä»¥è‡ªå·±éšæ„ç®—åˆ™ç«¯å£ï¼Œæœ¬æ•™ç¨‹åé¢çš„ 8070 å…¨éƒ¨æ›´æ”¹ä¸ºä½ è‡ªå·±çš„ç«¯å£å³å¯ é˜²ç«å¢™æ”¾è¡Œ 8070 ç«¯å£åœ¨ç»ˆç«¯è¾“å…¥ä»¥ä¸‹å‘½ä»¤ï¼Œå›è½¦è¿è¡Œ 1sudo iptables -I INPUT -p tcp --dport 8070 -j ACCEPT å¦‚æœç»ˆç«¯è¿”å›-bash: iptables: command not foundè¯·æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£… iptables åå†é‡æ–°æ‰§è¡Œä¸Šè¿°å‘½ä»¤ 1sudo apt-get update &amp;&amp; apt-get install iptables æ”¾è¡Œç«¯å£åè¯·æ‰§è¡Œä¸‹é¢çš„å‘½ä»¤ä»¥ä¿å­˜æ”¾è¡Œè§„åˆ™ 1sudo iptables-save è®¾ç½®å®Œå°±å·²ç»æ”¾è¡Œäº†æŒ‡å®šçš„ç«¯å£ï¼Œä½†é‡å¯åä¼šå¤±æ•ˆï¼Œä¸‹é¢è®¾ç½®æŒç»­ç”Ÿæ•ˆè§„åˆ™ï¼›å®‰è£…iptables-persistent 1sudo apt-get install iptables-persistent æ‰§è¡Œä¸‹è¿°å‘½ä»¤ä¿å­˜è§„åˆ™æŒç»­ç”Ÿæ•ˆ 12netfilter-persistent savenetfilter-persistent reload å°† qBt-nox åŠ å…¥å¼€æœºè‡ªå¯é¡¹è¾“å…¥ä»¥ä¸‹å‘½ä»¤æ‰“å¼€ rc.local æ–‡ä»¶ä»¥ç¼–è¾‘ 1sudo nano /etc/rc.local åœ¨ exit 0 å‰åŠ å…¥ 1sudo qbittorrent-nox --webui-port=8070 -d è¾“å…¥å®Œæˆåæ‘ control+X å†æ‘ä¸€ä¸‹ Y å†æ‘ä¸€ä¸‹å›è½¦å³å¯å®Œæˆä¿å­˜è®¾ç½®å®Œæˆå¼€æœºè‡ªå¯ æ‰“å¼€ qBt-nox çš„ webUI å¹¶æ›´æ”¹é»˜è®¤è´¦æˆ·ä¸å¯†ç æ‰“å¼€ä»»æ„æµè§ˆå™¨ï¼Œåœ¨æµè§ˆå™¨åœ°å€æ å†…è¾“å…¥ http:&#x2F;&#x2F;ä½ çš„å…¬ç½‘IP:8070 å›è½¦ é»˜è®¤ç”¨æˆ·åä¸º admin å¯†ç ä¸º adminadmin ç‚¹å‡»ä¸Šæ–¹å·¥å…·æ çš„ tool-Options-WebUI åœ¨ä¸Šæ–¹å¯ä»¥æ›´æ”¹è¯­è¨€ä¸ºç®€ä½“ä¸­æ–‡ï¼Œæ›´æ”¹åä¼šé‡æ–°è¿›å…¥ qBt çš„ WebUI ä¸‹æ–¹å¯ä»¥æ›´æ”¹é»˜è®¤çš„è´¦æˆ·å’Œå¯†ç ï¼Œå¼ºçƒˆå»ºè®®æ›´æ”¹ï¼Œè¯·å‹¿ä½¿ç”¨é»˜è®¤è®¾ç½®ï¼ï¼ï¼ è‡³æ­¤ï¼Œæ ‘è“æ´¾å®‰è£… qBittorrent-nox å®ç°ç§å­æœåŠ¡å™¨è‡ªåŠ¨ä¸‹è½½ä¸Šä¼ æ•™ç¨‹å·²å…¨éƒ¨ç»“æŸï¼ŒqBittorrent-nox çš„ä½¿ç”¨æ–¹å¼ä¸ PC ç«¯çš„æ–¹æ³•ä¸€è‡´ã€‚","categories":[{"name":"è½¯ä»¶","slug":"è½¯ä»¶","permalink":"https://www.zl-asica.com/categories/%E8%BD%AF%E4%BB%B6/"}],"tags":[{"name":"æ ‘è“æ´¾","slug":"æ ‘è“æ´¾","permalink":"https://www.zl-asica.com/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"}]},{"title":"Microsoft Office å›¾æ ‡æ¢æ–°-Officeå›¾æ ‡è¿­ä»£å²","slug":"Microsoft-Office-å›¾æ ‡æ¢æ–°-Officeå›¾æ ‡è¿­ä»£å²","date":"2019-10-14T01:00:00.000Z","updated":"2024-02-13T06:37:05.553Z","comments":true,"path":"2019/microsoft-office-tu-biao-huan-xin-office-tu-biao-die-dai-shi/","permalink":"https://www.zl-asica.com/2019/microsoft-office-tu-biao-huan-xin-office-tu-biao-die-dai-shi/","excerpt":"å‰è¨€&nbsp;&nbsp;å¦‚æœä½ å®‰è£…çš„æ˜¯ Office 2019 æˆ–è€… Office 365ï¼Œé‚£ä¹ˆä¸€å®šå‘ç°ï¼Œä½ çš„ Office å›¾æ ‡â€¦æ¢â€¦æ–°â€¦äº†â€¦ï¼å’Œæ—§ç‰ˆç›¸æ¯”ï¼Œæ–°ç‰ˆ Office å›¾æ ‡å»¶ç»­äº†æ‰å¹³ç”»é£ï¼Œä½†ç°ä»£æ„Ÿæ›´å¼ºã€‚å¾®ç«‹ä½“ã€æ¸å˜è‰²è¿™äº› 2019 å¹´æµè¡Œçš„å…ƒç´ ï¼Œå‡ ä¹éƒ½å¯ä»¥åœ¨è¿™å¥—æ–°å›¾æ ‡ä¸­æ‰¾åˆ°ã€‚","text":"å‰è¨€&nbsp;&nbsp;å¦‚æœä½ å®‰è£…çš„æ˜¯ Office 2019 æˆ–è€… Office 365ï¼Œé‚£ä¹ˆä¸€å®šå‘ç°ï¼Œä½ çš„ Office å›¾æ ‡â€¦æ¢â€¦æ–°â€¦äº†â€¦ï¼å’Œæ—§ç‰ˆç›¸æ¯”ï¼Œæ–°ç‰ˆ Office å›¾æ ‡å»¶ç»­äº†æ‰å¹³ç”»é£ï¼Œä½†ç°ä»£æ„Ÿæ›´å¼ºã€‚å¾®ç«‹ä½“ã€æ¸å˜è‰²è¿™äº› 2019 å¹´æµè¡Œçš„å…ƒç´ ï¼Œå‡ ä¹éƒ½å¯ä»¥åœ¨è¿™å¥—æ–°å›¾æ ‡ä¸­æ‰¾åˆ°ã€‚ é“¾æ¥ï¼šå¾®è½¯å®˜æ–¹å•†åŸ-Office &nbsp;&nbsp;æœ‰äººè¿˜å‘ç° 2019 ç‰ˆçš„ office æ–°å›¾æ ‡é…åˆå…¨æ–°çš„ Windows 10 Light Theme æ—¶ï¼Œæ›´æ˜¯é€¼æ ¼æ»¡æ»¡ã€‚ä¸‹é¢ä¾¿æ˜¯ office æ¯ä»£çš„å‘å±•å²å’Œä»–ä»¬ä¸åŒçš„å›¾æ ‡ å‘å±•å²Office 2019ã€Office 365ï¼ˆæœ€æ–°ï¼‰&nbsp;&nbsp;æœ€æ–°ç‰ˆ Office 2019 å’Œ Office 365 ä½¿ç”¨äº†æ–°å›¾æ ‡ï¼Œæ•´ä½“é£æ ¼ç®€çº¦å¤§æ°”ï¼Œèåˆäº†å¾®ç«‹ä½“ã€æ¸å˜è‰²ç­‰å¾ˆå¤šæ—¶ä¸‹æµè¡Œå…ƒç´ ï¼Œå†é…åˆ Windows 10 çš„ Light Theme æ›´æ˜¾é€¼æ ¼æ»¡æ»¡ã€‚æ­£ç‰ˆç”¨æˆ·ç°åœ¨å·²ç»å¯ä»¥æ›´æ–°ä½¿ç”¨äº†ï¼Œå¿«æ¥æ›´æ–°å¢åŠ é€¼æ ¼å§ï¼ Office 2019ï¼ˆæ—©æœŸï¼‰&nbsp;&nbsp;Office 2019 æœ€æ—©å‘å¸ƒäº 2018 å¹´ï¼Œåˆä»£ä½¿ç”¨ Office 2016 çš„å›¾æ ‡è®¾è®¡ï¼Œè¿™å¥—å›¾æ ‡å…¶å®æ—©åœ¨ 2013 å¹´å³å¼€å§‹å®šå‹ï¼Œ 16 ç‰ˆåˆ™å¯¹å®ƒè¿›è¡Œäº†å°å¹…æ”¹åŠ¨ã€‚è™½ç„¶ä¹Ÿæ˜¯æ‰å¹³åŒ–è®¾è®¡ï¼Œä½†å†æ—¶ 7 å¹´æ—¶é—´ï¼Œæ—©å·²è®©è¿™å¥—å›¾æ ‡å€æ˜¾è€æ°”ã€‚2018 å¹´åº•ï¼Œå¾®è½¯å®£å¸ƒå°†æ›´æ–° Office å›¾æ ‡ï¼ŒOffice å¼€å§‹èµ°ä¸Šæ–°è·¯é€”ã€‚ Office 2016&nbsp;&nbsp;å‘å¸ƒäº 2015 å¹´çš„ Office 2016ï¼Œå»¶ç»­äº† 13 ç‰ˆè®¾è®¡é£æ ¼ï¼Œåªæ˜¯åœ¨å…¶åŸºç¡€ä¸Šåšäº†ä¸€ç‚¹ç‚¹å°å¹…ä¼˜åŒ–ï¼Œçœ‹èµ·æ¥æ›´åŠ æ˜æ˜¾ã€‚å…¶å®è¿™ä¸ªç‰ˆæœ¬ä¹Ÿæ˜¯å¾ˆå¤šç½‘å‹åœ¨ç”¨çš„ç‰ˆæœ¬ã€‚ Office 2013&nbsp;&nbsp;Office 2013 å‘å¸ƒäº 2012 å¹´ï¼Œæ­£å€¼å¾®è½¯é¦–æ¬¾è·¨å¹³å°æ“ä½œç³»ç»Ÿ Win8 äº®ç›¸ã€‚æ•´ä½“é£æ ¼åå‘äºæ‰å¹³åŒ–ï¼Œæ°å¥½ä¸ Win8 çš„è¯‰æ±‚ç›¸äº’å¥‘åˆã€‚Office 2013 æ˜¯ä¸€ä¸ªè·¨æ—¶ä»£ç‰ˆæœ¬ï¼Œå®ƒçš„å¾ˆå¤šè®¾è®¡æœ€ç»ˆä¹Ÿå½±å“åˆ°åç»­äº§å“çš„å‘å±•ã€‚åŒ…æ‹¬ Office 2016ã€Office 2019 æ—©æœŸç‰ˆï¼Œæ— è®ºæ˜¯åŠŸèƒ½è¿˜æ˜¯å›¾æ ‡è®¾è®¡ï¼Œå‡ ä¹éƒ½æ˜¯åœ¨ 13 ç‰ˆçš„åŸºç¡€ä¸Šè¿›è¡Œçš„å°å¹…ä¼˜åŒ–ã€‚è¿˜æœ‰ä¸€ç‚¹å°±æ˜¯ï¼Œä»è¿™ä¸ªç‰ˆæœ¬å¼€å§‹ Office å¼€å§‹å¯ç”¨æ–° LOGOï¼Œä¸å†æ˜¯æ²¿ç”¨å¤šå¹´çš„å››å¶è‰å›¾æ ‡ã€‚ Office 2010&nbsp;&nbsp;Office 2010 å‘å¸ƒäº 2009 å¹´ï¼Œæ­¤æ—¶çš„å¾®è½¯å·²ç»è¢« Vista çš„é¢“åŠ¿æå¾—ç„¦å¤´çƒ‚é¢ï¼ŒåŒå¹´å‘å¸ƒçš„ Office 2010 è‡ªç„¶ä¹Ÿå»¶ç»­äº†å¾ˆå¤šå‰è¾ˆ Office 2007 çš„è®¾è®¡ï¼Œå›¾æ ‡æ˜¯åœ¨åŸæœ‰é£æ ¼åŸºç¡€ä¸Šåšäº†ä¸€äº›å°å¹…ä¼˜åŒ–ï¼Œé€šè¿‡åŠ å¤§é¦–å­—æ¯å…ƒç´ ï¼Œçªå‡ºå„ä¸ªæ¨¡å—çš„åŠŸèƒ½å®šä½ã€‚æ€»ä¹‹ä½ ä¹Ÿå¯ä»¥æŠŠå®ƒçœ‹ä½œæ˜¯ Office 2007 çš„ä¸€ä¸ªç¿»ç‰ˆï¼Œåªä¸è¿‡çœ‹ä¸Šå»æ›´åŠ åä¸½ã€‚ Office 2007&nbsp;&nbsp;Office 2007 æ˜¯ä¸€ä¸ªè·¨æ—¶ä»£ç‰ˆæœ¬ï¼Œä»è¿™ä¸€ç‰ˆæœ¬å¼€å§‹ï¼Œå¾®è½¯é¦–æ¬¡é‡‡ç”¨ RIBBON ä½œä¸ºè½¯ä»¶ç•Œé¢ã€‚ä»å›¾æ ‡ä¸Šè¯´ï¼ŒOffice 2007 çš„è®¾è®¡ä¹Ÿå’Œå‰ç‰ˆé£æ ¼è¿¥å¼‚ï¼Œç»™äººäº†ä¸€ç§è€³ç›®ä¸€æ–°çš„æ„Ÿè§‰ã€‚æ›´åŠ æœ‰è¶£çš„æ˜¯ï¼Œå¾®è½¯è¿˜å°†è¿™ä¸€è®¾è®¡å¤åˆ¶åˆ°å®ä½“åŒ…è£…ç›’ä¸Šï¼ŒåŒæ ·çš„åœ†æ»‘é€ å‹ï¼ŒåŒæ ·çš„é¢œè‰²å¸ƒè®¾ï¼Œå‡ ä¹å¯ä»¥å ªç§°å½“å¹´æœ€â€œINâ€çš„åŠå…¬è½¯ä»¶ã€‚ Office 2003&nbsp;&nbsp;Office 2003 å¯è°“æ˜¯ Office ç³»åˆ—ä¸­çš„ç»å…¸ï¼Œä»¥è‡³äºè‹¥å¹²å¹´åçš„ä»Šå¤©ï¼Œä»ç„¶èƒ½åœ¨å¾ˆå¤šåœ°æ–¹è§åˆ°å®ƒã€‚è¿™å¥—å›¾æ ‡ç›¸æ¯”ä¹‹å‰å¢åŠ äº†æ¸å˜æ•ˆæœï¼Œé…åˆå…¨æ–°çš„è“è‰²ç•Œé¢ï¼Œæ—¶å°šæ„Ÿéå¸¸ä¸é”™ã€‚åŒæ—¶ä¹Ÿæ­£å› ä¸ºå…¶è¶…é«˜çš„æ›å…‰ç‡ï¼Œæˆä¸ºå—ä¼—äººæ•°æœ€å¹¿çš„ ICON ä¹‹ä¸€ã€‚æ­¤å¤–å››å¶è‰ LOGO åœ¨è¿™ä¸€ç‰ˆä¸­ä¹Ÿé¦–æ¬¡æ›¿æ¢æ‰æ‹¼å›¾ LOGOï¼Œæ—¢æ—¶å°šåˆæ›´èƒ½å‡¸æ˜¾æ–°ç‰ˆçš„æ„å›¾ã€‚ Office XP(2002 ç‰ˆ)&nbsp;&nbsp;å’Œå‰åä¸¤ç‰ˆç›¸æ¯”ï¼ŒOffice 2002 çŸ¥é“çš„äººå¾ˆå°‘ï¼Œä¸€æ–¹é¢æ˜¯å…¶åŠŸèƒ½å˜åŒ–ä¸å¤§ï¼Œå¦ä¸€æ–¹é¢ä¹Ÿæ°å·§æ’åœ¨äº† Office 2000 å’Œ Office 2003 è¿™ä¸¤ä¸ªå²ä¸Šæœ€ç»å…¸ Office ç‰ˆæœ¬ä¹‹é—´ã€‚è¿™ä¸€ç‰ˆæœ¬è¯ç”Ÿäº 2001 å¹´ï¼Œä¸è‘—åçš„ Windows XP åŒå¹´è€Œç”Ÿï¼Œå› æ­¤å‘½åä¸Šä¹Ÿç½•è§åœ°é‡‡ç”¨äº†â€œXPâ€ç§°å·ï¼Œè€Œéæ­¤å‰çš„å¹´ä»½å‘½åã€‚å’Œå¾ˆå¤šè¿‡æ¸¡ç‰ˆæœ¬ä¸€æ ·ï¼ŒOffice 2002 ä¹Ÿæ²¿ç”¨äº† Office 2000 çš„å›¾æ ‡é£æ ¼ï¼Œå¹¶éåœ¨ä¸Šé¢è¿›è¡Œä»»ä½•ä¿®æ”¹ï¼Œä¾æ—§æ˜¯çªå‡ºå„ç‰ˆå—é—´è‡ªèº«çš„å“ç‰Œå®šä½ã€‚ Office 2000&nbsp;&nbsp;Office 2000 å‘å¸ƒäº 1999 å¹´ï¼ŒåŒæ ·ä¹Ÿæ˜¯ Office å†å²ä¸Šå¾ˆè‘—åçš„ç‰ˆæœ¬ä¹‹ä¸€ï¼Œå’Œç»å…¸çš„ Office 97 ç›¸æ¯”ï¼ŒOffice 2000 åŠŸèƒ½å˜åŒ–ä¸å¤§ï¼Œä»…ä»…æ˜¯å¢åŠ äº†ä¸€äº›äººæ€§åŒ–æ”¹è¿›ã€‚ä½†ç”±äºæ€§èƒ½ä¸å‰ç‰ˆå·®åˆ«ä¸å¤šï¼Œåœ¨å®é™…ä½¿ç”¨ä¸­æ™®åŠç‡å¾ˆé«˜ã€‚å›¾æ ‡æ–¹é¢ï¼ŒOffice 2000 é‡‡ç”¨äº†ä¸ä»¥å¾€å®Œå…¨ä¸åŒçš„è®¾è®¡ç†å¿µï¼Œå°†å„ç‰ˆå—é¦–å­—æ¯ç»è¿‡è‰ºæœ¯å¤„ç†åï¼Œä½œä¸ºå¯¹åº”ç»„ä»¶çš„ LOGOã€‚è€Œè¿™ä¸€è®¾è®¡ä¹Ÿå¼•é¢†äº†åä»£ Office è®¾è®¡ç†å¿µï¼Œå¯è°“å¼€åˆ›äº†ä¸€ä¸ªå…ˆæ²³ã€‚ Office 97&nbsp;&nbsp;Office 97 è¯ç”Ÿäº 1996 å¹´ï¼Œå ªç§° Office å†å²ä¸Šç¬¬ä¸€ä¸ªç»å…¸ä¹‹ä½œã€‚æ•´ä½“è®¾è®¡æ²¿è¢­æ­¤å‰çš„ Office 95 è®¾è®¡ï¼Œå›¾æ ‡å¤§ä½“æ¨¡æ‹Ÿäº†ç»„ä»¶åŠŸèƒ½ï¼Œç”¨ç°åœ¨çš„è¯è¯´ï¼Œå°±æ˜¯çº¯ç²¹çš„æ‹Ÿç‰©åŒ–è®¾è®¡ã€‚è€Œ Office 97 ç›¸æ¯” Office 95 çš„æœ€å¤§çœ‹ç‚¹ï¼Œæ˜¯æ•´ä½“ç•Œé¢æ›´åŠ å…»çœ¼å’Œè€çœ‹ï¼Œè¿™åœ¨ 20 å¹´å¤šå‰ï¼Œè¿˜æ˜¯ç›¸å½“æœ‰å¸å¼•åŠ›çš„ã€‚ Office 95&nbsp;&nbsp;Office 95 æ˜¯é¦–æ¬¾ä»¥å¹´ä»£ä¸ºä»£å·çš„ Office è½¯ä»¶ï¼Œå¾ˆå¤šäººçŸ¥é“ Officeï¼Œå…¶å®å°±æ˜¯ä»è¿™ä¸€ç‰ˆå¼€å§‹çš„ã€‚Office 95 çš„è®¾è®¡å¸¦æœ‰æµ“éƒçš„ä¸Šä¸–çºªå‘³é“ï¼Œå‡ ä¹çœ‹ä¸åˆ°ä»»ä½•ç°ä»£ Office å¥—ä»¶çš„å½±å­ã€‚è€Œ Office 95+Windows 95ï¼Œå‡ ä¹æ˜¯é‚£ä¸ªæ—¶ä»£çš„æ ‡é…ä¹‹ä½œã€‚ Office â€œä¸Šå¤æ—¶ä»£â€&nbsp;&nbsp;å½“ç„¶ Office 95 ä¹Ÿä¸æ˜¯ Office å†å²ä¸Šç¬¬ä¸€ä¸ªç‰ˆæœ¬ï¼Œåœ¨æ­¤ä¹‹å‰ï¼Œè¿˜æœ‰ Office 1.0ã€2.0ã€3.0ã€4.0 å››ä¸ªç‰ˆæœ¬ã€‚ä¸è¿‡é‚£æ—¶çš„æ“ä½œç³»ç»Ÿè¿œæ²¡æœ‰ Win95 å¥½ç”¨ï¼ŒåŸºæœ¬ä¸Šå¯ä»¥çœ‹ä½œæ˜¯æŒ‚æ¥åœ¨ DOS ç³»ç»Ÿä¸Šçš„ä¸€å¥— GUI ç¯å¢ƒï¼Œè€Œé‚£æ—¶çš„ Office ä¹Ÿä»…é™äº Wordã€Excelã€PowerPoint ä¸‰å¤§ä»¶ã€‚ä»¥ç°åœ¨çš„çœ¼å…‰æ¥çœ‹ï¼Œç®€ç›´å¯ä»¥è¯´æ˜¯ä¸‘é™‹æ— æ¯”ï¼Œä½†åœ¨å½“æ—¶è¿™æ ·çš„è®¾è®¡å·²ç»å¯ä»¥ç®—æ˜¯åˆ’æ—¶ä»£äº§å“äº†ã€‚ Microsoft Office for Mac&nbsp;&nbsp;é™¤äº†ä¸Šé¢ä»‹ç»çš„è¿™äº›ä»¥å¤–ï¼Œæˆ‘ä»¬ä¹Ÿä¸èƒ½å¿½è§† Office çš„å¦ä¸€å¤§åˆ†æ”¯ Mac Officeã€‚ä»æœ€å¼€å§‹çš„ MAC Office 2001 åˆ°æœ€æ–°ç‰ˆ MAC Office 2019ï¼Œä½ ä¼šå‘ç°å®ƒåœ¨å›¾æ ‡é£æ ¼ä¸Šå˜åŒ–å¾ˆå¤§ã€‚æ—©æœŸçš„ MAC Office é‡‡ç”¨äº†ä¸ Windows ç‰ˆæœ¬å®Œå…¨ä¸åŒçš„è®¾è®¡ï¼Œç”šè‡³ä½ ä¼šè®¤ä¸ºè¿™æ˜¯ä¸¤æ¬¾å®Œå…¨ä¸åŒçš„è½¯ä»¶ï¼Œç›´åˆ° 2016 ç‰ˆå¼€å§‹ï¼ŒMAC Office æ‰æ­£å¼ä¸ Windows Office ä¿æŒç»Ÿä¸€ï¼Œä½†ä¾æ—§ä¼šæœ‰ç»†èŠ‚ä¸Šçš„å·®åˆ«ã€‚ Microsoft Office for iPad&nbsp;&nbsp;Office for iPad æ˜¯ Office365 çš„ä¸€ä¸ªé™„èµ çš„åŠŸèƒ½ï¼Œç”±äºå¾®è½¯çš„æ”¿ç­–æ˜¯ 10.1 è‹±å¯¸ä»¥ä¸Šè®¾å¤‡ä½¿ç”¨ office å‡æ— å…è´¹ç‰ˆï¼Œæ‰€ä»¥ç›®å‰å¸‚é¢ä¸Šé™¤äº† 9.7 å¯¸å¤–çš„æ‰€æœ‰ iPadd è®¾å¤‡å‡éœ€è´­ä¹° Office 365 æœåŠ¡æ‰èƒ½ä½¿ç”¨ Office çš„å…¨éƒ¨åŠŸèƒ½ï¼Œä¸ä»˜è´¹ä»…èƒ½ä½¿ç”¨æŸ¥çœ‹åŠŸèƒ½ã€‚ä½†æ˜¯å³ä¾¿å¦‚æ­¤ Office for iPad ä¹Ÿå¤§å¤§æé«˜äº† iPad çš„åŠå…¬èƒ½åŠ›ã€‚ å°ç»“çœ‹å®Œäº†æ•´ä¸ª Office å›¾æ ‡è¿›åŒ–å²ä¹‹åï¼Œä½ ä»¬éƒ½æœ‰å“ªäº›æ„Ÿæ…¨å‘¢ï¼Ÿé‚£ä¹ˆä½ ç¬¬ä¸€æ¬¡æ¥è§¦è¿‡çš„ Office åˆæ˜¯ä¸ªä»€ä¹ˆæ ·å­å‘¢ï¼Ÿæ¬¢è¿åœ¨ä¸‹æ–¹è¯„è®ºåŒºç•™è¨€ã€‚","categories":[{"name":"éšæƒ³","slug":"éšæƒ³","permalink":"https://www.zl-asica.com/categories/%E9%9A%8F%E6%83%B3/"}],"tags":[{"name":"Microsoft","slug":"Microsoft","permalink":"https://www.zl-asica.com/tags/Microsoft/"}]},{"title":"Mac+centOS7+XAMPP æ­å»ºdiscuz!è®ºå›","slug":"Mac+centOS7+XAMPP æ­å»ºdiscuz!è®ºå›","date":"2019-08-18T16:00:00.000Z","updated":"2024-02-13T06:37:05.553Z","comments":true,"path":"2019/mac-centos7-xampp-da-jian-discuz-lun-tan/","permalink":"https://www.zl-asica.com/2019/mac-centos7-xampp-da-jian-discuz-lun-tan/","excerpt":"å‰è¨€æˆ‘æœ¬äººå½“å¹´åœ¨å»ºç«™çš„æ—¶å€™ä½¿ç”¨çš„å°±æ˜¯ä¸Šè¿°é…ç½®ï¼Œä½†ç”±äºç½‘ä¸Šæ•™ç¨‹åŸºæœ¬éƒ½æ˜¯é€‚ç”¨äº Windows å’Œè€ç‰ˆ xampp çš„æ•™ç¨‹ï¼Œåœ¨ Mac ç³»ç»Ÿå’Œæ–°ç‰ˆçš„ xampp æœ‰äº›åœ°æ–¹å·²ç»ä¸æ˜¯å¾ˆé€‚ç”¨ï¼Œæ‰€ä»¥å½“æ—¶æˆ‘å»ºç«™è·¯ç¨‹æ ¼å¤–çš„è¾›è‹¦ï¼Œå‡ ä¹ç¿»çœ‹è¿‡æ‰€æœ‰èƒ½ä»è°·æ­Œä¸Šæ‰¾åˆ°çš„ç›¸å…³æ–‡ç« ï¼Œä»Šå¤©æˆ‘å°±ç»™å¤§å®¶å†™ä¸€ä¸‹æˆ‘å½“æ—¶å»ºç«™çš„æµç¨‹ï¼Œä¾›å¤§å®¶å‚è€ƒå€Ÿé‰´ã€‚å¦‚æœ‰é”™è¯¯ï¼Œæ¬¢è¿åœ¨ä¸‹æ”¾ç•™è¨€æŒ‡å‡ºã€‚","text":"å‰è¨€æˆ‘æœ¬äººå½“å¹´åœ¨å»ºç«™çš„æ—¶å€™ä½¿ç”¨çš„å°±æ˜¯ä¸Šè¿°é…ç½®ï¼Œä½†ç”±äºç½‘ä¸Šæ•™ç¨‹åŸºæœ¬éƒ½æ˜¯é€‚ç”¨äº Windows å’Œè€ç‰ˆ xampp çš„æ•™ç¨‹ï¼Œåœ¨ Mac ç³»ç»Ÿå’Œæ–°ç‰ˆçš„ xampp æœ‰äº›åœ°æ–¹å·²ç»ä¸æ˜¯å¾ˆé€‚ç”¨ï¼Œæ‰€ä»¥å½“æ—¶æˆ‘å»ºç«™è·¯ç¨‹æ ¼å¤–çš„è¾›è‹¦ï¼Œå‡ ä¹ç¿»çœ‹è¿‡æ‰€æœ‰èƒ½ä»è°·æ­Œä¸Šæ‰¾åˆ°çš„ç›¸å…³æ–‡ç« ï¼Œä»Šå¤©æˆ‘å°±ç»™å¤§å®¶å†™ä¸€ä¸‹æˆ‘å½“æ—¶å»ºç«™çš„æµç¨‹ï¼Œä¾›å¤§å®¶å‚è€ƒå€Ÿé‰´ã€‚å¦‚æœ‰é”™è¯¯ï¼Œæ¬¢è¿åœ¨ä¸‹æ”¾ç•™è¨€æŒ‡å‡ºã€‚ ç³»ç»Ÿä»‹ç»ç”µè„‘ï¼šmacOS Mojave 10.14.3 (18D109)æœåŠ¡å™¨ï¼šé˜¿é‡Œäº‘ ç³»ç»Ÿæ˜¯ CentOS 7.4 64 ä½Discuz!ç‰ˆæœ¬ï¼š X3.4 å»ºç«™å‰éœ€è¦åšçš„äº‹ç¡®è®¤ä½ è¦å»ºç«™çš„ç›®çš„ä¸åŒçš„ç›®çš„å»ºè®¾çš„ç½‘ç«™æ˜¯ä¸ä¸€æ ·çš„,æ¯”å¦‚åšä¸»æ‰€å»ºçš„å°±æ˜¯ä¸€ä¸ª bbs è®ºå›ï¼Œé‚£ä¹ˆéœ€è¦ç”¨åˆ°è®ºå›çš„æ¨¡æ¿æ¯”å¦‚ wordpressã€phpwinã€Discuz!ç­‰ç­‰ã€‚ é€‰æ‹©è®ºå›ä½¿ç”¨çš„æ¨¡æ¿åšä¸»ä¸€å¼€å§‹é€‰æ‹©çš„æ¨¡æ¿æ˜¯ wordpressï¼Œç”±äºè¿™ä¸ªæ¨¡æ¿çš„æ¨¡å¼åŠŸèƒ½è¿‡äºå¼ºå¤§åŒæ—¶ä¹Ÿè¿‡äºå¤æ‚ï¼Œè€Œä¸”è¿™ç§è®ºå›çš„äº¤äº’æ¨¡å¼å¹¶ä¸é€‚åˆå›½å†…ä½¿ç”¨ï¼Œå›½å†…ç”¨æˆ·å¹¶ä¸ä¹ æƒ¯ï¼Œæ‰€ä»¥åœ¨å¤šç•ªæ–Ÿé…ŒåŠé€‰æ‹©åï¼Œåšä¸»é€‰æ‹©äº† Discuzï¼ä½œä¸ºè®ºå›çš„æ¨¡æ¿ã€‚ ç¡®è®¤ç½‘ç«™å—ä¼—åŠå¯èƒ½ä¼šäº§ç”Ÿçš„å³°å€¼æµé‡åœ¨æˆ‘ä»¬è´­ä¹°æœåŠ¡å™¨æ—¶ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘åˆ°æˆ‘ä»¬æ‰€éœ€è¦å¤šå°‘å¸¦å®½ï¼Œè€Œå¸¦å®½åˆ™ç”±ç½‘ç«™æµé‡å†³å®šï¼Œå¦‚æœ æœåŠ¡å™¨é…ç½® ç­‰å¾…æ›´æ–°ä¸­ã€‚ã€‚ã€‚å‚¬æ›´è¯·ç‚¹è¿™é‡Œå‘é€é‚®ä»¶å‚¬æ›´","categories":[{"name":"è½¯ä»¶","slug":"è½¯ä»¶","permalink":"https://www.zl-asica.com/categories/%E8%BD%AF%E4%BB%B6/"}],"tags":[{"name":"Discuz!","slug":"Discuz","permalink":"https://www.zl-asica.com/tags/Discuz/"}]}],"categories":[{"name":"éšæƒ³","slug":"éšæƒ³","permalink":"https://www.zl-asica.com/categories/%E9%9A%8F%E6%83%B3/"},{"name":"è½¯ä»¶","slug":"è½¯ä»¶","permalink":"https://www.zl-asica.com/categories/%E8%BD%AF%E4%BB%B6/"},{"name":"ä»£ç ","slug":"ä»£ç ","permalink":"https://www.zl-asica.com/categories/%E4%BB%A3%E7%A0%81/"}],"tags":[{"name":"Summary","slug":"Summary","permalink":"https://www.zl-asica.com/tags/Summary/"},{"name":"DL","slug":"DL","permalink":"https://www.zl-asica.com/tags/DL/"},{"name":"Python","slug":"Python","permalink":"https://www.zl-asica.com/tags/Python/"},{"name":"ML","slug":"ML","permalink":"https://www.zl-asica.com/tags/ML/"},{"name":"C++","slug":"C","permalink":"https://www.zl-asica.com/tags/C/"},{"name":"PR","slug":"PR","permalink":"https://www.zl-asica.com/tags/PR/"},{"name":"ç¾Šæ¯›","slug":"ç¾Šæ¯›","permalink":"https://www.zl-asica.com/tags/%E7%BE%8A%E6%AF%9B/"},{"name":"Java","slug":"Java","permalink":"https://www.zl-asica.com/tags/Java/"},{"name":"ffmpeg","slug":"ffmpeg","permalink":"https://www.zl-asica.com/tags/ffmpeg/"},{"name":"WordPress","slug":"WordPress","permalink":"https://www.zl-asica.com/tags/WordPress/"},{"name":"æ ‘è“æ´¾","slug":"æ ‘è“æ´¾","permalink":"https://www.zl-asica.com/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"},{"name":"Microsoft","slug":"Microsoft","permalink":"https://www.zl-asica.com/tags/Microsoft/"},{"name":"Discuz!","slug":"Discuz","permalink":"https://www.zl-asica.com/tags/Discuz/"}]}