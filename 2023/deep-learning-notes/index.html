<!doctype html>
<html lang="zh"><head>
<title>Deep Learning深度学习-学习笔记 - ZL Asica的博客</title>
<meta charset="UTF-8">
<meta name="keywords" content="iOS, Mac, Android, Windows, Web, Java, Python, blog, ZL Asica, ZLA, Deep Learning, Machine Learning, DL, ML, Artificial Intelligence, AI">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5">

<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<meta name="description" content="This notes’ content are all based on https:&#x2F;&#x2F;www.coursera.org&#x2F;specializations&#x2F;deep-learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Learning深度学习-学习笔记">
<meta property="og:url" content="https://www.zl-asica.com/2023/deep-learning-notes/index.html">
<meta property="og:site_name" content="ZL Asica的博客">
<meta property="og:description" content="This notes’ content are all based on https:&#x2F;&#x2F;www.coursera.org&#x2F;specializations&#x2F;deep-learning">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2023/11/20/EHtqxOI6c5NQuG8.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/11/20/9w2GKxylSrjfoFq.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/11/20/a6emOyncIbRlo8w.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/11/20/PBfq6ISu2h8vKeV.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/11/20/35UIkJnmlSHt82j.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/11/21/ROywuJQ3dNzfhBo.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/11/21/S4cZ3Aua8es7WfG.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/11/21/WTqeEg4MDPbtK12.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/11/21/jtT9fC5nw2R7xqo.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/11/21/MW7wNCXahR3FZx9.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/11/22/4yOYlp2bcEtPSuw.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/11/26/u6UVeshyTq8BXR1.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/11/28/utnJEFsobOKPfwQ.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/11/28/LqKNWzu4PyxGgbf.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/11/29/2lQR8VdCv7Fwg1W.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/12/03/kLT6ir7dZzPOafx.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/12/04/gnfUEb1wcCqjPoG.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/12/04/pY8ZKI7L3H2NWnT.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/12/04/KuJxvBthlrEYD89.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/12/04/pl2oRWbte7XZ1i3.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/12/04/L8X2VgAwNvjPcMG.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/12/04/Wz7woIvTYcZO8qb.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/12/04/2VwkSjJBQUe87ZN.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/12/04/Z1W9ofDxBlUyegz.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/12/04/9FGXNdz8lqtmnAh.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/12/04/V9FCB8xG2HcOaze.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/12/04/Prp8IkdA2cKUtW3.jpg">
<meta property="og:image" content="https://s2.loli.net/2024/01/10/LR3SMIsuQZmTbiH.png">
<meta property="og:image" content="https://s2.loli.net/2024/01/12/Ju5TjpoefSyzt3w.png">
<meta property="og:image" content="https://s2.loli.net/2024/01/13/7LoM6JaibGInBzr.png">
<meta property="og:image" content="https://s2.loli.net/2024/01/13/VeFyJ18fQ7kMjKz.png">
<meta property="og:image" content="https://s2.loli.net/2024/01/13/4GsDhIeF71TVgZv.png">
<meta property="og:image" content="https://s2.loli.net/2024/01/13/GC6AuH4Rr3zE8if.png">
<meta property="og:image" content="https://s2.loli.net/2024/01/13/VhZCJRq1im3GBFO.png">
<meta property="og:image" content="https://s2.loli.net/2024/01/13/EVAghCZjYbF7wzQ.png">
<meta property="og:image" content="https://s2.loli.net/2024/01/13/Kn5E2VFlbaAsTdM.png">
<meta property="og:image" content="https://s2.loli.net/2024/01/13/XcFKuDMT5xJfpWC.png">
<meta property="og:image" content="https://s2.loli.net/2024/01/13/UBQ4Xdpg7O9yZFz.png">
<meta property="og:image" content="https://s2.loli.net/2024/01/13/rt2LA5zhEg6Mx1V.png">
<meta property="og:image" content="https://s2.loli.net/2024/01/13/ULDT1WobnV4F3Pu.png">
<meta property="og:image" content="https://s2.loli.net/2024/01/13/GdOkYNRx8VycA6K.png">
<meta property="og:image" content="https://s2.loli.net/2024/01/13/m4QoDnL1pJSyzN6.png">
<meta property="og:image" content="https://s2.loli.net/2024/01/13/TdvisN1GKEZwuna.png">
<meta property="og:image" content="https://s2.loli.net/2024/01/16/eT1uh8jkNFc3gAy.png">
<meta property="og:image" content="https://s2.loli.net/2024/01/16/CPgRhtsSjOQkbLG.png">
<meta property="og:image" content="https://s2.loli.net/2024/01/16/mAw43Z9OPqcR8Bk.png">
<meta property="article:published_time" content="2023-11-17T18:18:00.000Z">
<meta property="article:modified_time" content="2024-10-16T04:33:10.159Z">
<meta property="article:author" content="ZL Asica">
<meta property="article:tag" content="DL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2023/11/20/EHtqxOI6c5NQuG8.jpg">

<link rel="stylesheet" href="/lib/fancybox/fancybox.css">
<link rel="stylesheet" href="/lib/mdui_043tiny/mdui.css">


<link rel="stylesheet" href="/lib/iconfont/iconfont.css?v=1729053201451">

<link rel="stylesheet" href="/css/style.css?v=1729053201451">




    
        <link rel="stylesheet" href="/custom.css?v=1729053201451">
    



<script src="/lib/mdui_043tiny/mdui.js" async></script>
<script src="/lib/fancybox/fancybox.umd.js" async></script>
<script src="/lib/lax.min.js" async></script>


<script async src="/js/app.js?v=1729053201451"></script>

 

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-DS0SN4Y59S"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-DS0SN4Y59S');
</script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4208452553759319"
   crossorigin="anonymous"></script>
   <script src="https://cdn.jsdelivr.net/gh/zl-asica/web-cdn/js/zlasica.js"></script>


<link rel="stylesheet"  href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/atom-one-dark.min.css">
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="ZL Asica的博客" type="application/atom+xml">
</head><body class="nexmoe mdui-drawer-body-left"><div id="nexmoe-background"><div class="nexmoe-bg" style="background-image: url(/katomegumi.jpg)"></div><div class="mdui-appbar mdui-shadow-0"><div class="mdui-toolbar"><a class="mdui-btn mdui-btn-icon mdui-ripple" mdui-drawer="{target: &#039;#drawer&#039;, swipe: true}" title="menu"><i class="mdui-icon nexmoefont icon-menu"></i></a><div class="mdui-toolbar-spacer"></div><a class="mdui-btn mdui-btn-icon" href="/" title="ZL Asica"><img src="https://cdn.v2ex.com/gravatar/cba8b28739dd6225f6fe961762bdb0b71b858d68c83d946a37cee3b0e0daece5?size=512" alt="ZL Asica"></a></div></div></div><div id="nexmoe-header"><div class="nexmoe-drawer mdui-drawer" id="drawer">
    <div class="nexmoe-avatar mdui-ripple">
        <a href="/" title="ZL Asica">
            <img src="https://cdn.v2ex.com/gravatar/cba8b28739dd6225f6fe961762bdb0b71b858d68c83d946a37cee3b0e0daece5?size=512" alt="ZL Asica" alt="ZL Asica">
        </a>
    </div>
    <div class="nexmoe-count">
        <div><span>文章</span>14</div>
        <div><span>标签</span>13</div>
        <div><span>分类</span>3</div>
    </div>
    <div class="nexmoe-list mdui-list" mdui-collapse="{accordion: true}">
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple false" href="/" title="回到首页">
            <i class="mdui-list-item-icon nexmoefont icon-home"></i>
            <div class="mdui-list-item-content">
                回到首页
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple false" href="/archive.html" title="文章归档">
            <i class="mdui-list-item-icon nexmoefont icon-container"></i>
            <div class="mdui-list-item-content">
                文章归档
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple false" href="/friends.html" title="我的朋友">
            <i class="mdui-list-item-icon nexmoefont icon-unorderedlist"></i>
            <div class="mdui-list-item-content">
                我的朋友
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple false" href="/about.html" title="关于博主">
            <i class="mdui-list-item-icon nexmoefont icon-info-circle"></i>
            <div class="mdui-list-item-content">
                关于博主
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple false" target="_blank" rel="noopener external nofollow noreferrer" href="https://www.zla.pub" title="WP站点">
            <i class="mdui-list-item-icon nexmoefont icon-ellipsis"></i>
            <div class="mdui-list-item-content">
                WP站点
            </div>
        </a>
        
    </div>
    
    
        
        <div class="nexmoe-widget-wrap">
    <div class="nexmoe-widget nexmoe-search">
         
            <form id="search_form" action_e="https://cn.bing.com/search?q=site:zl-asica.com" onsubmit="return search();">
                <label><input id="search_value" name="q" type="search" placeholder="搜索"></label>
            </form>
         
    </div>
</div>




    
        
        <div class="nexmoe-widget-wrap">
	<div class="nexmoe-widget nexmoe-social">
		<a
			class="mdui-ripple"
			href="https://t.me/zl_asica"
			target="_blank"
			mdui-tooltip="{content: 'Telegram'}"
			style="
				color: rgb(42, 171, 238);
				background-color: rgba(42, 171, 238, .1);
			"
		>
			<i
				class="nexmoefont icon-telegram"
			></i> </a
		><a
			class="mdui-ripple"
			href="https://space.bilibili.com/29018759"
			target="_blank"
			mdui-tooltip="{content: '哔哩哔哩'}"
			style="
				color: rgb(231, 106, 141);
				background-color: rgba(231, 106, 141, .1);
			"
		>
			<i
				class="nexmoefont icon-bilibili"
			></i> </a
		><a
			class="mdui-ripple"
			href="https://github.com/ZL-Asica/"
			target="_blank"
			mdui-tooltip="{content: 'GitHub'}"
			style="
				color: rgb(25, 23, 23);
				background-color: rgba(25, 23, 23, .1);
			"
		>
			<i
				class="nexmoefont icon-github"
			></i> </a
		><a
			class="mdui-ripple"
			href="https://www.zhihu.com/people/zl-asica"
			target="_blank"
			mdui-tooltip="{content: '知乎'}"
			style="
				color: rgb(30, 136, 229);
				background-color: rgba(30, 136, 229, .1);
			"
		>
			<i
				class="nexmoefont icon-zhihu"
			></i> </a
		><a
			class="mdui-ripple"
			href="https://twitter.com/ZL_Asica"
			target="_blank"
			mdui-tooltip="{content: 'Twitter'}"
			style="
				color: rgb(59, 151, 239);
				background-color: rgba(59, 151, 239, .1);
			"
		>
			<i
				class="nexmoefont icon-twitter"
			></i> </a
		><a
			class="mdui-ripple"
			href="mailto:zl@zla.moe"
			target="_blank"
			mdui-tooltip="{content: 'E-Mail'}"
			style="
				color: rgb(247, 56, 101);
				background-color: rgba(247, 56, 101, .1);
			"
		>
			<i
				class="nexmoefont icon-mail-fill"
			></i> </a
		><a
			class="mdui-ripple"
			href="https://weibo.com/5907278427"
			target="_blank"
			mdui-tooltip="{content: '微博'}"
			style="
				color: rgb(223, 32, 41);
				background-color: rgba(223, 32, 41, .1);
			"
		>
			<i
				class="nexmoefont icon-weibo"
			></i> </a
		><a
			class="mdui-ripple"
			href="https://www.zl-asica.com/atom.xml"
			target="_blank"
			mdui-tooltip="{content: 'RSS'}"
			style="
				color: rgb(247, 132, 34);
				background-color: rgba(247, 132, 34, .1);
			"
		>
			<i
				class="nexmoefont icon-rss"
			></i> </a
		>
	</div>
</div>

    
        
        
        
        <!-- 一言 -->
<div class="nexmoe-widget-wrap">
  <h3 class="nexmoe-widget-title">
    一言
  </h3>
  <div class="nexmoe-widget">
    <ul class="hitokoto-box">
      <li id="hitokoto_text_parent" class="hitokoto-text" hitokotoCategory="">
        <a href="#" id="hitokoto_text">
          
        </a>
        <a href="#" id="hitokoto_error_text" style="display: none;">
          
        </a>
      </li>
    </ul>
  </div>
</div>

<script>
  let hitokotoText = document.getElementById('hitokoto_text')
  let hitokotoErroText = document.getElementById('hitokoto_error_text')
  let hitokotoCategory = document.getElementById('hitokoto_text_parent').getAttribute('hitokotoCategory')
  window.addEventListener('load', function () {
    let url = 'https://v1.hitokoto.cn'
    if (hitokotoCategory) {
      url += '?c=' + hitokotoCategory
    }
    fetch(url)
      .then(response => response.json())
      .then(data => {
        hitokotoText.innerText = "「 " + data.hitokoto + " 」 from " + data.from
        hitokotoText.href = 'https://hitokoto.cn/?uuid=' + data.uuid
      })
      .catch((reason) => {
        console.error(11, reason)
        hitokotoText.style.display = 'none'
        hitokotoErroText.style.display = 'block'
      })
  })
</script>
    
        
        
        
   
    <div class="nexmoe-copyright">
        &copy; 2024 ZL Asica
        Powered by <a href="http://hexo.io/" rel="external nofollow noreferrer" target="_blank">Hexo</a>
        & <a href="https://github.com/theme-nexmoe/hexo-theme-nexmoe" rel="external nofollow noreferrer" target="_blank">Nexmoe</a>
        <br><p> Since 2017</p>
<a href="https://icp.gov.moe/?keyword=20245686" rel="external nofollow noreferrer" target="_blank">萌ICP备20245686号</a>
<br>
<a target="_blank" href="http://beian.miit.gov.cn/" rel="external nofollow noreferrer">鲁ICP备19057855号</a>

    </div>
</div><!-- .nexmoe-drawer --></div><div id="nexmoe-content"><div class="nexmoe-primary"><div class="nexmoe-post">
  <article>
    
        <div class="nexmoe-post-cover absolute" style="padding-top: NaN%;"> 
            <img src="https://fp1.fghrsh.net/2023/11/21/49843f304bf140153456b64733542a06.jpg!q80.jpeg" alt="Deep Learning深度学习-学习笔记" loading="lazy">
            <h1>Deep Learning深度学习-学习笔记</h1>
        </div>
    
    
    <div class="nexmoe-post-meta">
    <div class="nexmoe-rainbow">
        <a class="nexmoefont icon-calendar-fill">2023年11月18日</a>
        
            <a class="nexmoefont icon-appstore-fill -link" href="/categories/%E8%BD%AF%E4%BB%B6/">软件</a>
        
        
    </div>
    
    
    
    
    
</div>

    <p>This notes’ content are all based on <a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.coursera.org/specializations/deep-learning">https://www.coursera.org/specializations/deep-learning</a><span id="more"></span></p>
<blockquote>
<p>Latex may have some issues when displaying.</p>
</blockquote>
<h2 id="1-Neural-Networks-and-Deep-Learning"><a href="#1-Neural-Networks-and-Deep-Learning" class="headerlink" title="1. Neural Networks and Deep Learning"></a>1. Neural Networks and Deep Learning</h2><h3 id="1-1-Introduction-to-Deep-Learning"><a href="#1-1-Introduction-to-Deep-Learning" class="headerlink" title="1.1 Introduction to Deep Learning"></a>1.1 Introduction to Deep Learning</h3><h4 id="1-1-1-Supervised-Learning-with-Deep-Learning"><a href="#1-1-1-Supervised-Learning-with-Deep-Learning" class="headerlink" title="1.1.1 Supervised Learning with Deep Learning"></a>1.1.1 Supervised Learning with Deep Learning</h4><ul>
<li>Structured Data: Charts.</li>
<li>Unstructured Data: Audio, Image, Text.</li>
</ul>
<h4 id="1-1-2-Scale-drives-deep-learning-progress"><a href="#1-1-2-Scale-drives-deep-learning-progress" class="headerlink" title="1.1.2 Scale drives deep learning progress"></a>1.1.2 Scale drives deep learning progress</h4><ul>
<li>The larger the amount of data, the better the performance of the larger neural network compare to smaller one or supervised learning.</li>
<li>Sigmoid change to ReLU will make gradient descent much more faster. Since the gradient will not go to 0 really fast.</li>
</ul>
<h3 id="1-2-Basics-of-Neural-Network-Programming"><a href="#1-2-Basics-of-Neural-Network-Programming" class="headerlink" title="1.2 Basics of Neural Network Programming"></a>1.2 Basics of Neural Network Programming</h3><h4 id="1-2-1-Binary-Classification"><a href="#1-2-1-Binary-Classification" class="headerlink" title="1.2.1 Binary Classification"></a>1.2.1 Binary Classification</h4><ul>
<li>Input: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mi>x</mi></mrow></msup></mrow><annotation encoding="application/x-tex">X \in R^{nx} </annotation></semantics></math></span></li>
<li>Output: 0, 1</li>
</ul>
<h4 id="1-2-2-Logistic-Regression"><a href="#1-2-2-Logistic-Regression" class="headerlink" title="1.2.2 Logistic Regression"></a>1.2.2 Logistic Regression</h4><ul>
<li><p>Given <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span>, want <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\hat{y} = P(y=1|x)</annotation></semantics></math></span></p>
</li>
<li><p>Input: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi>R</mi><msub><mi>n</mi><mi>x</mi></msub></msup></mrow><annotation encoding="application/x-tex">x \in R^{n_x} </annotation></semantics></math></span></p>
</li>
<li><p>Parameters: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>∈</mo><msup><mi>R</mi><msub><mi>n</mi><mi>x</mi></msub></msup><mo separator="true">,</mo><mi>b</mi><mo>∈</mo><mi>R</mi></mrow><annotation encoding="application/x-tex">w \in R^{n_x}, b \in R </annotation></semantics></math></span></p>
</li>
<li><p>Output <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mi>T</mi></msup><mi>x</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\hat{y} = \sigma(w^Tx + b)</annotation></semantics></math></span></p>
<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>z</mi></mrow></msup></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\sigma(z)=\dfrac{1}{1+e^{-z}}</annotation></semantics></math></span></li>
<li>If <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z </annotation></semantics></math></span> large, <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>≈</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mn>0</mn></mrow></mfrac></mstyle><mo>≈</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\sigma(z)\approx\dfrac{1}{1+0}\approx1</annotation></semantics></math></span></li>
<li>If <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z </annotation></semantics></math></span> large negative number, <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>≈</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>B</mi><mi>i</mi><mi>g</mi><mi>n</mi><mi>u</mi><mi>m</mi></mrow></mfrac></mstyle><mo>≈</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\sigma(z)\approx\dfrac{1}{1+Bignum}\approx0</annotation></semantics></math></span></li>
</ul>
</li>
<li><p>Loss (error) function:</p>
<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mi>T</mi></msup><mi>x</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\hat{y} = \sigma(w^Tx + b)</annotation></semantics></math></span>, where <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>z</mi></mrow></msup></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\sigma(z)=\dfrac{1}{1+e^{-z}}</annotation></semantics></math></span>

<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><msup><mi>w</mi><mi>T</mi></msup><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">z^{(i)}=w^Tx^{(i)}+b</annotation></semantics></math></span></li>
</ul>
</li>
<li><p>Want <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>≈</mo><msup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">y^{(i)} \approx \hat{y}^{(i)} </annotation></semantics></math></span></p>
</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>y</mi><mo separator="true">,</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mo stretchy="false">[</mo><mi>y</mi><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo stretchy="false">)</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">L(y, \hat{y}) = -[y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})]</annotation></semantics></math></span>
<ul>
<li>If <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mn>1</mn><mo>:</mo><mi>L</mi><mo stretchy="false">(</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">y=1: L(\hat{y}, y)=-\log{\hat{y}} </annotation></semantics></math></span> &lt;- want <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>log</mi><mo>⁡</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\log{\hat{y}}</annotation></semantics></math></span> as large as possible, want <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math></span> large</li>
<li>If <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mn>0</mn><mo>:</mo><mi>L</mi><mo stretchy="false">(</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">y=0: L(\hat{y}, y)=-\log{(1-\hat{y})} </annotation></semantics></math></span> &lt;- want <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log{(1-\hat{y})}</annotation></semantics></math></span> as large as possible, want <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math></span> small</li>
</ul>
</li>
</ul>
</li>
<li><p>Cost function</p>
<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mo>∑</mo><mo>∗</mo><msup><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msup><mi>L</mi><mo stretchy="false">(</mo><msup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mo>∑</mo><mo>∗</mo><msup><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msup><mi>L</mi><mo stretchy="false">[</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><msup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">J(w, b)=\dfrac{1}{m}\sum\limits*{i=1}^{m}L(\hat{y}^{(i)},y^{(i)})=-\dfrac{1}{m}\sum\limits*{i=1}^{m}L[y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)})]</annotation></semantics></math></span></li>
</ul>
</li>
</ul>
<h4 id="1-2-3-Gradient-Descent"><a href="#1-2-3-Gradient-Descent" class="headerlink" title="1.2.3 Gradient Descent"></a>1.2.3 Gradient Descent</h4><ul>
<li>Repeat <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>:</mo><mo>=</mo><mi>w</mi><mo>−</mo><mi>α</mi><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>d</mi><mi>J</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mrow><mrow><mi>d</mi><mi>w</mi></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">w:=w-\alpha\dfrac{dJ(w)}{dw}</annotation></semantics></math></span>; <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mo>:</mo><mo>=</mo><mi>b</mi><mo>−</mo><mi>α</mi><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>J</mi><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">∂</mi><mi>b</mi></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">b:=b-\alpha\dfrac{\partial J(w,b)}{\partial b}</annotation></semantics></math></span><ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span>: Learning rate</li>
<li>Right side of minimum, <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>d</mi><mi>J</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mrow><mrow><mi>d</mi><mi>w</mi></mrow></mfrac></mstyle><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\dfrac{dJ(w)}{dw} &gt; 0</annotation></semantics></math></span>; Left side of minimum, <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>d</mi><mi>J</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mrow><mrow><mi>d</mi><mi>w</mi></mrow></mfrac></mstyle><mo>&lt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\dfrac{dJ(w)}{dw} &lt; 0</annotation></semantics></math></span></li>
</ul>
</li>
<li>Logistic Regression Gradient Descent<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">x_1,x_2,w_1,w_2,b</annotation></semantics></math></span>
<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>=</mo><msub><mi>w</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>2</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">z=w_1x_1+w_2x_2+b</annotation></semantics></math></span> --><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a=\sigma(z)</annotation></semantics></math></span> --><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L=(a,y)</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>a</mi><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>d</mi><mi>L</mi><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mrow><mi>d</mi><mi>a</mi></mrow></mfrac></mstyle><mo>=</mo><mo>−</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mi>y</mi><mi>a</mi></mfrac></mstyle><mo>+</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mn>1</mn><mo>−</mo><mi>y</mi></mrow><mrow><mn>1</mn><mo>−</mo><mi>a</mi></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">da=\dfrac{dL(a,y)}{da}=-\dfrac{y}{a}+\dfrac{1-y}{1-a}</annotation></semantics></math></span>
<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>d</mi><mi>L</mi><mo stretchy="false">(</mo><mi>y</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><mrow><mi>d</mi><mi>a</mi></mrow></mfrac></mstyle><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mi>d</mi><mrow><mi>d</mi><mi>a</mi></mrow></mfrac></mstyle><mo stretchy="false">(</mo><mo>−</mo><mi>y</mi><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo><mo>−</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo stretchy="false">)</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>a</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\dfrac{dL(y,a)}{da} = \dfrac{d}{da}(-y\log(a) - (1-y)\log(1-a))</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mi>d</mi><mrow><mi>d</mi><mi>a</mi></mrow></mfrac></mstyle><mo stretchy="false">(</mo><mo>−</mo><mi>y</mi><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mi>y</mi><mi>a</mi></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\dfrac{d}{da} (-y\log(a)) = -\dfrac{y}{a}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mi>d</mi><mrow><mi>d</mi><mi>a</mi></mrow></mfrac></mstyle><mo stretchy="false">(</mo><mo>−</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo stretchy="false">)</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>a</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mn>1</mn><mo>−</mo><mi>y</mi></mrow><mrow><mn>1</mn><mo>−</mo><mi>a</mi></mrow></mfrac></mstyle><mo>×</mo><mo stretchy="false">(</mo><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mn>1</mn><mo>−</mo><mi>y</mi></mrow><mrow><mn>1</mn><mo>−</mo><mi>a</mi></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\dfrac{d}{da} (-(1-y)\log(1-a)) = -\dfrac{1-y}{1-a} \times (-1) = \dfrac{1-y}{1-a} </annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mo>−</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mi>y</mi><mi>a</mi></mfrac></mstyle><mo>+</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mn>1</mn><mo>−</mo><mi>y</mi></mrow><mrow><mn>1</mn><mo>−</mo><mi>a</mi></mrow></mfrac></mstyle><mo>=</mo><mo>−</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mi>y</mi><mi>a</mi></mfrac></mstyle><mo>−</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>y</mi><mo>−</mo><mn>1</mn></mrow><mrow><mn>1</mn><mo>−</mo><mi>a</mi></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">=-\dfrac{y}{a} + \dfrac{1-y}{1-a} = -\dfrac{y}{a} - \dfrac{y-1}{1-a}</annotation></semantics></math></span></li>
</ul>
</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>z</mi><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>d</mi><mi>L</mi></mrow><mrow><mi>d</mi><mi>z</mi></mrow></mfrac></mstyle><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>d</mi><mi>L</mi><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mrow><mi>d</mi><mi>z</mi></mrow></mfrac></mstyle><mo>=</mo><mi>a</mi><mo>−</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">dz=\dfrac{dL}{dz}=\dfrac{dL(a,y)}{dz}=a-y</annotation></semantics></math></span>
<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>d</mi><mi>L</mi></mrow><mrow><mi>d</mi><mi>a</mi></mrow></mfrac></mstyle><mo>⋅</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>d</mi><mi>a</mi></mrow><mrow><mi>d</mi><mi>z</mi></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">=\dfrac{dL}{da}\cdot\dfrac{da}{dz}</annotation></semantics></math></span> (<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>d</mi><mi>a</mi></mrow><mrow><mi>d</mi><mi>z</mi></mrow></mfrac></mstyle><mo>=</mo><mi>a</mi><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\dfrac{da}{dz}=a(1-a)</annotation></semantics></math></span>)</li>
</ul>
</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>d</mi><mi>L</mi></mrow><mrow><mi>d</mi><msub><mi>w</mi><mn>1</mn></msub></mrow></mfrac></mstyle><mo>=</mo><mi mathvariant="normal">&quot;</mi><mi>d</mi><msub><mi>w</mi><mn>1</mn></msub><mi mathvariant="normal">&quot;</mi><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo>⋅</mo><mi>d</mi><mi>z</mi></mrow><annotation encoding="application/x-tex">\dfrac{dL}{dw_1}=&quot;dw_1&quot;=x_1\cdot dz</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>d</mi><mi>L</mi></mrow><mrow><mi>d</mi><msub><mi>w</mi><mn>2</mn></msub></mrow></mfrac></mstyle><mo>=</mo><mi mathvariant="normal">&quot;</mi><mi>d</mi><msub><mi>w</mi><mn>2</mn></msub><mi mathvariant="normal">&quot;</mi><mo>=</mo><msub><mi>x</mi><mn>2</mn></msub><mo>⋅</mo><mi>d</mi><mi>z</mi></mrow><annotation encoding="application/x-tex">\dfrac{dL}{dw_2}=&quot;dw_2&quot;=x_2\cdot dz</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>b</mi><mo>=</mo><mi>d</mi><mi>z</mi></mrow><annotation encoding="application/x-tex">db=dz</annotation></semantics></math></span></li>
</ul>
</li>
</ul>
</li>
<li>Gradient Descent on <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span> examples<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mo>∑</mo><mi mathvariant="normal">_</mi><msup><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msup><mi>L</mi><mo stretchy="false">(</mo><msup><mi>a</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">J(w, b)=\dfrac{1}{m}\sum\limits\_{i=1}^{m}L(a^{(i)},y^{(i)})</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mi mathvariant="normal">∂</mi><mrow><mi mathvariant="normal">∂</mi><mi>w</mi><mo>∗</mo><mn>1</mn></mrow></mfrac></mstyle><mi>J</mi><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mo>∑</mo><mo>∗</mo><msup><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msup><mstyle displaystyle="true" scriptlevel="0"><mfrac><mi mathvariant="normal">∂</mi><mrow><mi mathvariant="normal">∂</mi><msub><mi>w</mi><mn>1</mn></msub></mrow></mfrac></mstyle><mi>L</mi><mo stretchy="false">(</mo><msup><mi>a</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\dfrac{\partial}{\partial w*1}J(w,b)=\dfrac{1}{m}\sum\limits*{i=1}^{m}\dfrac{\partial}{\partial w_1}L(a^{(i)},y^{(i)})</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo>=</mo><mn>0</mn><mo separator="true">;</mo><mi>d</mi><msub><mi>w</mi><mn>1</mn></msub><mo>=</mo><mn>0</mn><mo separator="true">;</mo><mi>d</mi><msub><mi>w</mi><mn>2</mn></msub><mo>=</mo><mn>0</mn><mo separator="true">;</mo><mi>d</mi><mi>b</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">J=0;dw_1=0;dw_2=0;db=0</annotation></semantics></math></span>
<ul>
<li>for <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">i=1</annotation></semantics></math></span> to <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><msup><mi>w</mi><mi>T</mi></msup><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">z^{(i)}=w^Tx^{(i)}+b</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a^{(i)}=\sigma (z^{(i)})</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo>+</mo><mo>=</mo><mo>−</mo><mo stretchy="false">[</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mi>l</mi><mi>o</mi><mi>g</mi><msup><mi>a</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msup><mi>a</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">J+=-[y^{(i)}loga^{(i)}+(1-y^{(i)})log(1-a^{(i)})]</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><msup><mi>a</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>−</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">dz^{(i)}=a^{(i)}-y^{(i)}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msub><mi>w</mi><mn>1</mn></msub><mo>+</mo><mo>=</mo><msubsup><mi>x</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mi>d</mi><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">dw_1+=x_1^{(i)}dz^{(i)}</annotation></semantics></math></span> (for n = 2)</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msub><mi>w</mi><mn>2</mn></msub><mo>+</mo><mo>=</mo><msubsup><mi>x</mi><mn>2</mn><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mi>d</mi><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">dw_2+=x_2^{(i)}dz^{(i)}</annotation></semantics></math></span> (for n = 2)</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>b</mi><mo>+</mo><mo>=</mo><mi>d</mi><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">db+=dz^{(i)}</annotation></semantics></math></span></li>
</ul>
</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mi mathvariant="normal">/</mi><mo>=</mo><mi>m</mi><mo separator="true">;</mo><mi>d</mi><msub><mi>w</mi><mn>1</mn></msub><mi mathvariant="normal">/</mi><mo>=</mo><mi>m</mi><mo separator="true">;</mo><mi>d</mi><msub><mi>w</mi><mn>2</mn></msub><mi mathvariant="normal">/</mi><mo>=</mo><mi>m</mi><mo separator="true">;</mo><mi>d</mi><mi>b</mi><mi mathvariant="normal">/</mi><mo>=</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">J/=m;dw_1/=m;dw_2/=m;db/=m</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msub><mi>w</mi><mn>1</mn></msub><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>J</mi></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>w</mi><mn>1</mn></msub></mrow></mfrac></mstyle><mo separator="true">;</mo><mi>d</mi><msub><mi>w</mi><mn>2</mn></msub><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>J</mi></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>w</mi><mn>2</mn></msub></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">dw_1=\dfrac{\partial J}{\partial w_1}; dw_2=\dfrac{\partial J}{\partial w_2}</annotation></semantics></math></span>
<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>:</mo><mo>=</mo><msub><mi>w</mi><mn>1</mn></msub><mo>−</mo><mi>α</mi><mi>d</mi><msub><mi>w</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">w_1:=w_1-\alpha dw_1</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>2</mn></msub><mo>:</mo><mo>=</mo><msub><mi>w</mi><mn>2</mn></msub><mo>−</mo><mi>α</mi><mi>d</mi><msub><mi>w</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">w_2:=w_2-\alpha dw_2</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mo>:</mo><mo>=</mo><mi>b</mi><mo>−</mo><mi>α</mi><mi>d</mi><mi>b</mi></mrow><annotation encoding="application/x-tex">b:=b-\alpha db</annotation></semantics></math></span></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="1-2-4-Computational-Graph"><a href="#1-2-4-Computational-Graph" class="headerlink" title="1.2.4 Computational Graph"></a>1.2.4 Computational Graph</h4><ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>b</mi><mo separator="true">,</mo><mi>c</mi><mo stretchy="false">)</mo><mo>=</mo><mn>3</mn><mo stretchy="false">(</mo><mi>a</mi><mo>+</mo><mi>b</mi><mi>c</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">J(a,b,c)=3(a+bc)</annotation></semantics></math></span>

<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>=</mo><mi>b</mi><mi>c</mi></mrow><annotation encoding="application/x-tex">u=bc</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>=</mo><mi>a</mi><mo>+</mo><mi>u</mi></mrow><annotation encoding="application/x-tex">v=a+u</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo>=</mo><mn>3</mn><mi>v</mi></mrow><annotation encoding="application/x-tex">J=3v</annotation></semantics></math></span></li>
<li>Left to right computation</li>
</ul>
</li>
<li><p>Derivatives with a Computation Graph</p>
<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>d</mi><mi>J</mi></mrow><mrow><mi>d</mi><mi>v</mi></mrow></mfrac></mstyle><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">\dfrac{dJ}{dv}=3</annotation></semantics></math></span>
<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>d</mi><mi>J</mi></mrow><mrow><mi>d</mi><mi>a</mi></mrow></mfrac></mstyle><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">\dfrac{dJ}{da}=3</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>d</mi><mi>v</mi></mrow><mrow><mi>d</mi><mi>a</mi></mrow></mfrac></mstyle><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\dfrac{dv}{da}=1</annotation></semantics></math></span></li>
<li>Chain Rule: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>d</mi><mi>J</mi></mrow><mrow><mi>d</mi><mi>a</mi></mrow></mfrac></mstyle><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>d</mi><mi>J</mi></mrow><mrow><mi>d</mi><mi>v</mi></mrow></mfrac></mstyle><mo>⋅</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>d</mi><mi>v</mi></mrow><mrow><mi>d</mi><mi>a</mi></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\dfrac{dJ}{da}=\dfrac{dJ}{dv}\cdot\dfrac{dv}{da}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>d</mi><mi>J</mi></mrow><mrow><mi>d</mi><mi>u</mi></mrow></mfrac></mstyle><mo>=</mo><mn>3</mn><mo separator="true">;</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>d</mi><mi>u</mi></mrow><mrow><mi>d</mi><mi>b</mi></mrow></mfrac></mstyle><mo>=</mo><mn>2</mn><mo separator="true">;</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>d</mi><mi>J</mi></mrow><mrow><mi>d</mi><mi>b</mi></mrow></mfrac></mstyle><mo>=</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">\dfrac{dJ}{du}=3; \dfrac{du}{db}=2; \dfrac{dJ}{db}=6</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>d</mi><mi>u</mi></mrow><mrow><mi>d</mi><mi>c</mi></mrow></mfrac></mstyle><mo>=</mo><mn>3</mn><mo separator="true">;</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>d</mi><mi>J</mi></mrow><mrow><mi>d</mi><mi>c</mi></mrow></mfrac></mstyle><mo>=</mo><mn>9</mn></mrow><annotation encoding="application/x-tex">\dfrac{du}{dc}=3; \dfrac{dJ}{dc}=9</annotation></semantics></math></span></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="1-2-5-Vectorization"><a href="#1-2-5-Vectorization" class="headerlink" title="1.2.5 Vectorization"></a>1.2.5 Vectorization</h4><ul>
<li><p>avoid explicit for-loops.</p>
</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo>=</mo><mn>0</mn><mo separator="true">;</mo><mi>d</mi><mi>w</mi><mo>=</mo><mi>n</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>z</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>s</mi><mo stretchy="false">(</mo><mo stretchy="false">(</mo><msub><mi>n</mi><mi>x</mi></msub><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo separator="true">;</mo><mi>d</mi><mi>b</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">J=0;dw=np.zeros((n_x,1));db=0</annotation></semantics></math></span>
<ul>
<li>for <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">i=1</annotation></semantics></math></span> to <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><msup><mi>w</mi><mi>T</mi></msup><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">z^{(i)}=w^Tx^{(i)}+b</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a^{(i)}=\sigma (z^{(i)})</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo>+</mo><mo>=</mo><mo>−</mo><mo stretchy="false">[</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mi>l</mi><mi>o</mi><mi>g</mi><msup><mi>a</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msup><mi>a</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">J+=-[y^{(i)}loga^{(i)}+(1-y^{(i)})log(1-a^{(i)})]</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><msup><mi>a</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>−</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">dz^{(i)}=a^{(i)}-y^{(i)}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>w</mi><mo>+</mo><mo>=</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mi>d</mi><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">dw+=x^{(i)}dz^{(i)}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>b</mi><mo>+</mo><mo>=</mo><mi>d</mi><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">db+=dz^{(i)}</annotation></semantics></math></span></li>
</ul>
</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mi mathvariant="normal">/</mi><mo>=</mo><mi>m</mi><mo separator="true">;</mo><mi>d</mi><mi>w</mi><mi mathvariant="normal">/</mi><mo>=</mo><mi>m</mi><mo separator="true">;</mo><mi>d</mi><mi>b</mi><mi mathvariant="normal">/</mi><mo>=</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">J/=m;dw/=m;db/=m</annotation></semantics></math></span></li>
</ul>
</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Z</mi><mo>=</mo><mi>n</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>d</mi><mi>o</mi><mi>t</mi><mo stretchy="false">(</mo><mi>w</mi><mi mathvariant="normal">.</mi><mi>T</mi><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">Z=np.dot(w.T,x)+b</annotation></semantics></math></span> ; b(1,1)-->Broodcasting</li>
<li><p>Vectorization Logistic Regression</p>
<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><msup><mi>a</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>−</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">;</mo><mi>d</mi><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><msup><mi>a</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo>−</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">dz^{(1)}=a^{(1)}-y^{(1)}; dz^{(2)}=a^{(2)}-y^{(2)}...</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>z</mi><mo>=</mo><mo stretchy="false">[</mo><mi>d</mi><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><mi>d</mi><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>d</mi><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">dz=[dz^{(1)}, dz^{(2)},...,dz^{(m)}]</annotation></semantics></math></span> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">1\times m</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mo stretchy="false">[</mo><msup><mi>a</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>a</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msup><mi>a</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">A=[a^{(1)}, a^{(2)}, ..., a^{(m)}]</annotation></semantics></math></span> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>=</mo><mo stretchy="false">[</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">Y=[y^{(1)}, y^{(2)}, ..., y^{(m)}]</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>z</mi><mo>=</mo><mi>A</mi><mo>−</mo><mi>Y</mi><mo>=</mo><mo stretchy="false">[</mo><msup><mi>a</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>−</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>a</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo>−</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">dz=A-Y=[a^{(1)}-y^{(1)}, a^{(2)}-y^{(2)}, ...]</annotation></semantics></math></span></li>
<li>Get rid of <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>b</mi></mrow><annotation encoding="application/x-tex">db</annotation></semantics></math></span> and <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">dw</annotation></semantics></math></span> in for loop<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>b</mi><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mo>∑</mo><mi mathvariant="normal">_</mi><msup><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msup><mi>d</mi><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mi>n</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>s</mi><mi>u</mi><mi>m</mi><mo stretchy="false">(</mo><mi>d</mi><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">db=\dfrac{1}{m}\sum\limits\_{i=1}^{m}dz^{(i)}=\dfrac{1}{m} np.sum(dz)</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>w</mi><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mo>⋅</mo><mi>X</mi><mo>⋅</mo><mi>d</mi><msup><mi>z</mi><mi>T</mi></msup><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mo stretchy="false">[</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>d</mi><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo stretchy="false">]</mo><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mo>⋅</mo><mo stretchy="false">[</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mi>d</mi><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo>+</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msup><mi>d</mi><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">dw=\dfrac{1}{m}\cdot X\cdot dz^T=\dfrac{1}{m}[x^{(1)}...][dz^{(1)}...]=\dfrac{1}{m}\cdot[x^{(1)}dz^{(1)}+...+x^{(m)}dz^{(m)}]</annotation></semantics></math></span> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n\times 1</annotation></semantics></math></span></li>
</ul>
</li>
<li>New Form of Logistic Regression<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Z</mi><mo>=</mo><msup><mi>w</mi><mi>t</mi></msup><mi>X</mi><mo>+</mo><mi>b</mi><mo>=</mo><mi>n</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>d</mi><mi>o</mi><mi>t</mi><mo stretchy="false">(</mo><mi>w</mi><mi mathvariant="normal">.</mi><mi>T</mi><mo separator="true">,</mo><mi>X</mi><mo stretchy="false">)</mo><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">Z=w^tX+b=np.dot(w.T, X)+b</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><mi>Z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">A=\sigma (Z)</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>z</mi><mo>=</mo><mi>A</mi><mo>−</mo><mi>Y</mi></mrow><annotation encoding="application/x-tex">dz=A-Y</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>w</mi><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mo>⋅</mo><mi>X</mi><mo>⋅</mo><mi>d</mi><msup><mi>Z</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">dw=\dfrac{1}{m}\cdot X \cdot dZ^T</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>b</mi><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mi>n</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>s</mi><mi>u</mi><mi>m</mi><mo stretchy="false">(</mo><mi>d</mi><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">db=\dfrac{1}{m}np.sum(dz)</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>:</mo><mo>=</mo><mi>w</mi><mo>−</mo><mi>α</mi><mi>d</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">w:=w-\alpha dw</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mo>:</mo><mo>=</mo><mi>b</mi><mo>−</mo><mi>α</mi><mi>d</mi><mi>b</mi></mrow><annotation encoding="application/x-tex">b:=b-\alpha db</annotation></semantics></math></span></li>
</ul>
</li>
</ul>
</li>
<li><p>Broadcasting(same as bsxfun in Matlab&#x2F;Octave)</p>
<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>m</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(m,n)</annotation></semantics></math></span>+-\*/<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>1</mn><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(1,n)</annotation></semantics></math></span>-><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>m</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(m,n)</annotation></semantics></math></span> 1->m will be all the same number.</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>m</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(m,n)</annotation></semantics></math></span>+-\*/<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>m</mi><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(m,1)</annotation></semantics></math></span>-><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>m</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(m,n)</annotation></semantics></math></span> 1->n will be all the same number</li>
<li>Don’t use <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>=</mo><mi>n</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>o</mi><mi>m</mi><mi mathvariant="normal">.</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>n</mi><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a = np.random.randn(5)</annotation></semantics></math></span> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mi mathvariant="normal">.</mi><mi>s</mi><mi>h</mi><mi>a</mi><mi>p</mi><mi>e</mi><mo>=</mo><mo stretchy="false">(</mo><mn>5</mn><mo separator="true">,</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a.shape = (5,)</annotation></semantics></math></span> “rank 1 array”</li>
<li>Use <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>=</mo><mi>n</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>o</mi><mi>m</mi><mi mathvariant="normal">.</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>n</mi><mo stretchy="false">(</mo><mn>5</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a = np.random.randn(5,1)</annotation></semantics></math></span> or <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>=</mo><mi>n</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>o</mi><mi>m</mi><mi mathvariant="normal">.</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>n</mi><mo stretchy="false">(</mo><mn>1</mn><mo separator="true">,</mo><mn>5</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a = np.random.randn(1,5)</annotation></semantics></math></span></li>
<li>Check by <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mi>s</mi><mi>s</mi><mi>e</mi><mi>r</mi><mi>t</mi><mo stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">.</mi><mi>s</mi><mi>h</mi><mi>a</mi><mi>p</mi><mi>e</mi><mo>=</mo><mo>=</mo><mo stretchy="false">(</mo><mn>5</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">assert(a.shape == (5,1))</annotation></semantics></math></span></li>
<li>Fix rank 1 array by <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>=</mo><mi>a</mi><mi mathvariant="normal">.</mi><mi>r</mi><mi>e</mi><mi>s</mi><mi>h</mi><mi>a</mi><mi>p</mi><mi>e</mi><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mn>5</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a = a.reshape((5,1))</annotation></semantics></math></span></li>
</ul>
</li>
<li><p>Logistic Regression Cost Function</p>
<ul>
<li>Lost<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>y</mi></msup><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msup><mo stretchy="false">)</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">p(y|x)=\hat{y}^y(1-\hat{y})^{(1-y)}</annotation></semantics></math></span></li>
<li>If <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">y=1</annotation></semantics></math></span>: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">p(y|x)=\hat{y}</annotation></semantics></math></span></li>
<li>If <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">y=0</annotation></semantics></math></span>: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(y|x)=(1-\hat{y})</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>log</mi><mo>⁡</mo><msup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>y</mi></msup><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msup><mo stretchy="false">)</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mi>y</mi><mi>log</mi><mo>⁡</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo stretchy="false">)</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mi>L</mi><mo stretchy="false">(</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\log p(y|x)=\log \hat{y}^y(1-\hat{y})^{(1-y)}=y\log \hat{y}+(1-y)\log(1-\hat{y})=-L(\hat{y},y)</annotation></semantics></math></span></li>
</ul>
</li>
<li>Cost<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><mi>l</mi><mi>a</mi><mi>b</mi><mi>e</mi><mi>l</mi><mi>s</mi><mtext> </mtext><mi>i</mi><mi>n</mi><mtext> </mtext><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi><mi>i</mi><mi>n</mi><mi>g</mi><mtext> </mtext><mi>s</mi><mi>e</mi><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>log</mi><mo>⁡</mo><mi mathvariant="normal">Π</mi><mi mathvariant="normal">_</mi><msup><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msup><mi>p</mi><mo stretchy="false">(</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\log p(labels\space in\space training\space set)=\log \Pi\_{i=1}^{m}p(y^{(i)},x^{(i)})</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><mi>l</mi><mi>a</mi><mi>b</mi><mi>e</mi><mi>l</mi><mi>s</mi><mtext> </mtext><mi>i</mi><mi>n</mi><mtext> </mtext><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi><mi>i</mi><mi>n</mi><mi>g</mi><mtext> </mtext><mi>s</mi><mi>e</mi><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mo>∑</mo><mo>∗</mo><msup><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msup><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mo>∑</mo><mo>∗</mo><msup><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msup><mi>L</mi><mo stretchy="false">(</mo><msup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\log p(labels\space in\space training\space set)=\sum\limits*{i=1}^m\log p(y^{(i)},x^{(i)})=-\sum\limits*{i=1}^mL(\hat{y}^{(i)},y^{(i)})</annotation></semantics></math></span></li>
<li>Use maximum likelihood estimation(MLE)</li>
<li>Cost(minmize): <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mo>∑</mo><mi mathvariant="normal">_</mi><msup><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msup><mi>L</mi><mo stretchy="false">(</mo><msup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">J(w,b)=\dfrac{1}{m}\sum\limits\_{i=1}^mL(\hat{y}^{(i)},y^{(i)})</annotation></semantics></math></span></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="1-3-Shallow-Neural-Networks"><a href="#1-3-Shallow-Neural-Networks" class="headerlink" title="1.3 Shallow Neural Networks"></a>1.3 Shallow Neural Networks</h3><h4 id="1-3-1-Neural-Network-Representation"><a href="#1-3-1-Neural-Network-Representation" class="headerlink" title="1.3.1 Neural Network Representation"></a>1.3.1 Neural Network Representation</h4><ul>
<li><p><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/11/20/EHtqxOI6c5NQuG8.jpg" alt="deep-learning-notes_1-3-1" data-caption="deep-learning-notes_1-3-1" loading="lazy"></p>
</li>
<li><p>Input layer, hidden layer, output layer</p>
<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">a^{[0]}=x</annotation></semantics></math></span> -> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mo stretchy="false">[</mo><mo stretchy="false">[</mo><msubsup><mi>a</mi><mn>1</mn><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msubsup><mo separator="true">,</mo><msubsup><mi>a</mi><mn>2</mn><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msubsup><mo separator="true">,</mo><msubsup><mi>a</mi><mn>3</mn><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msubsup><mo separator="true">,</mo><msubsup><mi>a</mi><mn>4</mn><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msubsup><mo stretchy="false">]</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">a^{[1]}=[[a^{[1]}_1,a^{[1]}_2,a^{[1]}_3,a^{[1]}_4]]</annotation></semantics></math></span> -> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">a^{[2]}</annotation></semantics></math></span></li>
<li>Layers count by # of hidden layer+# of output layer.</li>
</ul>
</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">x_1,x_2,x_3</annotation></semantics></math></span> -> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mtext> </mtext><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi><mtext> </mtext><mi>n</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">4\space hidden\space nodes</annotation></semantics></math></span> -> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi><mtext> </mtext><mi>l</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi></mrow><annotation encoding="application/x-tex">Output\space layer</annotation></semantics></math></span>
<ul>
<li>First hidden node: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>1</mn><mo>=</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mi>T</mi></mrow></msup><mi mathvariant="normal">_</mi><mn>1</mn><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>1</mn><mo separator="true">,</mo><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>1</mn><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">z^{[1]}\_1=w^{[1]T}\_1+b^{[1]}\_1, a^{[1]}\_1=\sigma(z^{[1]}\_1)</annotation></semantics></math></span></li>
<li>Seconde hidden node: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>2</mn><mo>=</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mi>T</mi></mrow></msup><mi mathvariant="normal">_</mi><mn>2</mn><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>2</mn><mo separator="true">,</mo><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>2</mn><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>2</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">z^{[1]}\_2=w^{[1]T}\_2+b^{[1]}\_2, a^{[1]}\_2=\sigma(z^{[1]}\_2)</annotation></semantics></math></span></li>
<li>Third hidden node: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>3</mn><mo>=</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mi>T</mi></mrow></msup><mi mathvariant="normal">_</mi><mn>3</mn><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>3</mn><mo separator="true">,</mo><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>3</mn><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>3</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">z^{[1]}\_3=w^{[1]T}\_3+b^{[1]}\_3, a^{[1]}\_3=\sigma(z^{[1]}\_3)</annotation></semantics></math></span></li>
<li>Forth hidden node: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>4</mn><mo>=</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mi>T</mi></mrow></msup><mi mathvariant="normal">_</mi><mn>4</mn><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>4</mn><mo separator="true">,</mo><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>4</mn><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>4</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">z^{[1]}\_4=w^{[1]T}\_4+b^{[1]}\_4, a^{[1]}\_4=\sigma(z^{[1]}\_4)</annotation></semantics></math></span></li>
</ul>
</li>
<li><p>Vectorization</p>
<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mtable rowspacing="0.25em" columnalign="center" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mi>T</mi></mrow></msup><mi mathvariant="normal">_</mi><mn>1</mn><mo>−</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mi>T</mi></mrow></msup><mi mathvariant="normal">_</mi><mn>2</mn><mo>−</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mi>T</mi></mrow></msup><mi mathvariant="normal">_</mi><mn>3</mn><mo>−</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mi>T</mi></mrow></msup><mi mathvariant="normal">_</mi><mn>4</mn><mo>−</mo></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mstyle></mtd></mtr></mtable><mo stretchy="false">(</mo><mn>4</mn><mo separator="true">,</mo><mn>3</mn><mo stretchy="false">)</mo><mi>m</mi><mi>a</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">w^{[1]}=\begin{gathered}\begin{bmatrix}-w^{[1]T}\_1- \\ -w^{[1]T}\_2- \\ -w^{[1]T}\_3- \\ -w^{[1]T}\_4- \end{bmatrix}\end{gathered} (4,3)matrix</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mtable rowspacing="0.25em" columnalign="center" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mi>T</mi></mrow></msup><mi mathvariant="normal">_</mi><mn>1</mn><mo>−</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mi>T</mi></mrow></msup><mi mathvariant="normal">_</mi><mn>2</mn><mo>−</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mi>T</mi></mrow></msup><mi mathvariant="normal">_</mi><mn>3</mn><mo>−</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mi>T</mi></mrow></msup><mi mathvariant="normal">_</mi><mn>4</mn><mo>−</mo></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mstyle></mtd></mtr></mtable><mo>⋅</mo><mtable rowspacing="0.25em" columnalign="center" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mn>1</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mn>2</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mn>3</mn></msub></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mstyle></mtd></mtr></mtable><mo>+</mo><mtable rowspacing="0.25em" columnalign="center" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>1</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>2</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>3</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>4</mn></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mstyle></mtd></mtr></mtable><mo>=</mo><mtable rowspacing="0.25em" columnalign="center" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mi>T</mi></mrow></msup><mi mathvariant="normal">_</mi><mn>1</mn><mo>⋅</mo><mi>x</mi><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>1</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mi>T</mi></mrow></msup><mi mathvariant="normal">_</mi><mn>2</mn><mo>⋅</mo><mi>x</mi><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>2</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mi>T</mi></mrow></msup><mi mathvariant="normal">_</mi><mn>3</mn><mo>⋅</mo><mi>x</mi><mo>+</mo><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>3</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mi>T</mi></mrow></msup><mi mathvariant="normal">_</mi><mn>4</mn><mo>⋅</mo><mi>x</mi><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>4</mn></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mstyle></mtd></mtr></mtable><mo>=</mo><mtable rowspacing="0.25em" columnalign="center" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>1</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>2</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>3</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>4</mn></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mstyle></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">z^{[1]}=\begin{gathered}\begin{bmatrix}-w^{[1]T}\_1- \\ -w^{[1]T}\_2- \\ -w^{[1]T}\_3- \\ -w^{[1]T}\_4- \end{bmatrix}\end{gathered}\cdot \begin{gathered}\begin{bmatrix}x_1 \\ x_2 \\ x_3 \end{bmatrix}\end{gathered} + \begin{gathered}\begin{bmatrix}b^{[1]}\_1 \\ b^{[1]}\_2 \\b^{[1]}\_3 \\ b^{[1]}\_4 \end{bmatrix}\end{gathered} =\begin{gathered}\begin{bmatrix}w^{[1]T}\_1\cdot x+b^{[1]}\_1 \\ w^{[1]T}\_2\cdot x+b^{[1]}\_2 \\ w^{[1]T}\_3\cdot x++b^{[1]}\_3 \\ w^{[1]T}\_4\cdot x+b^{[1]}\_4 \end{bmatrix}\end{gathered}=\begin{gathered}\begin{bmatrix}z^{[1]}\_1 \\ z^{[1]}\_2 \\z^{[1]}\_3 \\ z^{[1]}\_4 \end{bmatrix}\end{gathered}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mtable rowspacing="0.25em" columnalign="center" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>1</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>2</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>3</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mn>4</mn></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mstyle></mtd></mtr></mtable><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a^{[1]}=\begin{gathered}\begin{bmatrix}a^{[1]}\_1 \\ a^{[1]}\_2 \\a^{[1]}\_3 \\ a^{[1]}\_4 \end{bmatrix}\end{gathered}=\sigma(z^{[1]})</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>W</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>⋅</mo><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">z^{[2]}=W^{[2]}\cdot a^{[1]}+b^{[2]}</annotation></semantics></math></span> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo><mo separator="true">,</mo><mo stretchy="false">(</mo><mn>1</mn><mo separator="true">,</mo><mn>4</mn><mo stretchy="false">)</mo><mo separator="true">,</mo><mo stretchy="false">(</mo><mn>4</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo><mo separator="true">,</mo><mo stretchy="false">(</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(1, 1),(1, 4),(4, 1),(1, 1)</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a^{[2]}=\sigma(z^{[2]})</annotation></semantics></math></span> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo><mo separator="true">,</mo><mo stretchy="false">(</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(1,1),(1,1)</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">a^{[2](i)}</annotation></semantics></math></span>: layer <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span>; example <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span></li>
</ul>
</li>
<li><p>for i&#x3D;1 to m:</p>
<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><msup><mi>W</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>⋅</mo><mi>x</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">z^{[1](i)}=W^{[1]}\cdot x(i)+b^{[1]}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a^{[1](i)}=\sigma(z^{[1](i)})</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><msup><mi>W</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>⋅</mo><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">z^{[2](i)}=W^{[2]}\cdot a^{[1](i)}+b^{[2]}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a^{[2](i)}=\sigma(z^{[2](i)})</annotation></semantics></math></span></li>
</ul>
</li>
<li><p>Vectorizing of the above for loop</p>
<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mtable rowspacing="0.25em" columnalign="center" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi mathvariant="normal">∣</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mi mathvariant="normal">∣</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mi mathvariant="normal">∣</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mi mathvariant="normal">∣</mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msup></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi mathvariant="normal">∣</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mi mathvariant="normal">∣</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mi mathvariant="normal">∣</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mi mathvariant="normal">∣</mi></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mstyle></mtd></mtr></mtable><mo stretchy="false">(</mo><msub><mi>n</mi><mi>x</mi></msub><mo separator="true">,</mo><mi>m</mi><mo stretchy="false">)</mo><mi>m</mi><mi>a</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">X=\begin{gathered}\begin{bmatrix}| &amp; | &amp; | &amp; | \\ x^{(1)}, &amp; x^{(2)}, &amp; ..., &amp; x^{(m)} \\ | &amp; | &amp; | &amp; |\end{bmatrix}\end{gathered} (n_x,m)matrix</annotation></semantics></math></span> n is different hidden units</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>W</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>⋅</mo><mi>X</mi><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">Z^{[1]}=W^{[1]}\cdot X+b^{[1]}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">A^{[1]}=\sigma(Z^{[1]})</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>W</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>⋅</mo><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">Z^{[2]}=W^{[2]}\cdot A^{[1]}+b^{[2]}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">A^{[2]}=\sigma(Z^{[2]})</annotation></semantics></math></span></li>
<li>hrizontally: training examples; vertically: hidden units</li>
</ul>
</li>
</ul>
<h4 id="1-3-2-Activation-Functions"><a href="#1-3-2-Activation-Functions" class="headerlink" title="1.3.2 Activation Functions"></a>1.3.2 Activation Functions</h4><ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>g</mi><mrow><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">g^{[i]}</annotation></semantics></math></span>: activation function of layer <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span>
<ul>
<li>Sigmoid: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo stretchy="false">[</mo><mo>−</mo><mi>z</mi><mo stretchy="false">]</mo></mrow></msup></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">a=\dfrac{1}{1+e^{[-z]}}</annotation></semantics></math></span></li>
<li>Tanh: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><msup><mi>e</mi><mi>z</mi></msup><mo>−</mo><msup><mi>e</mi><mrow><mo stretchy="false">[</mo><mo>−</mo><mi>z</mi><mo stretchy="false">]</mo></mrow></msup></mrow><mrow><msup><mi>e</mi><mi>z</mi></msup><mo>+</mo><msup><mi>e</mi><mrow><mo stretchy="false">[</mo><mo>−</mo><mi>z</mi><mo stretchy="false">]</mo></mrow></msup></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">a=\dfrac{e^z-e^{[-z]}}{e^z+e^{[-z]}}</annotation></semantics></math></span></li>
<li>ReLU: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a=max(0,z)</annotation></semantics></math></span></li>
<li>Leaky ReLu: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mn>0.01</mn><mi>z</mi><mo separator="true">,</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a=max(0.01z, z)</annotation></semantics></math></span></li>
</ul>
</li>
<li>Rules to choose activation function<ol>
<li>Output is between {0, 1}, choose sigmoid.</li>
<li>Default choose ReLu.</li>
</ol>
</li>
<li>Why need non-liner activation function<ul>
<li>Use linear hidden layer will be useless to have multiple hidden layers. It will become <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>=</mo><msup><mi>w</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mi>x</mi><mo>+</mo><msup><mi>b</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">a=w&#x27;x+b&#x27;</annotation></semantics></math></span>.</li>
<li>Linear may sometime use at output layer but with non-linear at hidden layers.</li>
</ul>
</li>
</ul>
<h4 id="1-3-3-Forward-and-Backward-Propogation"><a href="#1-3-3-Forward-and-Backward-Propogation" class="headerlink" title="1.3.3 Forward and Backward Propogation"></a>1.3.3 Forward and Backward Propogation</h4><ul>
<li>Derivative of activation function<ul>
<li>Sigmoid: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>g</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mi>d</mi><mrow><mi>d</mi><mi>z</mi></mrow></mfrac></mstyle><mi>g</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo stretchy="false">[</mo><mo>−</mo><mi>z</mi><mo stretchy="false">]</mo></mrow></msup></mrow></mfrac></mstyle><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo stretchy="false">[</mo><mo>−</mo><mi>z</mi><mo stretchy="false">]</mo></mrow></msup></mrow></mfrac></mstyle><mo stretchy="false">)</mo><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>g</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mi>a</mi><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g&#x27;(z)=\dfrac{d}{dz}g(z)=\dfrac{1}{1+e^{[-z]}}(1-\dfrac{1}{1+e^{[-z]}})=g(z)(1-g(z))=a(1-a)</annotation></semantics></math></span></li>
<li>Tanh: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>g</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mi>d</mi><mrow><mi>d</mi><mi>z</mi></mrow></mfrac></mstyle><mi>g</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn><mo>−</mo><mo stretchy="false">(</mo><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">g&#x27;(z)=\dfrac{d}{dz}g(z)=1-(tanh(z))^2</annotation></semantics></math></span></li>
<li>ReLU: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>g</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.16em" columnalign="left right" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>i</mi><mi>f</mi><mtext> </mtext><mi>z</mi><mo>&lt;</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>i</mi><mi>f</mi><mtext> </mtext><mi>z</mi><mo>≥</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mstyle mathcolor="#cc0000"><mtext>\usepackage</mtext></mstyle><mrow><mi>u</mi><mi>n</mi><mi>d</mi><mi>e</mi><mi>f</mi><mi>i</mi><mi>n</mi><mi>e</mi><mi>d</mi></mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mstyle mathcolor="#cc0000"><mtext>\usepackage</mtext></mstyle><mrow><mi>i</mi><mi>f</mi><mtext> </mtext><mi>z</mi><mo>=</mo><mn>0</mn></mrow></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">g&#x27;(z)=\left\{\begin{array}{lr}0&amp;if \space z&lt;0 \\1&amp;if \space z\geq0\\\usepackage{undefined}&amp;\usepackage{if \space z=0}\end{array}\right.</annotation></semantics></math></span></li>
<li>Leaky ReLU: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>g</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.16em" columnalign="left right" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0.01</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>i</mi><mi>f</mi><mtext> </mtext><mi>z</mi><mo>&lt;</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>i</mi><mi>f</mi><mtext> </mtext><mi>z</mi><mo>≥</mo><mn>0</mn></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">g&#x27;(z)=\left\{\begin{array}{lr}0.01&amp;if \space z&lt;0 \\1&amp;if \space z\geq0\end{array}\right.</annotation></semantics></math></span></li>
</ul>
</li>
<li>Gradient descent for neural networks<ul>
<li>Parameters: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">(</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo><mo separator="true">,</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">(</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo><mo separator="true">,</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">(</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo><mo separator="true">,</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">(</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">w^{[1]}(n^{[1]},n^{[2]}), b^{[1]}(n^{[2]},1),w^{[2]}(n^{[2]},n^{[1]}), b^{[2]}(n^{[2]},1)</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mi>x</mi></msub><mo>=</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n_x=n^{[0]},n^{[1]},n^{[2]}=1</annotation></semantics></math></span></li>
<li>Cost function: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mo>∑</mo><mi mathvariant="normal">_</mi><msup><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msup><mi>L</mi><mo stretchy="false">(</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">J(w^{[1]}, b^{[1]},w^{[2]}, b^{[2]})=\dfrac{1}{m}\sum\limits\_{i=1}^nL(\hat{y},y)</annotation></semantics></math></span></li>
</ul>
</li>
<li>Forward propagation:<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>W</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>⋅</mo><mi>X</mi><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">Z^{[1]}=W^{[1]}\cdot X+b^{[1]}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>g</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">(</mo><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">A^{[1]}=g^{[1]}(Z^{[1]})</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>W</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>⋅</mo><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">Z^{[2]}=W^{[2]}\cdot A^{[1]}+b^{[2]}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>g</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">(</mo><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">A^{[2]}=g^{[2]}(Z^{[2]})=\sigma(Z^{[2]})</annotation></semantics></math></span></li>
</ul>
</li>
<li>Back Propogation:<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>−</mo><mi>Y</mi></mrow><annotation encoding="application/x-tex">dZ^{[2]}=A^{[2]}-Y</annotation></semantics></math></span> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>=</mo><mo stretchy="false">[</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">Y=[y^{(1)},y^{(2)},...,y^{(m)}]</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>W</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mi>d</mi><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mi>T</mi></mrow></msup></mrow><annotation encoding="application/x-tex">dW^{[2]}=\dfrac{1}{m}dZ^{[2]}A^{[1]T}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mi>n</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>s</mi><mi>u</mi><mi>m</mi><mo stretchy="false">(</mo><mi>d</mi><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mi>a</mi><mi>x</mi><mi>i</mi><mi>s</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mi>k</mi><mi>e</mi><mi>e</mi><mi>p</mi><mi>d</mi><mi>i</mi><mi>m</mi><mi>s</mi><mo>=</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">db^{[2]}=\dfrac{1}{m}np.sum(dZ^{[2]},axis=1,keepdims=True)</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>W</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo><mi>T</mi></mrow></msup><mi>d</mi><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mstyle mathcolor="#cc0000"><mtext>\*</mtext></mstyle><msup><mi>g</mi><mrow><mo mathvariant="normal">′</mo><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow></msup><mo stretchy="false">(</mo><msup><mi>Z</mi><mn>1</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">dZ^{[1]}=W^{[2]T}dZ^{[2]}\*g&#x27;^{[1]}(Z^{1})</annotation></semantics></math></span>
<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mi>m</mi><mo stretchy="false">)</mo><mo>−</mo><mo>&gt;</mo><mi>e</mi><mi>l</mi><mi>e</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mo>−</mo><mi>w</mi><mi>i</mi><mi>s</mi><mi>e</mi><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>d</mi><mi>u</mi><mi>c</mi><mi>t</mi><mo>−</mo><mo>&gt;</mo><mo stretchy="false">(</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mi>m</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(n^{[1]},m)-&gt;element-wise\space product-&gt;(n^{[1]},m)</annotation></semantics></math></span></li>
</ul>
</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>W</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mi>d</mi><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><msup><mi>X</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">dW^{[1]}=\dfrac{1}{m}dZ^{[1]}X^{T}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mi>n</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>s</mi><mi>u</mi><mi>m</mi><mo stretchy="false">(</mo><mi>d</mi><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mi>a</mi><mi>x</mi><mi>i</mi><mi>s</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mi>k</mi><mi>e</mi><mi>e</mi><mi>p</mi><mi>d</mi><mi>i</mi><mi>m</mi><mi>s</mi><mo>=</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">db^{[1]}=\dfrac{1}{m}np.sum(dZ^{[1]},axis=1,keepdims=True)</annotation></semantics></math></span></li>
</ul>
</li>
<li>Random Initialization<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>−</mo><mo>&gt;</mo><msubsup><mi>a</mi><mn>1</mn><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msubsup><mo separator="true">,</mo><msubsup><mi>a</mi><mn>2</mn><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msubsup><mo>−</mo><mo>&gt;</mo><msubsup><mi>a</mi><mn>1</mn><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msubsup><mo>−</mo><mo>&gt;</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">x_1,x_2-&gt;a_1^{[1]},a_2^{[1]}-&gt;a_1^{[2]}-&gt;\hat{y}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mi>n</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>o</mi><mi>m</mi><mi mathvariant="normal">.</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>n</mi><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mn>2</mn><mo separator="true">,</mo><mn>2</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mstyle mathcolor="#cc0000"><mtext>\*</mtext></mstyle><mn>0.01</mn></mrow><annotation encoding="application/x-tex">w^{[1]}=np.random.randn((2,2))\*0.01</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mi>n</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>z</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>s</mi><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mn>2</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">b^{[1]}=np.zeros((2,1))</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mi>n</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>o</mi><mi>m</mi><mi mathvariant="normal">.</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>n</mi><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mstyle mathcolor="#cc0000"><mtext>\*</mtext></mstyle><mn>0.01</mn></mrow><annotation encoding="application/x-tex">w^{[2]}=np.random.randn((1,2))\*0.01</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">b^{[2]}=0</annotation></semantics></math></span></li>
</ul>
</li>
</ul>
<h3 id="1-4-Deep-Neural-Networks"><a href="#1-4-Deep-Neural-Networks" class="headerlink" title="1.4 Deep Neural Networks"></a>1.4 Deep Neural Networks</h3><h4 id="1-4-1-Deep-L-Layer-Neural-Network"><a href="#1-4-1-Deep-L-Layer-Neural-Network" class="headerlink" title="1.4.1 Deep L-Layer Neural Network"></a>1.4.1 Deep L-Layer Neural Network</h4><ul>
<li>Deep neural network notation<ul>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/11/20/9w2GKxylSrjfoFq.jpg" alt="deep-learning-notes_1-4-1" data-caption="deep-learning-notes_1-4-1" loading="lazy"></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">L=4</annotation></semantics></math></span> (#layers)</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mi mathvariant="normal">#</mi><mtext> </mtext><mi>u</mi><mi>n</mi><mi>i</mi><mi>t</mi><mi>s</mi><mtext> </mtext><mi>i</mi><mi>n</mi><mtext> </mtext><mi>l</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mtext> </mtext><mi>l</mi></mrow><annotation encoding="application/x-tex">n^{[l]}= \#\space units\space in\space layer\space l </annotation></semantics></math></span>
<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mn>5</mn><mo separator="true">,</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mn>5</mn><mo separator="true">,</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mn>3</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mn>3</mn><mo separator="true">,</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mn>4</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n^{[1]}=5,n^{[2]}=5,n^{[3]}=3,n^{[4]}=n^{[l]}=1</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msub><mi>n</mi><mi>x</mi></msub><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">n^{[0]}=n_x=3</annotation></semantics></math></span></li>
</ul>
</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mi>a</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>s</mi><mtext> </mtext><mi>i</mi><mi>n</mi><mtext> </mtext><mi>l</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mtext> </mtext><mi>l</mi></mrow><annotation encoding="application/x-tex">a^{[l]}=activations\space in\space layer\space l</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>g</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo><mo separator="true">,</mo><mtext> </mtext><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mi>w</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>s</mi><mtext> </mtext><mi>f</mi><mi>o</mi><mi>r</mi><mtext> </mtext><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mtext> </mtext><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mi>b</mi><mi>i</mi><mi>a</mi><mi>s</mi><mtext> </mtext><mi>f</mi><mi>o</mi><mi>r</mi><mtext> </mtext><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">a^{[l]}=g^{[l]}(z^{[l]}),\space w^{[l]}=weights\space for\space z^{[l]},\space b^{[l]}=bias\space for\space z^{[l]}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mtext> </mtext><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><msup><mi>a</mi><mi>l</mi></msup></mrow><annotation encoding="application/x-tex">x=a^{[0]},\space \hat{y}=a^{l}</annotation></semantics></math></span></li>
</ul>
</li>
</ul>
<h4 id="1-4-2-Forward-Propagation-in-a-Deep-Network"><a href="#1-4-2-Forward-Propagation-in-a-Deep-Network" class="headerlink" title="1.4.2 Forward Propagation in a Deep Network"></a>1.4.2 Forward Propagation in a Deep Network</h4><ul>
<li>General: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>g</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">(</mo><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Z^{[l]}=w^{[l]}A^{[l-1]}+b^{[l]}, A^{[l]}=g^{[l]}(Z^{[l]})</annotation></semantics></math></span><ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>:</mo><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>g</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x: z^{[1]}=w^{[1]}a^{[0]}+b^{[1]}, a^{[1]}=g^{[1]}(z^{[1]})</annotation></semantics></math></span> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mi>X</mi></mrow><annotation encoding="application/x-tex">a^{[0]}=X</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>g</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">z^{[2]}=w^{[2]}a^{[1]}+b^{[2]}, a^{[1]}=g^{[2]}(z^{[2]})</annotation></semantics></math></span></li>
<li>…</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>4</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>4</mn><mo stretchy="false">]</mo></mrow></msup><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>3</mn><mo stretchy="false">]</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>4</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mn>4</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>g</mi><mrow><mo stretchy="false">[</mo><mn>4</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>4</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo><mo>=</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">z^{[4]}=w^{[4]}a^{[3]}+b^{[4]}, a^{[4]}=g^{[4]}(z^{[4]})=\hat{y}</annotation></semantics></math></span></li>
</ul>
</li>
<li>Vectorizing:<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>g</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">(</mo><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Z^{[1]}=w^{[1]}A^{[0]}+b^{[1]}, A^{[1]}=g^{[1]}(Z^{[1]})</annotation></semantics></math></span> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mi>X</mi></mrow><annotation encoding="application/x-tex">A^{[0]}=X</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>g</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">(</mo><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Z^{[2]}=w^{[2]}A^{[1]}+b^{[2]}, A^{[2]}=g^{[2]}(Z^{[2]})</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mn>4</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo><mo>=</mo><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mn>4</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\hat{Y}=g(Z^{[4]})=A^{[4]}</annotation></semantics></math></span></li>
</ul>
</li>
<li>Matrix dimensions<ul>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/11/20/a6emOyncIbRlo8w.jpg" alt="deep-learning-notes_1-4-2" data-caption="deep-learning-notes_1-4-2" loading="lazy"></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>⋅</mo><mi>x</mi><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">z^{[1]}=w^{[1]}\cdot x+b^{[1]}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mo stretchy="false">(</mo><mn>3</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo><mo separator="true">,</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mo stretchy="false">(</mo><mn>3</mn><mo separator="true">,</mo><mn>2</mn><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>x</mi><mo>=</mo><mo stretchy="false">(</mo><mn>2</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo><mo separator="true">,</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mo stretchy="false">(</mo><mn>3</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">z^{[1]}=(3,1),w^{[1]}=(3,2),x=(2,1),b^{[1]}=(3,1)</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mo stretchy="false">(</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo><mo separator="true">,</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mo stretchy="false">(</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>x</mi><mo>=</mo><mo stretchy="false">(</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo><mo separator="true">,</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mo stretchy="false">(</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">z^{[1]}=(n^{[1]},1),w^{[1]}=(n^{[1]},n^{[0]}),x=(n^{[0]},1),b^{[1]}=(n^{[1]},1)</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">/</mi><mi>d</mi><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mo stretchy="false">(</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo><mo separator="true">,</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">/</mi><mi>d</mi><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mo stretchy="false">(</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">w^{[l]}/dw^{[l]}=(n^{[l]},n^{[l-1]}),b^{[l]}/db^{[l]}=(n^{[l]},1)</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mo stretchy="false">(</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo><mo separator="true">,</mo><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">/</mi><mi>d</mi><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">/</mi><mi>d</mi><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mo stretchy="false">(</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">z^{[l]},a^{[l]}=(n^{[l]},1),Z^{[l]}/dZ^{[l]},A^{[l]}/dA^{[l]}=(n^{[l]},1)</annotation></semantics></math></span> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mo>=</mo><mn>0</mn><mo separator="true">,</mo><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mi>X</mi><mo>=</mo><mo stretchy="false">(</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mi>m</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">l=0, A^{[0]}=X=(n^{[0]},m)</annotation></semantics></math></span></li>
</ul>
</li>
<li>Why deep representation?<ul>
<li>Earier layers learn simple features; later deeper layers put together to detect more complex things.</li>
<li>Circuit theory and deep learning: Informally: There are functions you can compute with a “small” L-layer deep neural network that shallower networks require exponentially more hidden units to compute.</li>
</ul>
</li>
</ul>
<h4 id="1-4-3-Building-Blocks-of-Deep-Neural-Networks"><a href="#1-4-3-Building-Blocks-of-Deep-Neural-Networks" class="headerlink" title="1.4.3 Building Blocks of Deep Neural Networks"></a>1.4.3 Building Blocks of Deep Neural Networks</h4><ul>
<li>Forward and backward functions<ul>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/11/20/PBfq6ISu2h8vKeV.jpg" alt="deep-learning-notes_1-4-3" data-caption="deep-learning-notes_1-4-3" loading="lazy"></li>
<li>Layer <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mo>:</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">l:w^{[l]},b^{[l]}</annotation></semantics></math></span></li>
<li>Forward: Input <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">a^{[l-1]}</annotation></semantics></math></span>, output <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">a^{[l]}</annotation></semantics></math></span><ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>:</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">z^{[l]}:w^{[l]}a^{[l-1]}+b^{[l]}</annotation></semantics></math></span> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mi>a</mi><mi>c</mi><mi>h</mi><mi>e</mi><mtext> </mtext><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">cache\space z^{[l]}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>:</mo><msup><mi>g</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a^{[l]}:g^{[l]}(z^{[l]})</annotation></semantics></math></span></li>
</ul>
</li>
<li>Backward: Input <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mi>c</mi><mi>a</mi><mi>c</mi><mi>h</mi><mi>e</mi><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">da^{[l]}, cache(z^{[l]})</annotation></semantics></math></span>, output <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mi>d</mi><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mi>d</mi><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">da^{[l-1]},dw^{[l]},db^{[l]}</annotation></semantics></math></span></li>
</ul>
</li>
<li>One iteration of gradient descent of neural network<ul>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/11/20/35UIkJnmlSHt82j.jpg" alt="deep-learning-notes_1-4-3-2" data-caption="deep-learning-notes_1-4-3-2" loading="lazy"></li>
</ul>
</li>
<li>How to implement?<ul>
<li>Forward propagation for layer <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><ul>
<li>Input <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">a^{[l-1]}</annotation></semantics></math></span>, output <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mi>c</mi><mi>a</mi><mi>c</mi><mi>h</mi><mi>e</mi><mtext> </mtext><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a^{[l]},cache\space (z^{[l]})</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>g</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a^{[l]}=g^{[l]}(z^{[l]})</annotation></semantics></math></span></li>
<li>Vectoried<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>W</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>g</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">(</mo><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">A^{[l]}=g^{[l]}(Z^{[l]})</annotation></semantics></math></span></li>
</ul>
</li>
</ul>
</li>
<li>Backward propagation for layer <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><ul>
<li>Input <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mi>c</mi><mi>a</mi><mi>c</mi><mi>h</mi><mi>e</mi><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">da^{[l]}, cache(z^{[l]})</annotation></semantics></math></span>, output <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mi>d</mi><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mi>d</mi><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">da^{[l-1]},dw^{[l]},db^{[l]}</annotation></semantics></math></span><ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mi>d</mi><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mstyle mathcolor="#cc0000"><mtext>\*</mtext></mstyle><msup><mi>g</mi><mrow><mo mathvariant="normal">′</mo><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></mrow></msup><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">dz^{[l]}=da^{[l]}\*g&#x27;^{[l]}(z^{[l]})</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mi>d</mi><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>⋅</mo><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">dw^{[l]}=dz^{[l]}\cdot a^{[l-1]}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mi>d</mi><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">db^{[l]}=dz^{[l]}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo><mi>T</mi></mrow></msup><mo>⋅</mo><mi>d</mi><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">da^{[l-1]}=w^{[l]T}\cdot dz^{[l]}</annotation></semantics></math></span></li>
</ul>
</li>
<li>Vectorized:<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mi>d</mi><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mstyle mathcolor="#cc0000"><mtext>\*</mtext></mstyle><msup><mi>g</mi><mrow><mo mathvariant="normal">′</mo><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></mrow></msup><mo stretchy="false">(</mo><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">dZ^{[l]}=dA^{[l]}\*g&#x27;^{[l]}(Z^{[l]})</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>W</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mi>d</mi><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo><mi>T</mi></mrow></msup></mrow><annotation encoding="application/x-tex">dW^{[l]}=\dfrac{1}{m}dZ^{[l]}A^{[l-1]T}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mi>n</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>s</mi><mi>u</mi><mi>m</mi><mo stretchy="false">(</mo><mi>d</mi><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mi>a</mi><mi>x</mi><mi>i</mi><mi>s</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mi>k</mi><mi>e</mi><mi>e</mi><mi>p</mi><mi>d</mi><mi>i</mi><mi>m</mi><mi>s</mi><mo>=</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">db^{[l]}=\dfrac{1}{m}np.sum(dZ^{[l]},axis=1,keepdims=True)</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>W</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo><mi>T</mi></mrow></msup><mo>⋅</mo><mi>d</mi><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">dA^{[l-1]}=W^{[l]T}\cdot dZ^{[l]}</annotation></semantics></math></span></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="1-4-4-Parameters-vs-Hyperparameters"><a href="#1-4-4-Parameters-vs-Hyperparameters" class="headerlink" title="1.4.4 Parameters vs. Hyperparameters"></a>1.4.4 Parameters vs. Hyperparameters</h4><ul>
<li>Parameters: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>W</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>W</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]},...</annotation></semantics></math></span></li>
<li>Hyperparameters (will affect&#x2F;control&#x2F;determine parameters):<ul>
<li>learning rate <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span></li>
<li># iterations</li>
<li># of hidden units <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">n^{[1]},n^{[2]},...</annotation></semantics></math></span></li>
<li># of hidden layers</li>
<li>Choice of activation function</li>
</ul>
</li>
<li>Later: momemtum, minibatch size, regularization parameters,…</li>
</ul>
<h2 id="II-Improving-Deep-Neural-Networks-Hyperparameter-Tuning-Regularization-and-Optimization"><a href="#II-Improving-Deep-Neural-Networks-Hyperparameter-Tuning-Regularization-and-Optimization" class="headerlink" title="II. Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization"></a>II. Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization</h2><h3 id="2-1-Practical-Aspects-of-Deep-Learning"><a href="#2-1-Practical-Aspects-of-Deep-Learning" class="headerlink" title="2.1 Practical Aspects of Deep Learning"></a>2.1 Practical Aspects of Deep Learning</h3><h4 id="2-1-1-Train-Dev-Test-sets"><a href="#2-1-1-Train-Dev-Test-sets" class="headerlink" title="2.1.1 Train &#x2F; Dev &#x2F; Test sets"></a>2.1.1 Train &#x2F; Dev &#x2F; Test sets</h4><ul>
<li>Big data may need only 1% or even less dev&#x2F;test sets.</li>
<li>Mismatched: Make sure dev&#x2F;test come from same distribution</li>
<li>Not having a test set might be okay. (Only dev set.)</li>
</ul>
<h4 id="2-1-2-Bias-Variance"><a href="#2-1-2-Bias-Variance" class="headerlink" title="2.1.2 Bias &#x2F; Variance"></a>2.1.2 Bias &#x2F; Variance</h4><p><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/11/21/ROywuJQ3dNzfhBo.jpg" alt="deep-learning-notes_2-1-2" data-caption="deep-learning-notes_2-1-2" loading="lazy"></p>
<p><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/11/21/S4cZ3Aua8es7WfG.jpg" alt="deep-learning-notes_2-1-2-2" data-caption="deep-learning-notes_2-1-2-2" loading="lazy"></p>
<ul>
<li>Assume optimal (Bayes) error: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≈</mo><mn>0</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">\approx0\%</annotation></semantics></math></span></li>
<li>High bias (underfitting): The prediction cannot classify different elemets as we want.<ul>
<li>Training set error <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>15</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">15\%</annotation></semantics></math></span>, Dev set error <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">16\%</annotation></semantics></math></span>.</li>
<li>Training set error <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>15</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">15\%</annotation></semantics></math></span>, Dev set error <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>30</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">30\%</annotation></semantics></math></span>.</li>
</ul>
</li>
<li>“just right”: The prediction perfectly classify different elemets as we want.<ul>
<li>Training set error <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.5</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">0.5\%</annotation></semantics></math></span>, Dev set error <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">1\%</annotation></semantics></math></span>.</li>
</ul>
</li>
<li>High variance (overfitting): The prediction 100% classify different elemets.<ul>
<li>Training set error <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">1\%</annotation></semantics></math></span>, Dev set error <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>11</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">11\%</annotation></semantics></math></span>.</li>
<li>Training set error <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>15</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">15\%</annotation></semantics></math></span>, Dev set error <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>30</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">30\%</annotation></semantics></math></span>.</li>
</ul>
</li>
</ul>
<h4 id="2-1-3-Basic-Recipe-for-Machine-Learning"><a href="#2-1-3-Basic-Recipe-for-Machine-Learning" class="headerlink" title="2.1.3 Basic Recipe for Machine Learning"></a>2.1.3 Basic Recipe for Machine Learning</h4><h5 id="2-1-3-1-Basic-Recipe"><a href="#2-1-3-1-Basic-Recipe" class="headerlink" title="2.1.3.1 Basic Recipe"></a>2.1.3.1 Basic Recipe</h5><ul>
<li>High bias(training data performance)<ul>
<li>Bigger network</li>
<li>Train longer</li>
<li>(NN architecture search)</li>
</ul>
</li>
<li>High variance (dev set performance)<ul>
<li>More data</li>
<li>Regulairzation</li>
<li>(NN architecture search)</li>
</ul>
</li>
</ul>
<h5 id="2-1-3-2-Regularization"><a href="#2-1-3-2-Regularization" class="headerlink" title="2.1.3.2 Regularization"></a>2.1.3.2 Regularization</h5><ul>
<li>Logistic regression. <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>min</mi><mo>⁡</mo><mi mathvariant="normal">_</mi><mrow><mi>w</mi><mo separator="true">,</mo><mi>b</mi></mrow><mi>J</mi><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\min\limits\_{w,b}J(w,b)</annotation></semantics></math></span><ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><msub><mi>n</mi><mi>x</mi></msub></msup><mo separator="true">,</mo><mi>b</mi><mo>∈</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">w\in\mathbb{R}^{n_x}, b\in\mathbb{R}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mo>=</mo><mi>r</mi><mi>e</mi><mi>g</mi><mi>u</mi><mi>l</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>z</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mtext> </mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>a</mi><mi>m</mi><mi>e</mi><mi>t</mi><mi>e</mi><mi>r</mi></mrow><annotation encoding="application/x-tex">\lambda=regularization\space parameter</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mo>∑</mo><mi mathvariant="normal">_</mi><msup><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msup><mi>L</mi><mo stretchy="false">(</mo><msup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>+</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mi>λ</mi><mrow><mn>2</mn><mi>m</mi></mrow></mfrac></mstyle><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><msup><mi>w</mi><mn>2</mn></msup><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">_</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">J(w,b)=\dfrac{1}{m}\sum\limits\_{i=1}^mL(\hat{y}^{(i)},y^{(i)})+\dfrac{\lambda}{2m}||w^2||\_2</annotation></semantics></math></span></li>
<li>L2 regularization <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><msup><mi>w</mi><mn>2</mn></msup><mi mathvariant="normal">∣</mi><msub><mi mathvariant="normal">∣</mi><mn>2</mn></msub><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mi>x</mi></msub></msubsup><msubsup><mi>w</mi><mi>j</mi><mn>2</mn></msubsup><mo>=</mo><msup><mi>w</mi><mi>T</mi></msup><mi>w</mi></mrow><annotation encoding="application/x-tex">||w^2||_2=\sum\limits_{j=1}^{n_x}w_j^2=w^Tw</annotation></semantics></math></span></li>
<li>L1 regularization <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mi>λ</mi><mrow><mn>2</mn><mi>m</mi></mrow></mfrac></mstyle><mo>∑</mo><mi mathvariant="normal">_</mi><msup><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mi>x</mi></msub></msup><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mi>j</mi></msub><mi mathvariant="normal">∣</mi><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mi>λ</mi><mrow><mn>2</mn><mi>m</mi></mrow></mfrac></mstyle><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>w</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">_</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">\dfrac{\lambda}{2m}\sum\limits\_{j=1}^{n_x}|w_j|=\dfrac{\lambda}{2m}||w||\_1</annotation></semantics></math></span><ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span> will be spouse(for L1) (will have lots of 0 in it, only help a little bit)</li>
</ul>
</li>
</ul>
</li>
<li>Neural network<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mo>∑</mo><mo>∗</mo><msup><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msup><mi>L</mi><mo stretchy="false">(</mo><msup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>+</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mi>λ</mi><mrow><mn>2</mn><mi>m</mi></mrow></mfrac></mstyle><mo>∑</mo><mo>∗</mo><msup><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></msup><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><msup><mi>w</mi><mn>2</mn></msup><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">_</mi><mi>F</mi></mrow><annotation encoding="application/x-tex">J(w^{[1]},b^{[1]},...,w^{[l]},b^{[l]})=\dfrac{1}{m}\sum\limits*{i=1}^{m}L(\hat{y}^{(i)},y^{(i)})+\dfrac{\lambda}{2m}\sum\limits*{l=1}^{l}||w^2||\_F</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">∣</mi><msubsup><mi mathvariant="normal">∣</mi><mi>F</mi><mn>2</mn></msubsup><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></msubsup><mo>∑</mo><mo>∗</mo><msup><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></msup><mo stretchy="false">(</mo><mi>w</mi><mo>∗</mo><msup><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">||w^{[l]}||_F^2=\sum\limits_{i=1}^{n^{[l-1]}}\sum\limits*{j=1}^{n^{[l]}}(w*{ij}^{[l]})^2</annotation></semantics></math></span> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>:</mo><mo stretchy="false">(</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">w: (w^{[l]},w^{[l-1]})</annotation></semantics></math></span>
<ul>
<li>Frobenius norm: Square root of square sum of all elements in a matrix.</li>
</ul>
</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mo stretchy="false">(</mo><mi>f</mi><mi>r</mi><mi>o</mi><mi>m</mi><mtext> </mtext><mi>b</mi><mi>a</mi><mi>c</mi><mi>k</mi><mi>p</mi><mi>r</mi><mi>o</mi><mi>p</mi><mo stretchy="false">)</mo><mo>+</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mi>λ</mi><mi>m</mi></mfrac></mstyle><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">dw^{[l]}=(from\space backprop)+\dfrac{\lambda}{m}w^{[l]}</annotation></semantics></math></span>
<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>:</mo><mo>=</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>−</mo><mi>α</mi><mi>d</mi><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">w^{[l]}:=w^{[l]}-\alpha dw^{[l]}</annotation></semantics></math></span> (keep the same)</li>
<li>Weight decay<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>:</mo><mo>=</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>−</mo><mi>α</mi><mo stretchy="false">[</mo><mo stretchy="false">(</mo><mi>f</mi><mi>r</mi><mi>o</mi><mi>m</mi><mtext> </mtext><mi>b</mi><mi>a</mi><mi>c</mi><mi>k</mi><mi>p</mi><mi>r</mi><mi>o</mi><mi>p</mi><mo stretchy="false">)</mo><mo>+</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mi>λ</mi><mi>m</mi></mfrac></mstyle><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">w^{[l]}:=w^{[l]}-\alpha[(from\space backprop)+\dfrac{\lambda}{m}w^{[l]}]</annotation></semantics></math></span></li>
<li>​ <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>−</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>α</mi><mi>λ</mi></mrow><mi>m</mi></mfrac></mstyle><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>−</mo><mi>α</mi><mo stretchy="false">(</mo><mi>f</mi><mi>r</mi><mi>o</mi><mi>m</mi><mtext> </mtext><mi>b</mi><mi>a</mi><mi>c</mi><mi>k</mi><mi>p</mi><mi>r</mi><mi>o</mi><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">=w^{[l]}-\dfrac{\alpha\lambda}{m}w^{[l]}-\alpha(from\space backprop)</annotation></semantics></math></span></li>
<li>​ <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>α</mi><mi>λ</mi></mrow><mi>m</mi></mfrac></mstyle><mo stretchy="false">)</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>−</mo><mi>α</mi><mo stretchy="false">(</mo><mi>f</mi><mi>r</mi><mi>o</mi><mi>m</mi><mtext> </mtext><mi>b</mi><mi>a</mi><mi>c</mi><mi>k</mi><mi>p</mi><mi>r</mi><mi>o</mi><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">=(1-\dfrac{\alpha\lambda}{m})w^{[l]}-\alpha(from\space backprop)</annotation></semantics></math></span></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>How does regularization prevent overfitting: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span> bigger <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">w^{[l]}</annotation></semantics></math></span> smaller <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">z^{[l]}</annotation></semantics></math></span> smaller, which will make the activation function nearly linear(take tanh as an example). This will cause the network really hard to draw boundary with curve.</li>
<li>Dropout regularization<ul>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/11/21/WTqeEg4MDPbtK12.jpg" alt="deep-learning-notes_2-1-3-2" data-caption="deep-learning-notes_2-1-3-2" loading="lazy"></li>
<li>Implementing dropout(“Inverted dropout”)<ul>
<li>Illustrate with layer <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">l=3</annotation></semantics></math></span> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mi>e</mi><mi>e</mi><mi>p</mi><mo>−</mo><mi>p</mi><mi>r</mi><mi>o</mi><mi>b</mi><mo>=</mo><mn>0.8</mn></mrow><annotation encoding="application/x-tex">keep-prob=0.8</annotation></semantics></math></span> (means 0.2 chance get dropout&#x2F;be 0 out)</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mn>3</mn><mo>=</mo><mi>n</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>o</mi><mi>m</mi><mi mathvariant="normal">.</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>d</mi><mo stretchy="false">(</mo><mi>a</mi><mn>3.</mn><mi>s</mi><mi>h</mi><mi>a</mi><mi>p</mi><mi>e</mi><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo separator="true">,</mo><mi>a</mi><mn>3.</mn><mi>s</mi><mi>h</mi><mi>a</mi><mi>p</mi><mi>e</mi><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mo stretchy="false">)</mo><mo>&lt;</mo><mi>k</mi><mi>e</mi><mi>e</mi><mi>p</mi><mo>−</mo><mi>p</mi><mi>r</mi><mi>o</mi><mi>b</mi></mrow><annotation encoding="application/x-tex">d3 = np.random.rand(a3.shape[0],a3.shape[1]) &lt; keep-prob</annotation></semantics></math></span> #This will set d3 to be a same shape matrix as a3 with True (1), False (0) value.</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mn>3</mn><mo>=</mo><mi>n</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>m</mi><mi>u</mi><mi>l</mi><mi>t</mi><mi>i</mi><mi>p</mi><mi>l</mi><mi>y</mi><mo stretchy="false">(</mo><mi>a</mi><mn>3</mn><mo separator="true">,</mo><mi>d</mi><mn>3</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a3 = np.multiply(a3, d3)</annotation></semantics></math></span> #a3\*=d3; This will let some neruons been dropout</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mn>3</mn><mi mathvariant="normal">/</mi><mo>=</mo><mi>k</mi><mi>e</mi><mi>e</mi><mi>p</mi><mo>−</mo><mi>p</mi><mi>r</mi><mi>o</mi><mi>b</mi></mrow><annotation encoding="application/x-tex">a3/=keep-prob</annotation></semantics></math></span> #inverted dropout, keep the total avtivation the same before and after dropout.</li>
</ul>
</li>
<li>Why work: Can’t rely on any one feature, so have to spread out weights.(shrink weights)</li>
<li>First make sure the J is decreasing during iteration, then turn on dropout.</li>
</ul>
</li>
<li>Data augmentation<ul>
<li>Image: crop, flop, twist…</li>
</ul>
</li>
<li>Early stopping<ul>
<li>Mid-size <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>w</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">_</mi><msup><mi>F</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">||w||\_F^2</annotation></semantics></math></span></li>
<li>May caused optimize cost function and not overfir at the same time.</li>
</ul>
</li>
<li>Orthogonalization<ul>
<li>Only consider optimize cost function or consider not overfit at one time.</li>
</ul>
</li>
</ul>
<h5 id="2-1-3-3-Setting-up-your-optimization-problem"><a href="#2-1-3-3-Setting-up-your-optimization-problem" class="headerlink" title="2.1.3.3 Setting up your optimization problem"></a>2.1.3.3 Setting up your optimization problem</h5><ul>
<li>Normalizing training sets<ul>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/11/21/jtT9fC5nw2R7xqo.jpg" alt="deep-learning-notes_2-1-3-3" data-caption="deep-learning-notes_2-1-3-3" loading="lazy"></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mtable rowspacing="0.25em" columnalign="center" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mn>1</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mn>2</mn></msub></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mstyle></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">x=\begin{gathered}\begin{bmatrix}x_1 \\ x_2\end{bmatrix}\end{gathered}</annotation></semantics></math></span></li>
<li>Subtract mean:<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mo>∑</mo><mi mathvariant="normal">_</mi><msup><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msup><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\mu=\dfrac{1}{m}\sum\limits\_{i=1}^{m}x^{(i)}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>:</mo><mo>=</mo><mi>x</mi><mo>−</mo><mi>μ</mi></mrow><annotation encoding="application/x-tex">x:=x-\mu</annotation></semantics></math></span></li>
</ul>
</li>
<li>Normalize variance:<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mo>∑</mo><mi mathvariant="normal">_</mi><msup><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msup><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>∗</mo><mo>∗</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">\sigma^2=\dfrac{1}{m}\sum\limits\_{i=1}^{m}x^{(i)}**2</annotation></semantics></math></span> "**" element-wise</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mi mathvariant="normal">/</mi><mo>=</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">x/=\sigma^2</annotation></semantics></math></span></li>
</ul>
</li>
<li>Use same <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo separator="true">,</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\mu,\sigma^2</annotation></semantics></math></span> to normalize test set.</li>
<li>Why normalize inputs?<ul>
<li>When inputs in very different scales will help a lot for performance and gradient descent&#x2F;learning rate.</li>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/11/21/MW7wNCXahR3FZx9.jpg" alt="deep-learning-notes_2-1-3-3-2" data-caption="deep-learning-notes_2-1-3-3-2" loading="lazy"></li>
</ul>
</li>
</ul>
</li>
<li>Vanishing&#x2F;exploding gradients<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>&gt;</mo><mi>I</mi></mrow><annotation encoding="application/x-tex">w^{[l]}&gt;I</annotation></semantics></math></span> Just slightly, will make the gradient increase really fast (exploding).</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>&lt;</mo><mi>I</mi></mrow><annotation encoding="application/x-tex">w^{[l]}&lt;I</annotation></semantics></math></span> Just slightly, will make the gradient decrease really slow (varnishing).</li>
</ul>
</li>
<li>Weight initalization (Single neuron)<ul>
<li>large <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span> (number of input features) –&gt; smaller <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">w_i</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>a</mi><mi>n</mi><mi>c</mi><mi>e</mi><mo stretchy="false">(</mo><mi>w</mi><mo>:</mo><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>n</mi></mfrac></mstyle></mrow><annotation encoding="application/x-tex">Variance(w:)=\dfrac{1}{n}</annotation></semantics></math></span> (sigmoid/tanh) ReLU: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>2</mn><mi>n</mi></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\dfrac{2}{n}</annotation></semantics></math></span> (variance can be a hyperparameter, DO NOT DO THAT)</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mi>n</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>o</mi><mi>m</mi><mi mathvariant="normal">.</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>n</mi><mo stretchy="false">(</mo><mi>s</mi><mi>h</mi><mi>a</mi><mi>p</mi><mi>e</mi><mi>O</mi><mi>f</mi><mi>M</mi><mi>a</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>x</mi><mo stretchy="false">)</mo><mstyle mathcolor="#cc0000"><mtext>\*</mtext></mstyle><mi>n</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>s</mi><mi>q</mi><mi>r</mi><mi>t</mi><mo stretchy="false">(</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mfrac></mstyle><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">w^{[l]}=np.random.randn(shapeOfMatrix)\*np.sqrt(\dfrac{1}{n^{[l-1]}})</annotation></semantics></math></span> ReLU: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>2</mn><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\dfrac{2}{n^{[l-1]}}</annotation></semantics></math></span></li>
<li>Xavier initialization: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mfrac></mstyle><mo stretchy="false">)</mo></mrow></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{\dfrac{1}{n^{[l-1]}})}</annotation></semantics></math></span> Sometime <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>2</mn><mrow><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>+</mo><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow></mfrac></mstyle><mo stretchy="false">)</mo></mrow></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{\dfrac{2}{n^{[l-1]}+n^{[l]}})}</annotation></semantics></math></span></li>
</ul>
</li>
<li>Numerical approximation of gradients<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>θ</mi><mo>+</mo><mi>ϵ</mi><mo stretchy="false">)</mo><mo>−</mo><mi>f</mi><mo stretchy="false">(</mo><mi>θ</mi><mo>−</mo><mi>ϵ</mi><mo stretchy="false">)</mo></mrow><mrow><mn>2</mn><mi>ϵ</mi></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\dfrac{f(\theta+\epsilon)-f(\theta-\epsilon)}{2\epsilon}</annotation></semantics></math></span></li>
</ul>
</li>
<li>Gradient checking (Grad check)<ul>
<li>Take <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>W</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msup><mi>W</mi><mrow><mo stretchy="false">[</mo><mi>L</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>L</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">W^{[1]},b^{[1]},...,W^{[L]},b^{[L]}</annotation></semantics></math></span> and reshape into a big vector <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span>.</li>
<li>Take <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>W</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mi>d</mi><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>d</mi><msup><mi>W</mi><mrow><mo stretchy="false">[</mo><mi>L</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mi>d</mi><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>L</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">dW^{[1]},db^{[1]},...,dW^{[L]},db^{[L]}</annotation></semantics></math></span> and reshape into a big vector <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>θ</mi></mrow><annotation encoding="application/x-tex">d\theta</annotation></semantics></math></span>.</li>
<li>for each i:<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>θ</mi><mi mathvariant="normal">_</mi><mrow><mi>a</mi><mi>p</mi><mi>p</mi><mi>r</mi><mi>o</mi><mi>x</mi></mrow><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>J</mi><mo stretchy="false">(</mo><msub><mi>θ</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>θ</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>+</mo><mi>ϵ</mi><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo stretchy="false">)</mo><mo>−</mo><mi>J</mi><mo stretchy="false">(</mo><msub><mi>θ</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>θ</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>−</mo><mi>ϵ</mi><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo stretchy="false">)</mo></mrow><mrow><mn>2</mn><mi>ϵ</mi></mrow></mfrac></mstyle><mo>≈</mo><mi>d</mi><mi>θ</mi><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>J</mi></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>θ</mi><mi>i</mi></msub></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">d\theta\_{approx}[i]=\dfrac{J(\theta_1,\theta_2,...,\theta_i+\epsilon,...)-J(\theta_1,\theta_2,...,\theta_i-\epsilon,...)}{2\epsilon}\approx d\theta[i]=\dfrac{\partial J}{\partial \theta_i}</annotation></semantics></math></span></li>
<li>Check Euclidean distance <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>d</mi><mi>θ</mi><mo>∗</mo><mrow><mi>a</mi><mi>p</mi><mi>p</mi><mi>r</mi><mi>o</mi><mi>x</mi></mrow><mo>−</mo><mi>d</mi><mi>θ</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">_</mi><mn>2</mn></mrow><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>d</mi><mi>θ</mi><mo>∗</mo><mrow><mi>a</mi><mi>p</mi><mi>p</mi><mi>r</mi><mi>o</mi><mi>x</mi></mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">_</mi><mn>2</mn><mo>+</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>d</mi><mi>θ</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">_</mi><mn>2</mn></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\dfrac{||d\theta*{approx}-d\theta||\_2}{||d\theta*{approx}||\_2+||d\theta||\_2}</annotation></semantics></math></span> (<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">_</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">||.||\_2</annotation></semantics></math></span> is Euclidean norm, sqare root of the sum of all elements’ power of 2)</li>
<li>take <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi><mo>=</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>7</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\epsilon=10^{-7}</annotation></semantics></math></span>, if above Euclidean distance is <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≈</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>7</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\approx10^{-7}</annotation></semantics></math></span> or smaller, is great.</li>
<li>If is <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">10^{-5}</annotation></semantics></math></span> or bigger may need to check.</li>
<li>If is <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">10^{-3}</annotation></semantics></math></span> or bigger may need to worry, maybe a bug. Check which i approx is difference between the real value.</li>
</ul>
</li>
<li>notes:<ul>
<li>Don’t use in training - only to debug.</li>
<li>If algorithm fails grad check, look at components to try to identify bug.</li>
<li>Remember regularization. (include the <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mi>λ</mi><mrow><mn>2</mn><mi>m</mi></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\dfrac{\lambda}{2m}</annotation></semantics></math></span>)</li>
<li>Doesn’t work with dropout. (since is random, implement without dropout)</li>
<li>Run at random initialization; perhaps again after some training. (not work when <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo separator="true">,</mo><mi>b</mi><mo>≈</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">w,b\approx0</annotation></semantics></math></span>)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2-2-Optimization-Algorithms"><a href="#2-2-Optimization-Algorithms" class="headerlink" title="2.2 Optimization Algorithms"></a>2.2 Optimization Algorithms</h3><h4 id="2-2-1-Mini-batch-gradient-descent"><a href="#2-2-1-Mini-batch-gradient-descent" class="headerlink" title="2.2.1 Mini-batch gradient descent"></a>2.2.1 Mini-batch gradient descent</h4><ul>
<li>Batch vs. mini-batch gradient descent<ul>
<li>Normal batch may have large amount of data like millions of elements.<ul>
<li>set <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>=</mo><mn>5</mn><mo separator="true">,</mo><mn>000</mn><mo separator="true">,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">m=5,000,000</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mo stretchy="false">[</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>1000</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>1001</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>2000</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">]</mo><mo stretchy="false">(</mo><msub><mi>n</mi><mi>x</mi></msub><mo separator="true">,</mo><mi>m</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">X=[x^{(1)},x^{(2)},x^{(3)},...,x^{(1000)},x^{(1001)},...,x^{(2000)},...,x^{(m)}] (n_x,m)</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>=</mo><mo stretchy="false">[</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">]</mo><mo stretchy="false">(</mo><mn>1</mn><mo separator="true">,</mo><mi>m</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Y=[y^{(1)},y^{(2)},y^{(3)},...,y^{(m)}] (1,m)</annotation></semantics></math></span></li>
</ul>
</li>
<li>Mini-batches make 1,000 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span> each.<ul>
<li>Mini-batch number <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>:</mo><msup><mi>X</mi><mrow><mo stretchy="false">{</mo><mi>t</mi><mo stretchy="false">}</mo></mrow></msup><mo separator="true">,</mo><msup><mi>Y</mi><mrow><mo stretchy="false">{</mo><mi>t</mi><mo stretchy="false">}</mo></mrow></msup></mrow><annotation encoding="application/x-tex">t:X^{\{t\}},Y^{\{t\}}</annotation></semantics></math></span><ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">x^{(i)}</annotation></semantics></math></span> ith in trainning set, <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">z^{[l]}</annotation></semantics></math></span> layer in network <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>X</mi><mrow><mo stretchy="false">{</mo><mi>t</mi><mo stretchy="false">}</mo></mrow></msup></mrow><annotation encoding="application/x-tex">X^{\{t\}}</annotation></semantics></math></span> batch in mini-batch</li>
</ul>
</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mo stretchy="false">[</mo><msup><mi>X</mi><mrow><mo stretchy="false">{</mo><mn>1</mn><mo stretchy="false">}</mo></mrow></msup><mo separator="true">,</mo><msup><mi>X</mi><mrow><mo stretchy="false">{</mo><mn>2</mn><mo stretchy="false">}</mo></mrow></msup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msup><mi>X</mi><mrow><mo stretchy="false">{</mo><mn>5000</mn><mo stretchy="false">}</mo></mrow></msup><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">X = [X^{\{1\}},X^{\{2\}},...,X^{\{5000\}}]</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>=</mo><mo stretchy="false">[</mo><msup><mi>Y</mi><mrow><mo stretchy="false">{</mo><mn>1</mn><mo stretchy="false">}</mo></mrow></msup><mo separator="true">,</mo><msup><mi>Y</mi><mrow><mo stretchy="false">{</mo><mn>2</mn><mo stretchy="false">}</mo></mrow></msup><mo separator="true">,</mo><msup><mi>Y</mi><mrow><mo stretchy="false">{</mo><mn>3</mn><mo stretchy="false">}</mo></mrow></msup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msup><mi>Y</mi><mrow><mo stretchy="false">(</mo><mn>5</mn><mo separator="true">,</mo><mn>000</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">Y=[Y^{\{1\}},Y^{\{2\}},Y^{\{3\}},...,Y^{(5,000)}]</annotation></semantics></math></span></li>
</ul>
</li>
</ul>
</li>
<li>Mini-batch gradient descent<ul>
<li>1 step of gradient descent using <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>X</mi><mrow><mo stretchy="false">{</mo><mi>t</mi><mo stretchy="false">}</mo></mrow></msup><mo separator="true">,</mo><msup><mi>Y</mi><mrow><mo stretchy="false">{</mo><mi>t</mi><mo stretchy="false">}</mo></mrow></msup></mrow><annotation encoding="application/x-tex">X^{\{t\}},Y^{\{t\}}</annotation></semantics></math></span> (1000)<ul>
<li>1 epoch: single pass through training set.</li>
</ul>
</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>o</mi><mi>r</mi><mtext> </mtext><mi>t</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mn>5000</mn></mrow><annotation encoding="application/x-tex">for\space t=1,...,5000</annotation></semantics></math></span>
<ul>
<li>Forward prop on <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>X</mi><mrow><mo stretchy="false">{</mo><mi>t</mi><mo stretchy="false">}</mo></mrow></msup></mrow><annotation encoding="application/x-tex">X^{\{t\}}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>W</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><msup><mi>X</mi><mrow><mo stretchy="false">{</mo><mi>t</mi><mo stretchy="false">}</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">Z^{[1]}=W^{[1]}X^{\{t\}}+b^{[1]}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>g</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">(</mo><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">A^{[1]}=g^{[1]}(Z^{[1]})</annotation></semantics></math></span></li>
<li>…</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>g</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">(</mo><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">A^{[l]}=g^{[l]}(Z^{[l]})</annotation></semantics></math></span></li>
</ul>
</li>
<li>Compute cost <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>J</mi><mrow><mo stretchy="false">{</mo><mi>t</mi><mo stretchy="false">}</mo></mrow></msup><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mn>1000</mn></mfrac></mstyle><mo>∑</mo><mo>∗</mo><msup><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></msup><mi>L</mi><mo stretchy="false">(</mo><msup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>+</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mi>λ</mi><mrow><mn>2</mn><mo>⋅</mo><mn>1000</mn></mrow></mfrac></mstyle><mo>∑</mo><mo>∗</mo><msup><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></msup><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">_</mi><msup><mi>F</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">J^{\{t\}}=\dfrac{1}{1000}\sum\limits*{i=1}^{l}L(\hat{y}^{(i)},y^{(i)})+\dfrac{\lambda}{2\cdot1000}\sum\limits*{l=1}^{l}||w^{[l]}||\_F^2</annotation></semantics></math></span><ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\hat{y}^{(i)},y^{(i)}</annotation></semantics></math></span> --> from <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>X</mi><mrow><mo stretchy="false">{</mo><mi>t</mi><mo stretchy="false">}</mo></mrow></msup><mo separator="true">,</mo><msup><mi>Y</mi><mrow><mo stretchy="false">{</mo><mi>t</mi><mo stretchy="false">}</mo></mrow></msup></mrow><annotation encoding="application/x-tex">X^{\{t\}},Y^{\{t\}}</annotation></semantics></math></span></li>
</ul>
</li>
<li>Backprop to compute gradient cost <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>J</mi><mrow><mo stretchy="false">{</mo><mi>t</mi><mo stretchy="false">}</mo></mrow></msup><mtext> </mtext><mo stretchy="false">(</mo><mi>u</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>g</mi><mtext> </mtext><mo stretchy="false">(</mo><msup><mi>X</mi><mrow><mo stretchy="false">{</mo><mi>t</mi><mo stretchy="false">}</mo></mrow></msup><mo separator="true">,</mo><msup><mi>Y</mi><mrow><mo stretchy="false">{</mo><mi>t</mi><mo stretchy="false">}</mo></mrow></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">J^{\{t\}}\space (using\space (X^{\{t\}},Y^{\{t\}}))</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>:</mo><mo>=</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>−</mo><mi>α</mi><mi>d</mi><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>:</mo><mo>=</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>−</mo><mi>α</mi><mi>d</mi><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">w^{[l]}:=w^{[l]}-\alpha dw^{[l]}, b^{[l]}:=b^{[l]}-\alpha db^{[l]}</annotation></semantics></math></span></li>
</ul>
</li>
<li>Understanding mini-batch gradient descent<ul>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/11/22/4yOYlp2bcEtPSuw.jpg" alt="deep-learning-notes_2-2-1" data-caption="deep-learning-notes_2-2-1" loading="lazy"></li>
<li>If mini-batch size&#x3D;m:batch gradient descent (Too long per iteration).–<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>X</mi><mrow><mo stretchy="false">{</mo><mi>t</mi><mo stretchy="false">}</mo></mrow></msup><mo separator="true">,</mo><msup><mi>Y</mi><mrow><mo stretchy="false">{</mo><mi>t</mi><mo stretchy="false">}</mo></mrow></msup><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mi>X</mi><mo separator="true">,</mo><mi>Y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(X^{\{t\}},Y^{\{t\}})=(X,Y)</annotation></semantics></math></span></li>
<li>If mini-batch size&#x3D;1:Stochatic gradient descent (noisy, not converge, loos speedup from vectorization).– Every example is it own mini-batch.</li>
<li>In practice: select in-between 1 and m.<ul>
<li>Get lots of vectorization</li>
<li>Make progress without needing to wait entire training set.</li>
</ul>
</li>
</ul>
</li>
<li>Choosing mini-batch size<ul>
<li>No need for small training set (<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>&lt;</mo><mn>2000</mn></mrow><annotation encoding="application/x-tex">m&lt;2000</annotation></semantics></math></span>)</li>
<li>Typical mini-batch size: 64, 128, 256, 512. (Use power of 2)</li>
<li>Make sure minibatch fir in CPU&#x2F;GPU memory.</li>
</ul>
</li>
</ul>
<h4 id="2-2-2-Exponentially-weighted-averages"><a href="#2-2-2-Exponentially-weighted-averages" class="headerlink" title="2.2.2 Exponentially weighted averages"></a>2.2.2 Exponentially weighted averages</h4><ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo>∗</mo><mi>t</mi><mo>=</mo><mi>β</mi><mi>V</mi><mo>∗</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>β</mi><mo stretchy="false">)</mo><msub><mi>θ</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">V*t = \beta V*{t-1} + (1 - \beta) \theta_t</annotation></semantics></math></span>

<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">V_t</annotation></semantics></math></span> is the weighted average at time <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span>.</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\theta_t</annotation></semantics></math></span> is the actual observed value at time <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span>.</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span> is the decay rate (usually between 0 and 1).</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mi mathvariant="normal">_</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">V\_{t-1}</annotation></semantics></math></span> is the weighted average at the previous time step.</li>
</ul>
</li>
<li><p>Impact of Decay Rate <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span>: The value of <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span> significantly affects the smoothness of the weighted average curve:</p>
<ul>
<li>A larger <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span> makes the curve smoother, as it gives more weight to past observations, thereby reducing the impact of recent changes on the weighted average.</li>
<li>A smaller <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span> makes the curve more responsive to recent changes, as it gives more weight to recent observations.</li>
</ul>
</li>
<li><p>Interpretation of <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>ϵ</mi><msup><mo stretchy="false">)</mo><mfrac><mn>1</mn><mi>ϵ</mi></mfrac></msup></mrow><mtext>some constant</mtext></mfrac></mstyle><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>e</mi></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\dfrac{(1-\epsilon)^{\frac{1}{\epsilon}}}{\text{some constant}} = \dfrac{1}{e}</annotation></semantics></math></span></p>
<ul>
<li>Defining <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span> as <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">1 - \beta</annotation></semantics></math></span> provides insight into how the influence of past data gradually diminishes as <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span> approaches 1 (i.e., <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span> approaches 0).</li>
<li>As <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span> approaches 0, <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>ϵ</mi><msup><mo stretchy="false">)</mo><mfrac><mn>1</mn><mi>ϵ</mi></mfrac></msup></mrow><annotation encoding="application/x-tex">(1-\epsilon)^{\frac{1}{\epsilon}}</annotation></semantics></math></span> approaches <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>e</mi></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\dfrac{1}{e}</annotation></semantics></math></span>, indicating that even though past data is given more weight (high <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span>), its actual impact on the current value is decreasing.</li>
</ul>
</li>
<li><p>Implementation</p>
<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mi mathvariant="normal">_</mi><mi>θ</mi><mo>:</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">v\_{\theta}:=0</annotation></semantics></math></span></li>
<li>Repear for each day:<ul>
<li>Get the next <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\theta_t</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>∗</mo><mi>θ</mi><mo>:</mo><mo>=</mo><mi>β</mi><mi>v</mi><mo>∗</mo><mi>θ</mi><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>β</mi><mo stretchy="false">)</mo><msub><mi>θ</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">v*\theta:=\beta v*\theta+(1-\beta)\theta_t</annotation></semantics></math></span></li>
</ul>
</li>
</ul>
</li>
<li><p>Bias correction in exponentially weighted averages</p>
<ul>
<li>Bias correction is applied to counteract the initial bias in exponentially weighted averages, especially when the number of data points is small or at the start of the calculation.</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><msub><mi>v</mi><mi>t</mi></msub><mrow><mn>1</mn><mo>−</mo><msup><mi>β</mi><mi>t</mi></msup></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\dfrac{v_t}{1-\beta^t}</annotation></semantics></math></span> Here, <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">v_t </annotation></semantics></math></span> is the uncorrected exponentially weighted average at time <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span>, and <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span> is the decay rate.</li>
<li>It ensures that the moving averages are not underestimated, particularly when <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span> is high and in the early stages of the iteration. With iteration goes on, the affect of this correction will become smaller since <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>β</mi><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\beta^t</annotation></semantics></math></span> is closer to 1.</li>
</ul>
</li>
<li><p>Gradient descent with momentum</p>
<ul>
<li>On iteration t:<ul>
<li>Compute <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>w</mi><mo separator="true">,</mo><mi>d</mi><mi>b</mi></mrow><annotation encoding="application/x-tex">dw, db</annotation></semantics></math></span> on current mini-batch (whole batch if not using mini-batch)</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>∗</mo><mrow><mi>d</mi><mi>w</mi></mrow><mo>=</mo><mi>β</mi><mi>v</mi><mo>∗</mo><mrow><mi>d</mi><mi>w</mi></mrow><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>β</mi><mo stretchy="false">)</mo><mi>d</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">v*{dw}=\beta v*{dw}+(1-\beta)dw</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>∗</mo><mrow><mi>d</mi><mi>b</mi></mrow><mo>=</mo><mi>β</mi><mi>v</mi><mo>∗</mo><mrow><mi>d</mi><mi>b</mi></mrow><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>β</mi><mo stretchy="false">)</mo><mi>d</mi><mi>b</mi></mrow><annotation encoding="application/x-tex">v*{db}=\beta v*{db}+(1-\beta)db</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>:</mo><mo>=</mo><mi>w</mi><mo>−</mo><mi>α</mi><mi>v</mi><mo>∗</mo><mrow><mi>d</mi><mi>w</mi></mrow><mo separator="true">,</mo><mi>b</mi><mo>:</mo><mo>=</mo><mi>b</mi><mo>−</mo><mi>α</mi><mi>v</mi><mo>∗</mo><mrow><mi>d</mi><mi>b</mi></mrow></mrow><annotation encoding="application/x-tex">w:=w-\alpha v*{dw}, b:=b-\alpha v*{db}</annotation></semantics></math></span></li>
</ul>
</li>
<li>initiate <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>∗</mo><mrow><mi>d</mi><mi>w</mi></mrow><mtext> </mtext><mi>a</mi><mi>n</mi><mi>d</mi><mtext> </mtext><mi>v</mi><mo>∗</mo><mrow><mi>d</mi><mi>b</mi></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">v*{dw}\space and\space v*{db} = 0</annotation></semantics></math></span></li>
<li>Smooth out gradient descent<ul>
<li>The momentum term <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span> effectively provides a smoothing effect since it is an average of past gradients. This means that extreme gradient changes in a single iteration are averaged out, reducing the volatility of the update steps.</li>
<li>This smoothing effect is particularly useful on loss function surfaces that are not flat or have many local minima.</li>
</ul>
</li>
<li>Consider set <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span> as <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.9</mn></mrow><annotation encoding="application/x-tex">0.9</annotation></semantics></math></span> (common, about the average last 10 gradients), it gives more weight to <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mi mathvariant="normal">_</mi><mrow><mi>d</mi><mi>w</mi></mrow></mrow><annotation encoding="application/x-tex">v\_{dw}</annotation></semantics></math></span>, consider <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">dw</annotation></semantics></math></span> as the acceleration. With <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mi>e</mi><mi>t</mi><mi>a</mi></mrow><annotation encoding="application/x-tex">beta</annotation></semantics></math></span> decreasing, velocity increasing slower and acceleration increasing faster.</li>
</ul>
</li>
</ul>
<h4 id="2-2-3-RMSprop-and-Adam-optimization"><a href="#2-2-3-RMSprop-and-Adam-optimization" class="headerlink" title="2.2.3 RMSprop and Adam optimization"></a>2.2.3 RMSprop and Adam optimization</h4><ul>
<li>RMSprop (Root Mean Square Propagation)<ul>
<li>On iteration t:<ul>
<li>Compute <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>w</mi><mo separator="true">,</mo><mi>d</mi><mi>b</mi></mrow><annotation encoding="application/x-tex">dw, db</annotation></semantics></math></span> on current mini-batch</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>∗</mo><mrow><mi>d</mi><mi>w</mi></mrow><mo>=</mo><msub><mi>β</mi><mn>2</mn></msub><mi>s</mi><mo>∗</mo><mrow><mi>d</mi><mi>w</mi></mrow><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msub><mi>β</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mi>d</mi><msup><mi>w</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">s*{dw}=\beta_2 s*{dw}+(1-\beta_2)dw^2</annotation></semantics></math></span> Hope to be relative small.</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>∗</mo><mrow><mi>d</mi><mi>b</mi></mrow><mo>=</mo><msub><mi>β</mi><mn>2</mn></msub><mi>s</mi><mo>∗</mo><mrow><mi>d</mi><mi>b</mi></mrow><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msub><mi>β</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mi>d</mi><msup><mi>b</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">s*{db}=\beta_2 s*{db}+(1-\beta_2)db^2</annotation></semantics></math></span> Hope to be relative large.</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>:</mo><mo>=</mo><mi>w</mi><mo>−</mo><mi>α</mi><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>d</mi><mi>w</mi></mrow><mrow><msqrt><mrow><mi>s</mi><mo>∗</mo><mrow><mi>d</mi><mi>w</mi></mrow></mrow></msqrt><mo>+</mo><mi>ϵ</mi></mrow></mfrac></mstyle><mo separator="true">,</mo><mi>b</mi><mo>:</mo><mo>=</mo><mi>b</mi><mo>−</mo><mi>α</mi><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>d</mi><mi>b</mi></mrow><mrow><msqrt><mrow><mi>s</mi><mo>∗</mo><mrow><mi>d</mi><mi>b</mi></mrow></mrow></msqrt><mo>+</mo><mi>ϵ</mi></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">w:=w-\alpha\dfrac{dw}{\sqrt{s*{dw}}+\epsilon}, b:=b-\alpha\dfrac{db}{\sqrt{s*{db}}+\epsilon}</annotation></semantics></math></span> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span> is a realative small number(<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>8</mn></mrow></msup></mrow><annotation encoding="application/x-tex">10^{-8}</annotation></semantics></math></span>) ot prevent nominaotr being 0.</li>
</ul>
</li>
<li>Slow down in vertical direction, fast in horizontal direction.</li>
</ul>
</li>
<li>Adam (Adaptive moment estimation) optimization algorithm<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>∗</mo><mrow><mi>d</mi><mi>w</mi></mrow><mo>=</mo><mn>0</mn><mo separator="true">,</mo><mi>s</mi><mo>∗</mo><mrow><mi>d</mi><mi>w</mi></mrow><mo>=</mo><mn>0.</mn><mi>v</mi><mo>∗</mo><mrow><mi>d</mi><mi>b</mi></mrow><mo>=</mo><mn>0</mn><mo separator="true">,</mo><mi>s</mi><mo>∗</mo><mrow><mi>d</mi><mi>w</mi></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">v*{dw}=0, s*{dw}=0. v*{db}=0, s*{dw}=0</annotation></semantics></math></span></li>
<li>On iteration t:<ul>
<li>Compute <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>w</mi><mo separator="true">,</mo><mi>b</mi><mi>d</mi></mrow><annotation encoding="application/x-tex">dw, bd</annotation></semantics></math></span> using current mini-batch</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>∗</mo><mrow><mi>d</mi><mi>w</mi></mrow><mo>=</mo><msub><mi>β</mi><mn>1</mn></msub><mi>v</mi><mo>∗</mo><mrow><mi>d</mi><mi>w</mi></mrow><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>β</mi><mo>∗</mo><mn>1</mn><mo stretchy="false">)</mo><mi>d</mi><mi>w</mi><mo separator="true">,</mo><mi>v</mi><mo>∗</mo><mrow><mi>d</mi><mi>b</mi></mrow><mo>=</mo><mi>β</mi><mo>∗</mo><mn>1</mn><mi>v</mi><mo>∗</mo><mrow><mi>d</mi><mi>b</mi></mrow><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msub><mi>β</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mi>d</mi><mi>b</mi></mrow><annotation encoding="application/x-tex">v*{dw}=\beta_1v*{dw}+(1-\beta*1)dw,v*{db}=\beta*1v*{db}+(1-\beta_1)db</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>∗</mo><mrow><mi>d</mi><mi>w</mi></mrow><mo>=</mo><msub><mi>β</mi><mn>2</mn></msub><mi>s</mi><mo>∗</mo><mrow><mi>d</mi><mi>w</mi></mrow><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>β</mi><mo>∗</mo><mn>2</mn><mo stretchy="false">)</mo><mi>d</mi><msup><mi>w</mi><mn>2</mn></msup><mo separator="true">,</mo><mi>s</mi><mo>∗</mo><mrow><mi>d</mi><mi>b</mi></mrow><mo>=</mo><mi>β</mi><mo>∗</mo><mn>2</mn><mi>s</mi><mo>∗</mo><mrow><mi>d</mi><mi>b</mi></mrow><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msub><mi>β</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mi>d</mi><mi>b</mi></mrow><annotation encoding="application/x-tex">s*{dw}=\beta_2s*{dw}+(1-\beta*2)dw^2,s*{db}=\beta*2s*{db}+(1-\beta_2)db</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>∗</mo><msup><mrow><mi>d</mi><mi>w</mi></mrow><mrow><mi>c</mi><mi>o</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow></msup><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>v</mi><mo>∗</mo><mrow><mi>d</mi><mi>w</mi></mrow></mrow><mrow><mn>1</mn><mo>−</mo><mi>β</mi><mo>∗</mo><msup><mn>1</mn><mi>t</mi></msup></mrow></mfrac></mstyle><mo separator="true">,</mo><mi>v</mi><mo>∗</mo><msup><mrow><mi>d</mi><mi>b</mi></mrow><mrow><mi>c</mi><mi>o</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow></msup><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>v</mi><mi mathvariant="normal">_</mi><mrow><mi>d</mi><mi>b</mi></mrow></mrow><mrow><mn>1</mn><mo>−</mo><msubsup><mi>β</mi><mn>1</mn><mi>t</mi></msubsup></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">v*{dw}^{corrected}=\dfrac{v*{dw}}{1-\beta*1^t}, v*{db}^{corrected}=\dfrac{v\_{db}}{1-\beta_1^t}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>∗</mo><msup><mrow><mi>d</mi><mi>w</mi></mrow><mrow><mi>c</mi><mi>o</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow></msup><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>s</mi><mo>∗</mo><mrow><mi>d</mi><mi>w</mi></mrow></mrow><mrow><mn>1</mn><mo>−</mo><mi>β</mi><mo>∗</mo><msup><mn>2</mn><mi>t</mi></msup></mrow></mfrac></mstyle><mo separator="true">,</mo><mi>s</mi><mo>∗</mo><msup><mrow><mi>d</mi><mi>b</mi></mrow><mrow><mi>c</mi><mi>o</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow></msup><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>s</mi><mi mathvariant="normal">_</mi><mrow><mi>d</mi><mi>b</mi></mrow></mrow><mrow><mn>1</mn><mo>−</mo><msubsup><mi>β</mi><mi>s</mi><mi>t</mi></msubsup></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">s*{dw}^{corrected}=\dfrac{s*{dw}}{1-\beta*2^t}, s*{db}^{corrected}=\dfrac{s\_{db}}{1-\beta_s^t}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>:</mo><mo>=</mo><mi>w</mi><mo>−</mo><mi>α</mi><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>v</mi><mo>∗</mo><msup><mrow><mi>d</mi><mi>w</mi></mrow><mrow><mi>c</mi><mi>o</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow></msup></mrow><mrow><msqrt><mrow><mi>s</mi><mo>∗</mo><msup><mrow><mi>d</mi><mi>w</mi></mrow><mrow><mi>c</mi><mi>o</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow></msup></mrow></msqrt><mo>+</mo><mi>ϵ</mi></mrow></mfrac></mstyle><mo separator="true">,</mo><mi>b</mi><mo>:</mo><mo>=</mo><mi>b</mi><mo>−</mo><mi>α</mi><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>v</mi><mo>∗</mo><msup><mrow><mi>d</mi><mi>b</mi></mrow><mrow><mi>c</mi><mi>o</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow></msup></mrow><mrow><msqrt><mrow><mi>s</mi><mo>∗</mo><msup><mrow><mi>d</mi><mi>b</mi></mrow><mrow><mi>c</mi><mi>o</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow></msup></mrow></msqrt><mo>+</mo><mi>ϵ</mi></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">w:=w-\alpha\dfrac{v*{dw}^{corrected}}{\sqrt{s*{dw}^{corrected}}+\epsilon}, b:=b-\alpha\dfrac{v*{db}^{corrected}}{\sqrt{s*{db}^{corrected}}+\epsilon}</annotation></semantics></math></span></li>
</ul>
</li>
<li>Hyperparameters choice:<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span>: needs to be tune</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\beta_1</annotation></semantics></math></span>: 0.9 (<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">dw</annotation></semantics></math></span>) First moment</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\beta_2</annotation></semantics></math></span>: 0.999 (<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>w</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">dw^2</annotation></semantics></math></span>) Second moment</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi><mo>:</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>8</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\epsilon: 10^{-8}</annotation></semantics></math></span> Not affect performance</li>
</ul>
</li>
</ul>
</li>
<li>Learning rate decay<ul>
<li>1 epoch &#x3D; 1 pass through the data</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>d</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>y</mi><mo>−</mo><mi>r</mi><mi>a</mi><mi>t</mi><mi>e</mi><mstyle mathcolor="#cc0000"><mtext>\*</mtext></mstyle><mi>e</mi><mi>p</mi><mi>o</mi><mi>c</mi><mi>h</mi><mo>−</mo><mi>n</mi><mi>u</mi><mi>m</mi></mrow></mfrac></mstyle><msub><mi>α</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\alpha=\dfrac{1}{1+decay-rate\*epoch-num}\alpha_0</annotation></semantics></math></span></li>
<li>Other methods<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0.9</mn><msup><mn>5</mn><mrow><mi>e</mi><mi>p</mi><mi>o</mi><mi>c</mi><mi>h</mi><mo>−</mo><mi>n</mi><mi>u</mi><mi>m</mi></mrow></msup><mo>⋅</mo><msub><mi>α</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\alpha=0.95^{epoch-num}\cdot \alpha_0</annotation></semantics></math></span> ---- exponentially decay</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mi>k</mi><msqrt><mrow><mi>e</mi><mi>p</mi><mi>o</mi><mi>c</mi><mi>h</mi><mo>−</mo><mi>n</mi><mi>u</mi><mi>m</mi></mrow></msqrt></mfrac></mstyle><mo>⋅</mo><msub><mi>α</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\alpha=\dfrac{k}{\sqrt{epoch-num}}\cdot\alpha_0</annotation></semantics></math></span> or <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mi>k</mi><msqrt><mi>t</mi></msqrt></mfrac></mstyle><mo>⋅</mo><msub><mi>α</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\dfrac{k}{\sqrt{t}}\cdot\alpha_0</annotation></semantics></math></span> ---- discrete staircase</li>
<li>Manual decay (small number of model)</li>
</ul>
</li>
</ul>
</li>
<li>The problem of local optima<ul>
<li>Unlikely to stuck in a bad local optima, since there are too many dimensions and all algorithms in deep learning.</li>
<li>saddle point —- gradient &#x3D; 0</li>
<li>Problem of plateaus: Make learning slow</li>
</ul>
</li>
</ul>
<h3 id=""><a href="#" class="headerlink" title=""></a></h3><h4 id="2-3-1-Tuning-process"><a href="#2-3-1-Tuning-process" class="headerlink" title="2.3.1 Tuning process"></a>2.3.1 Tuning process</h4><ul>
<li>Hyperparameters<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span>: learning rate (1st)</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span>: momentum (2nd)</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>β</mi><mn>2</mn></msub><mo separator="true">,</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\beta_1, \beta_2, \epsilon</annotation></semantics></math></span></li>
<li># of layers (3rd)</li>
<li># of hidden units (2nd)</li>
<li>learning rate decay (3rd)</li>
<li>mini-batch size (2nd)</li>
</ul>
</li>
<li>Try random values: Don’t use a grid</li>
<li>Coarse to fine: Trying coarse random first, then fine in working well range.</li>
</ul>
<h4 id="2-3-2-Using-an-appropriate-scale-to-pick-hyperparameters"><a href="#2-3-2-Using-an-appropriate-scale-to-pick-hyperparameters" class="headerlink" title="2.3.2 Using an appropriate scale to pick hyperparameters"></a>2.3.2 Using an appropriate scale to pick hyperparameters</h4><ul>
<li><p>Learning rate: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0.0001</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha = 0.0001,...,1</annotation></semantics></math></span></p>
<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>=</mo><mo>−</mo><mn>4</mn><mstyle mathcolor="#cc0000"><mtext>\*</mtext></mstyle><mi>n</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>o</mi><mi>m</mi><mi mathvariant="normal">.</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>d</mi><mo stretchy="false">(</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">r=-4\*np.random.rand()</annotation></semantics></math></span> ---- <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>∈</mo><mo stretchy="false">[</mo><mo>−</mo><mn>4</mn><mo separator="true">,</mo><mn>0</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">r\in[-4,0]</annotation></semantics></math></span>
<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>∈</mo><mo stretchy="false">[</mo><mi>a</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">r\in[a,b]</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo>∗</mo><mn>100.0001</mn><mo>=</mo><mo>−</mo><mn>4</mn><mo separator="true">,</mo><mi>b</mi><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo>∗</mo><mn>101</mn><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">a=log*{10}0.0001 = -4, b=log*{10}1 = 0</annotation></semantics></math></span></li>
</ul>
</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>1</mn><msup><mn>0</mn><mi>r</mi></msup></mrow><annotation encoding="application/x-tex">\alpha=10^r</annotation></semantics></math></span> ----- <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mn>1</mn><msup><mn>0</mn><mn>0</mn></msup><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\alpha\in[10^{-4}...10^0]</annotation></semantics></math></span></li>
</ul>
</li>
<li><p>Exponentially Weighted Averages Decay Rate: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>=</mo><mn>0.9</mn><mo stretchy="false">(</mo><mi>l</mi><mi>a</mi><mi>s</mi><mi>t</mi><mtext> </mtext><mn>10</mn><mo stretchy="false">)</mo><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mn>0.999</mn><mo stretchy="false">(</mo><mi>l</mi><mi>a</mi><mi>s</mi><mi>t</mi><mtext> </mtext><mn>1000</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\beta=0.9(last\space 10),...,0.999(last\space1000)</annotation></semantics></math></span></p>
<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mi>β</mi><mo>=</mo><mn>0.1</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mn>0.001</mn><mtext> </mtext><mi>r</mi><mo>∈</mo><mo stretchy="false">[</mo><mo>−</mo><mn>3</mn><mo separator="true">,</mo><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">1-\beta=0.1,...,0.001\space r\in[-3,-1]</annotation></semantics></math></span>
<ul>
<li>Reason for focusing on this instead of single <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span>: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span> is too close to 1, small changes may have big affects.</li>
</ul>
</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mi>β</mi><mo>=</mo><mn>1</mn><msup><mn>0</mn><mi>r</mi></msup></mrow><annotation encoding="application/x-tex">1-\beta=10^r</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>=</mo><mn>1</mn><mo>−</mo><mn>1</mn><msup><mn>0</mn><mi>r</mi></msup></mrow><annotation encoding="application/x-tex">\beta=1-10^r</annotation></semantics></math></span></li>
</ul>
</li>
<li><p>In practice:</p>
<ul>
<li>Re-test&#x2F;Re-evaluate occasionally.</li>
<li>Babysitting one model (don’t have enough training capacity) (Panda): One model at one time.</li>
<li>Training many models in parallel (Caviar): Can try many at same time.</li>
</ul>
</li>
</ul>
<h4 id="2-3-3-Batch-Normalization"><a href="#2-3-3-Batch-Normalization" class="headerlink" title="2.3.3 Batch Normalization"></a>2.3.3 Batch Normalization</h4><ul>
<li>Implementing Batch Norm<ul>
<li>Batch Norm: make sure hidden units have standardized mean and variance.</li>
<li>Given some intermediate value in NN <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msup><mo>−</mo><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">z^{(1)},...,z^{(m)}-z^{[l](i)}</annotation></semantics></math></span> (<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span> for some hidden layers, <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span> for 1 through <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span>)<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mo>∑</mo><mi mathvariant="normal">_</mi><mi>i</mi><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\mu=\dfrac{1}{m}\sum\limits\_{i}z^{(i)}</annotation></semantics></math></span> (Mean)</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><munder><mo>∑</mo><mi>i</mi></munder><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo>−</mo><mi>μ</mi><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\sigma^2=\dfrac{1}{m}\sum\limits_i(z_i-\mu)^2</annotation></semantics></math></span> (Variance)</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mi mathvariant="normal">_</mi><msup><mrow><mi>n</mi><mi>o</mi><mi>r</mi><mi>m</mi></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>−</mo><mi>μ</mi></mrow><msqrt><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mfrac></mstyle></mrow><annotation encoding="application/x-tex">z\_{norm}^{(i)}=\dfrac{z^{(i)}-\mu}{\sqrt{\sigma^2+\epsilon}}</annotation></semantics></math></span> (Make sure mean=0, variance=1. <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span> prevent denominator=0)</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mover accent="true"><mi>z</mi><mo stretchy="true">~</mo></mover><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mi>γ</mi><mi>z</mi><mi mathvariant="normal">_</mi><msup><mrow><mi>n</mi><mi>o</mi><mi>r</mi><mi>m</mi></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\widetilde{z}^{(i)}=\gamma z\_{norm}^{(i)}+\beta</annotation></semantics></math></span> (<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mo separator="true">,</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\gamma, \beta</annotation></semantics></math></span> are learnable parameters of model)
<ul>
<li>If<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mo>=</mo><msqrt><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mrow><annotation encoding="application/x-tex">\gamma=\sqrt{\sigma^2+\epsilon}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>=</mo><mi>μ</mi></mrow><annotation encoding="application/x-tex">\beta=\mu</annotation></semantics></math></span></li>
</ul>
</li>
<li>Then <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mover accent="true"><mi>z</mi><mo stretchy="true">~</mo></mover><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mi>z</mi><mi mathvariant="normal">_</mi><msup><mrow><mi>n</mi><mi>o</mi><mi>r</mi><mi>m</mi></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\widetilde{z}^{(i)}=z\_{norm}^{(i)}</annotation></semantics></math></span></li>
</ul>
</li>
</ul>
</li>
<li>Use <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mover accent="true"><mi>z</mi><mo stretchy="true">~</mo></mover><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\widetilde{z}^{[l](i)}</annotation></semantics></math></span> instead of <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">z^{[l](i)}</annotation></semantics></math></span></li>
</ul>
</li>
<li>Adding Batch Norm to a network<ul>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/11/26/u6UVeshyTq8BXR1.jpg" alt="deep-learning-notes_2-3-3" data-caption="deep-learning-notes_2-3-3" loading="lazy"></li>
<li>Parameters: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>β</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>γ</mi><mrow><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>β</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><msup><mi>γ</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">w^{[1]},b^{[1]},\beta^{[1]},\gamma^{[1]},...,w^{[l]},b^{[l]},\beta^{[l]},\gamma^{[l]}</annotation></semantics></math></span><ul>
<li>May use gradient&#x2F;Adam&#x2F;momentum to tune <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>β</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">d\beta^{[l]}</annotation></semantics></math></span> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>β</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>β</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>−</mo><mi>α</mi><mi>d</mi><msup><mi>β</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\beta^{[l]}=\beta^{[l]}-\alpha d\beta^{[l]}</annotation></semantics></math></span></li>
</ul>
</li>
<li>Working with mini-batches: Work the same but on single batches. No need for <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">b^{[l]}</annotation></semantics></math></span>, since variance are all 1. <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mtext>  </mtext><mi>γ</mi></mrow><annotation encoding="application/x-tex">\beta\space\space\gamma</annotation></semantics></math></span> have same dimension with <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span>.</li>
</ul>
</li>
<li>Implementing gradient descent (works with momentum, RMSprop, Adam)<ul>
<li>for <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t=1</annotation></semantics></math></span>…numMiniBatches<ul>
<li>Compute forwardProp on <span class="katex-error" title="ParseError: KaTeX parse error: Expected group after &#x27;^&#x27; at position 2: X^̲" style="color:#cc0000">X^</span>.<ul>
<li>In each hidden layer use BN to replace <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">z^{[l]}</annotation></semantics></math></span> with <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mover accent="true"><mi>z</mi><mo stretchy="true">~</mo></mover><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\widetilde{z}^{[l]}</annotation></semantics></math></span></li>
</ul>
</li>
<li>Use backprop to compute <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mi>d</mi><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mi>d</mi><msup><mi>β</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo separator="true">,</mo><mi>d</mi><msup><mi>γ</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">dw^{[l]},db^{[l]},d\beta^{[l]},d\gamma^{[l]}</annotation></semantics></math></span> (no <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>b</mi></mrow><annotation encoding="application/x-tex">db</annotation></semantics></math></span>)</li>
<li>Update parameters<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>:</mo><mo>=</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>−</mo><mi>α</mi><mi>d</mi><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">w^{[l]}:=w^{[l]}-\alpha dw^{[l]}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>β</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>:</mo><mo>=</mo><msup><mi>β</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>−</mo><mi>α</mi><mi>d</mi><msup><mi>β</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\beta^{[l]}:=\beta^{[l]}-\alpha d\beta^{[l]}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>γ</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>:</mo><mo>=</mo><msup><mi>γ</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>−</mo><mi>α</mi><mi>d</mi><msup><mi>γ</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\gamma^{[l]}:=\gamma^{[l]}-\alpha d\gamma^{[l]}</annotation></semantics></math></span></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Why does Batch Norm work<ul>
<li>Covariate Shift: Different test and training data (training on black cats but try to test on other color of cats).<ul>
<li>Internal Covariate Shift: Between different layers of the network, the distribution of inputs to each layer changes. Recursively it changes the input of the latter layer. May lead to instability and reduced efficiency.</li>
<li>Batch norm reduces the problem of input values changes. Make input stable. Let the network learn more independent.</li>
</ul>
</li>
<li>Batch norm as regularization<ul>
<li>In mini-batch, each batch is scaled by the mean&#x2F;variance computed on just that mini-batch. May adds some noise to each hidden layer’s (since is not consider the whole training set) (similar to dropout).</li>
<li>This has a slight regularization effect. (Use larger mini-batch size could reduce regularization)</li>
</ul>
</li>
</ul>
</li>
<li>Batch Norm at test time<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo separator="true">,</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\mu, \sigma^2</annotation></semantics></math></span>: estimate using exponentially weighted average (across mini-batch).
<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi><mo>∗</mo><mtext>global</mtext><mo>=</mo><mi>β</mi><mi>μ</mi><mo>∗</mo><mtext>global</mtext><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>β</mi><mo stretchy="false">)</mo><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu*{\text{global}} = \beta \mu*{\text{global}} + (1 - \beta) \mu </annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo>∗</mo><mtext>global</mtext><mo>=</mo><mi>β</mi><msup><mi>σ</mi><mn>2</mn></msup><mo>∗</mo><mtext>global</mtext><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>β</mi><mo stretchy="false">)</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\sigma^2*{\text{global}} = \beta \sigma^2*{\text{global}} + (1 - \beta) \sigma^2 </annotation></semantics></math></span></li>
</ul>
</li>
<li>During testing, use the global mean and variance estimates for normalization, instead of the statistics from the current test sample or mini-batch.</li>
</ul>
</li>
</ul>
<h4 id="2-3-4-Multi-class-classification"><a href="#2-3-4-Multi-class-classification" class="headerlink" title="2.3.4 Multi-class classification"></a>2.3.4 Multi-class classification</h4><ul>
<li>Softmax regression<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span> = # classes = 4 (0,...,3)</li>
<li>Output layer: 4 nodes for each class. <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math></span> is (4,1) matrix, sum should be 1.</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>L</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>L</mi><mo stretchy="false">]</mo></mrow></msup><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>L</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mi>L</mi></msup></mrow><annotation encoding="application/x-tex">z^{[L]}=w^{[L]}a^{[L-1]}+b^{L}</annotation></semantics></math></span> (4,1) vector (L represents the output layer)</li>
<li>Activation function:<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><msup><mi>e</mi><mrow><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>L</mi><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">t=e^{(z^{[L]})}</annotation></semantics></math></span> (4,1) vector</li>
</ul>
</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>L</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><msup><mi>e</mi><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>L</mi><mo stretchy="false">]</mo></mrow></msup></msup><mrow><mo>∑</mo><mo>∗</mo><msup><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mn>4</mn></msup><msub><mi>t</mi><mi>i</mi></msub></mrow></mfrac></mstyle><mtext>  </mtext><mo stretchy="false">(</mo><mn>4</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo><mo separator="true">,</mo><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>L</mi><mo stretchy="false">]</mo></mrow></msup><mi mathvariant="normal">_</mi><mi>i</mi><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><msub><mi>t</mi><mi>i</mi></msub><mrow><mo>∑</mo><mo>∗</mo><msup><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mn>4</mn></msup><msub><mi>t</mi><mi>i</mi></msub></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">a^{[L]}=\dfrac{e^{z^{[L]}}}{\sum\limits*{j=1}^4t_i}\space\space(4,1), a^{[L]}\_i=\dfrac{t_i}{\sum\limits*{j=1}^4t_i}</annotation></semantics></math></span></li>
</ul>
</li>
<li>Hardmax: Change beigest to 1, rest all set to 0.</li>
<li>Training a softmax classifier<ul>
<li>If <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">C=2</annotation></semantics></math></span>, softmax reduces to logistic regression.</li>
<li>Loss function:<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>L</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">z^{[L]}</annotation></semantics></math></span>-><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>L</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">a^{[L]}=\hat{y}</annotation></semantics></math></span>-><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L(\hat{y},y)</annotation></semantics></math></span> (4,1)</li>
<li>Backprop: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>L</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>−</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">dz^{[L]}=\hat{y}-y</annotation></semantics></math></span> (4,1) <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>L</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>J</mi></mrow><mrow><mi mathvariant="normal">∂</mi><msup><mi>Z</mi><mrow><mo stretchy="false">[</mo><mi>L</mi><mo stretchy="false">]</mo></mrow></msup></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">dz^{[L]}=\dfrac{\partial{J}}{\partial{Z^{[L]}}}</annotation></semantics></math></span></li>
</ul>
</li>
</ul>
</li>
<li>Deep Learning frameworks<ul>
<li>TensorFlow<img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/11/28/utnJEFsobOKPfwQ.jpg" alt="deep-learning-notes_2-3-4" data-caption="deep-learning-notes_2-3-4" loading="lazy"></li>
</ul>
</li>
</ul>
<h2 id="III-Structuring-Machine-Learning-Projects"><a href="#III-Structuring-Machine-Learning-Projects" class="headerlink" title="III. Structuring Machine Learning Projects"></a>III. Structuring Machine Learning Projects</h2><h3 id="3-1-ML-Strategy-1"><a href="#3-1-ML-Strategy-1" class="headerlink" title="3.1 ML Strategy (1)"></a>3.1 ML Strategy (1)</h3><h4 id="3-1-1-Setting-up-your-goal"><a href="#3-1-1-Setting-up-your-goal" class="headerlink" title="3.1.1 Setting up your goal"></a>3.1.1 Setting up your goal</h4><ul>
<li><p>Orthogonalization</p>
<ul>
<li>Chain of assumptions in ML<ul>
<li>Fir training set well on cost function: bigger network; Adam</li>
<li>Fit dev set well on cost function: Regularization; Bigger training set</li>
<li>Fit test set well on cost function: Bigger dev set</li>
<li>Perorms well in real world: Change dev set or cost function</li>
</ul>
</li>
</ul>
</li>
<li><p>Single number evaluation metric</p>
<ul>
<li>Precision: In examples recognized, what percentage are actually true.</li>
<li>Recall: What percentage of target are correctly recognized in whole test set.</li>
<li>F1 Score: Average of precision and recall. <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>P</mi></mfrac></mstyle><mo>+</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>R</mi></mfrac></mstyle></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\dfrac{1}{\dfrac{1}{P}+\dfrac{1}{R}}</annotation></semantics></math></span> (harmonic mean)</li>
<li>Dev set + Single number evaluation matric: Speed up iteration</li>
<li>Use average error rate instead of single error rate for each classes in estimate many classes at same time.</li>
</ul>
</li>
<li><p>Satisficing and optimizing matrics</p>
<ul>
<li><p>Consider classifiers with accuracy and running time.</p>
<ul>
<li>maximize accuracy and subject to running time &lt;&#x3D; 100ms</li>
<li>Accuracy: optimizing</li>
<li>Running time: satisfiying</li>
</ul>
</li>
<li><p>N metic: 1 optimizing, n-1 satisficing</p>
</li>
</ul>
</li>
<li><p>Train&#x2F;dev&#x2F;test distributions</p>
<ul>
<li>Come from same distribution. (Use randomly shuffle)</li>
<li>Choose a dev set and test set to reflect data you expect to get in the future and consider important to do well on.</li>
</ul>
</li>
<li><p>Size of dev&#x2F;test set</p>
<ul>
<li>For large data set, use 98% training, 1% dev, 1% test</li>
<li>Size of test set: Set your test set to be big enough to give high confidence in the overall performance of your system.</li>
<li>Sometime use only train+dev, without test set.</li>
</ul>
</li>
<li><p>When to change dev&#x2F;test sets and metrics</p>
<ul>
<li><p>Filter pornographic images out of error rate:</p>
<ul>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/11/28/LqKNWzu4PyxGgbf.jpg" alt="deep-learning-notes_3-1-1" data-caption="deep-learning-notes_3-1-1" loading="lazy"></li>
</ul>
</li>
<li><p>Two Steps</p>
<ol>
<li>How to define a metric to evaluate classifiers.</li>
<li>How to do well on this metric.</li>
</ol>
</li>
<li><p>If doing well on your metric + dev&#x2F;test set does not correspond to doing well on your application, change your metric and&#x2F;or dev&#x2F;test set.</p>
</li>
</ul>
</li>
</ul>
<h4 id="3-1-2-Comparing-to-human-level-performance"><a href="#3-1-2-Comparing-to-human-level-performance" class="headerlink" title="3.1.2 Comparing to human-level performance"></a>3.1.2 Comparing to human-level performance</h4><ul>
<li>Why human-level performance:<ul>
<li>Bayes (optimal) error: best possible error. Can never surpass.</li>
<li>Humans are quite good at a lot of tasks. So long as ML is worse than humans, you can:<ul>
<li>Get labeled data from humans.</li>
<li>Gain insight from manual error analysis: Why did a person get this right?</li>
<li>Better analysis of bias&#x2F;variance.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="3-1-3-Analyzing-bias-and-variance"><a href="#3-1-3-Analyzing-bias-and-variance" class="headerlink" title="3.1.3 Analyzing bias and variance"></a>3.1.3 Analyzing bias and variance</h4><ul>
<li>Avoidable bias<ul>
<li>If training error is far from human error (bayes error), focus on bias (avoidable bias). If training error is close to human error but far from dev error, focus on variance.</li>
<li>Consider human-level error as a proxy for Bayes error (since is not too far from human-level error to Bayes error).</li>
</ul>
</li>
<li>Understanding human-level performance:<ul>
<li>Based on purpose defined which is the human-level error want to use.</li>
<li>If human can perform really well, we can use human-level error as proxy for Bayes error.</li>
</ul>
</li>
<li>Surpassing human-level performance<ul>
<li>Not natural perception</li>
<li>Lots of data</li>
</ul>
</li>
<li>Improving your model performance<ul>
<li>The two fundamental assumptions of supervised learning<ul>
<li>You can fit the training set pretty well. (Avoidable bias)</li>
<li>The training set performance generalizes pretty well to the dev&#x2F;test set. (Variance)</li>
</ul>
</li>
<li>Reducing (avoidable) bias and variance<ul>
<li>Avoidable bias:<ul>
<li>Train bigger model.</li>
<li>Train longer&#x2F;better optimization, algorithms (momentum, RMSprop, Adam).</li>
<li>NN architecture&#x2F;hyperparameters search (RNN, CNN).</li>
</ul>
</li>
<li>Variance:<ul>
<li>More data.</li>
<li>Regularization (L2, dropout, data augmentation).</li>
<li>NN architecture&#x2F;hyperparameters search.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="3-2-ML-Strategy-2"><a href="#3-2-ML-Strategy-2" class="headerlink" title="3.2 ML Strategy (2)"></a>3.2 ML Strategy (2)</h3><h4 id="3-2-1-Error-analysis"><a href="#3-2-1-Error-analysis" class="headerlink" title="3.2.1 Error analysis"></a>3.2.1 Error analysis</h4><ul>
<li>Carrying out error analysis<ul>
<li>Error analysis (count mislabel, minus from the error rate get the ceiling of error rate)<ul>
<li>Get ~100 mislabeled dev set examples.</li>
<li>Count up how many are dogs.</li>
</ul>
</li>
<li>Evaluate multiple ideas in parallel (ideas for cat detection)<ul>
<li>Fix pictures of dogs being recognized as cats</li>
<li>Fix great cats （lions, panthers, etc.） being misrecognized</li>
<li>Improve performance on blurry images</li>
<li>Check the details of mislabeled images (only few minutes&#x2F;hours)</li>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/11/29/2lQR8VdCv7Fwg1W.jpg" alt="deep-learning-notes_3-2-1" data-caption="deep-learning-notes_3-2-1" loading="lazy"></li>
</ul>
</li>
</ul>
</li>
<li>Cleaning up incorrectly labeled data<ul>
<li>DL algorithms are quite robust to random errors in the training set. (random error will not affect the algorithm too much)</li>
<li>DL algorithms are less robust to systematic errors.</li>
<li>When a high fraction of mistake is due to incorrectly label, should spend time to fix it.</li>
<li>Correcting incorrect dev&#x2F;test set examples<ul>
<li>Apply same process to your dev and test sets to make sure they continue to come from the same distribution.</li>
<li>Consider examining examples your algorithm got right as well as ones it got wrong.</li>
<li>Train and dev&#x2F;test data may now come from slightly different distributions.</li>
</ul>
</li>
</ul>
</li>
<li>Build your first system quickly, then iterate<ul>
<li>Set up dev&#x2F;test set and metric</li>
<li>Build initial system quickly</li>
<li>Use Bias&#x2F;Variance analysis &amp; Error analysis to prioritize next steps.</li>
</ul>
</li>
<li>Training and testing on different distributions<ul>
<li>200,000 from high quality webpages, 10,000 from low quality mobile app (but we care about this).<ul>
<li>Shuffle before use those data. (not a good option, will cause the influence of what we care small.)</li>
<li>Use mobile app as dev&#x2F;test set, and just really small part of training set from app. (This we will make our target to what we want.) Maybe 50% in training, 25% in dev, and 25% test.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="3-2-2-Mismatched-training-and-dev-test-set"><a href="#3-2-2-Mismatched-training-and-dev-test-set" class="headerlink" title="3.2.2 Mismatched training and dev&#x2F;test set"></a>3.2.2 Mismatched training and dev&#x2F;test set</h4><ul>
<li>Training-dev set: Same distribution as training set, but not used for training.</li>
<li>Training error - Training-dev error - Dev error<ul>
<li>Human level - traning set error: avoidable bias</li>
<li>Traning error - Training-dev error: Variance</li>
<li>Training-dev error - Dev error: Data mismatch</li>
<li>Dev error - Test error: degree of overfitting to dev set.</li>
</ul>
</li>
<li>Addressing data mismatch<ul>
<li>Carry out manual error analysis to try to understand difference between training and dev&#x2F;test sets.</li>
<li>Make training data more similar; or collect more data similar to dev&#x2F;test sets.</li>
<li>Artificial data synthesis:<ul>
<li>Possible issue (overfitting): Original data is 10000, only have the noise of 1, maybe overfit to this 1.</li>
</ul>
</li>
</ul>
</li>
<li>Transfer learning<ul>
<li>Pre-training&#x2F;Fine-tune</li>
<li>From relatively large data to relatively small data.</li>
<li>But if the target data is too small may not be suitable for transfer learning. (Depend on the outcome we want, it would be valuable to have more data)</li>
<li>When makes sense (transfer from A-&gt; B):<ul>
<li>Task A and B have the same input x.</li>
<li>You have a lot more data for Task A than Task B (want this one).</li>
<li>Low level features from A could be helpful for learning B.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="3-2-3-Learning-from-multiple-tasks"><a href="#3-2-3-Learning-from-multiple-tasks" class="headerlink" title="3.2.3 Learning from multiple tasks"></a>3.2.3 Learning from multiple tasks</h4><ul>
<li>Loss function for multiple tasks<ul>
<li>Loss: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mo stretchy="false">(</mo><mn>4</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mo>∑</mo><mi mathvariant="normal">_</mi><msup><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mn>4</mn></msup><mi>L</mi><mo stretchy="false">(</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi mathvariant="normal">_</mi><msup><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msubsup><mi>y</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\hat{y}_{(4,1)}^{(i)}=\dfrac{1}{m}\sum\limits_{i=1}^m\sum\limits\_{j=1}^4L(\hat{y}\_j^{(i)},y_j^{(i)})</annotation></semantics></math></span></li>
<li>Sum only over valid of j with 0&#x2F;1 label. (some of them may only labeled some feature)</li>
<li>Unlike softmax regression: One image can have multiple labels</li>
</ul>
</li>
<li>When multi-task learning makes sense<ul>
<li>Training on a set of tasks that could benefit from having shared lower-level features.</li>
<li>Usually: Amount of data you have for each task is quite similar.</li>
<li>Can train a big enough neural network to do well on all the tasks.</li>
</ul>
</li>
</ul>
<h4 id="3-2-4-End-to-end-deep-learning"><a href="#3-2-4-End-to-end-deep-learning" class="headerlink" title="3.2.4 End-to-end deep learning"></a>3.2.4 End-to-end deep learning</h4><ul>
<li>End-to-end needs lots of data to work well.</li>
<li>Breaking small data scenario into different deep learning will be better results.</li>
<li>Wether to use end-to-end learning<ul>
<li>Pros:<ul>
<li>Let the data speak.</li>
<li>Less hand-designing of components needed.</li>
</ul>
</li>
<li>Cons:<ul>
<li>May need large amount of data</li>
<li>Excludes potentially useful hand-designed components.</li>
</ul>
</li>
<li>Key question: Do you have sufficient data to learn a function of the complexity needed to map x to y？<ul>
<li>Use DL to learn individual components.</li>
<li>Carefully choose X-&gt;Y mappping depending on what tasks you can got data for.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="IV-Convolutional-Neural-Networks"><a href="#IV-Convolutional-Neural-Networks" class="headerlink" title="IV. Convolutional Neural Networks"></a>IV. Convolutional Neural Networks</h2><h3 id="4-1-Foundations-of-Convolutional-Neural-Networks"><a href="#4-1-Foundations-of-Convolutional-Neural-Networks" class="headerlink" title="4.1 Foundations of Convolutional Neural Networks"></a>4.1 Foundations of Convolutional Neural Networks</h3><h4 id="4-1-1-Convolutional-operatin"><a href="#4-1-1-Convolutional-operatin" class="headerlink" title="4.1.1 Convolutional operatin"></a>4.1.1 Convolutional operatin</h4><ul>
<li><p>Vertical Edge Detection</p>
<ul>
<li>Used to identify vertical edges in images, which is a crucial step in image analysis and understanding.</li>
<li>A small matrix, typically 3x3 or 5x5, is used as a convolution kernel to detect vertical edges.</li>
<li>The kernel slides over the image, moving one pixel at a time.</li>
<li>At each position, element-wise multiplication is performed between the kernel and the overlapping image area, followed by a sum to produce an output feature map.</li>
<li>High values in the output feature map indicate the presence of a vertical edge at that location.</li>
<li>$<br>\begin{bmatrix}<br>1 &amp; 0 &amp; -1 \<br>1 &amp; 0 &amp; -1 \<br>1 &amp; 0 &amp; -1<br>\end{bmatrix}<br>$</li>
<li>Based on this matrix example below, it will detect lighter on the left and darker on the right.</li>
</ul>
</li>
<li><p>Horizontal Edge Detection</p>
<ul>
<li>Brighter on the top and darker on the bottom</li>
<li>$<br>\begin{bmatrix}<br>1 &amp; 1 &amp; 1 \<br>0 &amp; 0 &amp; 0 \<br>-1 &amp; -1 &amp; -1<br>\end{bmatrix}<br>$</li>
<li>TBC</li>
</ul>
</li>
<li><p>Other Common Filters</p>
<ul>
<li><p>Sobel filter</p>
<ul>
<li>$<br>\begin{bmatrix}<br>1 &amp; 0 &amp; -1 \<br>2 &amp; 0 &amp; -2 \<br>1 &amp; 0 &amp; -1<br>\end{bmatrix}<br>$</li>
</ul>
</li>
<li><p>Scharr filter</p>
<ul>
<li>$<br>\begin{bmatrix}<br>3 &amp; 0 &amp; -3 \<br>10 &amp; 0 &amp; -10 \<br>3 &amp; 0 &amp; -3<br>\end{bmatrix}<br>$</li>
</ul>
</li>
</ul>
</li>
<li><p>Padding</p>
<ul>
<li><p>nxn * fxf &#x3D; n-f+1 x n-f+1</p>
</li>
<li><p>Problems of convolution:</p>
<ul>
<li>Shrinking output</li>
<li>Through away information from edge.</li>
</ul>
</li>
<li><p>Add a padding(p) of 0</p>
<ul>
<li>n+2pxn+2p * fxf &#x3D; n+2p-f+1 x n+2p-f+1</li>
</ul>
</li>
<li><p>Valid convolutions: No padding</p>
</li>
<li><p>Same convolutions: Pad so that output size is the same as the input size. (padding is <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>f</mi><mo>−</mo><mn>1</mn></mrow><mn>2</mn></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\dfrac{f-1}{2}</annotation></semantics></math></span>)</p>
</li>
<li><p>f is usually odd.</p>
</li>
</ul>
</li>
<li><p>Strided convolution</p>
<ul>
<li>Stepping s steps instead of 1.</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>n</mi><mo>+</mo><mn>2</mn><mi>p</mi><mo>−</mo><mi>f</mi></mrow><mi>s</mi></mfrac></mstyle><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\dfrac{n+2p-f}{s}+1</annotation></semantics></math></span> x <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>n</mi><mo>+</mo><mn>2</mn><mi>p</mi><mo>−</mo><mi>f</mi></mrow><mi>s</mi></mfrac></mstyle><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\dfrac{n+2p-f}{s}+1</annotation></semantics></math></span> (If not integer, bound down to the nearest integer.)</li>
</ul>
</li>
<li><p>cross-correlation is the real name of convolution in DL.</p>
</li>
<li><p>Convolution over volume</p>
<ul>
<li>Set the filter into the same volume as the input matrix. (e.g. RGB image with 3x3x3 filter)</li>
<li>If only look at an individual channel, just make other channel with all 0.</li>
<li>If consider vertical and horitental seperately, each output 4x4, the final could stack together get a 4x4x2 volume.</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi><mo>×</mo><msub><mi>n</mi><mi>c</mi></msub><mstyle mathcolor="#cc0000"><mtext>\*</mtext></mstyle><mi>f</mi><mo>×</mo><mi>f</mi><mo>×</mo><msub><mi>n</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">n\times n\times n_c \* f\times f \times n_c</annotation></semantics></math></span> -> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>−</mo><mi>f</mi><mo>+</mo><mn>1</mn><mo>×</mo><mi>n</mi><mo>−</mo><mi>f</mi><mo>+</mo><mn>1</mn><mo>×</mo><msubsup><mi>n</mi><mi>c</mi><msup><mrow></mrow><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msubsup></mrow><annotation encoding="application/x-tex">n-f+1 \times n-f+1 \times n_c^{&#x27;}</annotation></semantics></math></span> (\# of filters)</li>
</ul>
</li>
<li><p>One layer of a CNN</p>
<ul>
<li>Each output add a bias and apply non-learner to it. ReLU(Output+b) –&gt; Consider stack all outputs after this as volume as the a in a&#x3D;g(z)</li>
<li>Consider output as the same as the w in z&#x3D;wa+b.</li>
<li>Number of parameters in one layer: If you have 10 filters that are 3x3x 3 in one layer of a neural network, how many parameters does that layer have？(Consider 3x3x3 + bias, it will be 280 parameters)</li>
</ul>
</li>
<li><p>Summary of notation (If layer 1 is a convolution layer)</p>
<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>f</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo></mrow><annotation encoding="application/x-tex">f^{[l]}=</annotation></semantics></math></span> filter size (3x3 filter will be f=3)</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>p</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo></mrow><annotation encoding="application/x-tex">p^{[l]}=</annotation></semantics></math></span> padding</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>s</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo></mrow><annotation encoding="application/x-tex">s^{[l]}=</annotation></semantics></math></span> stride</li>
<li>Input: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>n</mi><mi>H</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msubsup><mo>×</mo><msubsup><mi>n</mi><mi>W</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msubsup><mo>×</mo><msubsup><mi>n</mi><mi>C</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">n_H^{[l-1]}\times n_W^{[l-1]}\times n_C^{[l-1]}</annotation></semantics></math></span></li>
<li>Output: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>n</mi><mi>H</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msubsup><mo>×</mo><msubsup><mi>n</mi><mi>W</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msubsup><mo>×</mo><msubsup><mi>n</mi><mi>C</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">n_H^{[l]}\times n_W^{[l]}\times n_C^{[l]}</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>∗</mo><msup><mrow><mi>H</mi><mi mathvariant="normal">/</mi><mi>W</mi></mrow><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>n</mi><mo>∗</mo><msup><mrow><mi>H</mi><mi mathvariant="normal">/</mi><mi>W</mi></mrow><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>+</mo><mn>2</mn><msup><mi>p</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>−</mo><msup><mi>f</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><msup><mi>x</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mfrac></mstyle><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n*{H/W}^{[l]}=\dfrac{n*{H/W}^{[l-1]}+2p^{[l]}-f^{[l]}}{x^{[l]}}+1</annotation></semantics></math></span> Round down to nearest integer</li>
<li>Each filter is: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>f</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>×</mo><msup><mi>f</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>×</mo><msubsup><mi>n</mi><mi>C</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">f^{[l]}\times f^{[l]}\times n_C^{[l-1]}</annotation></semantics></math></span></li>
<li>Activations: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">a^{[l]}</annotation></semantics></math></span> -&gt; <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>n</mi><mi>H</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msubsup><mo>×</mo><msubsup><mi>n</mi><mi>W</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msubsup><mo>×</mo><msubsup><mi>n</mi><mi>C</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">n_H^{[l]}\times n_W^{[l]}\times n_C^{[l]}</annotation></semantics></math></span> Batch gradient descent <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>A</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">A^{[l]}</annotation></semantics></math></span> -&gt; <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><msubsup><mi>n</mi><mi>H</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msubsup><mo>×</mo><msubsup><mi>n</mi><mi>W</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msubsup><mo>×</mo><msubsup><mi>n</mi><mi>C</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">m\times n_H^{[l]}\times n_W^{[l]}\times n_C^{[l]}</annotation></semantics></math></span></li>
<li>Weights: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>f</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>×</mo><msup><mi>f</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>×</mo><msubsup><mi>n</mi><mi>C</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msubsup><mo>×</mo><msubsup><mi>n</mi><mi>C</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">f^{[l]}\times f^{[l]}\times n_C^{[l-1]}\times n_C^{[l]}</annotation></semantics></math></span> (The last quantity is # filters in layer l)</li>
<li>bias: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>n</mi><mi>C</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">n_C^{[l]}</annotation></semantics></math></span> - <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo separator="true">,</mo><msubsup><mi>n</mi><mi>C</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(1,1,1,n_C^{[l]})</annotation></semantics></math></span></li>
</ul>
</li>
<li><p>A simple example ConvNet</p>
<ul>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/12/03/kLT6ir7dZzPOafx.jpg" alt="deep-learning-notes_4-1-1" data-caption="deep-learning-notes_4-1-1" loading="lazy"></li>
<li>Get the final output(7x7x40) and take it as a 1960 vector pass through logistic&#x2F;softmax to get out actual final value.</li>
</ul>
</li>
<li><p>Types of layer in a convolutional network</p>
<ul>
<li>Convolution (CONV)</li>
<li>Pooling (POOL)</li>
<li>Fully connected (FC)</li>
</ul>
</li>
</ul>
<h4 id="4-1-2-Pooling-layers"><a href="#4-1-2-Pooling-layers" class="headerlink" title="4.1.2 Pooling layers"></a>4.1.2 Pooling layers</h4><ul>
<li>No parameters to learn.</li>
<li>Max pooling<ul>
<li>Consider input is 4x4 matrix, output a 2x2 matrix. f(filter) &#x3D; 2, s(stride) &#x3D; 2. Just max each 2x2 in the input and put it into one cell in the output matrix.</li>
<li>Hyperparameters: f(filter) and s(stride).</li>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/12/04/gnfUEb1wcCqjPoG.jpg" alt="deep-learning-notes_4-1-2" data-caption="deep-learning-notes_4-1-2" loading="lazy"></li>
</ul>
</li>
<li>Average pooling<ul>
<li>Instead of take the maxium, take the average.</li>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/12/04/pY8ZKI7L3H2NWnT.jpg" alt="deep-learning-notes_4-1-2-2" data-caption="deep-learning-notes_4-1-2-2" loading="lazy"></li>
</ul>
</li>
<li>Input: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mi>H</mi></msub><mo>×</mo><msub><mi>n</mi><mi>W</mi></msub><mo>×</mo><msub><mi>n</mi><mi>C</mi></msub></mrow><annotation encoding="application/x-tex">n_H\times n_W\times n_C</annotation></semantics></math></span></li>
<li>Output: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><msub><mi>n</mi><mi>H</mi></msub><mo>−</mo><mi>f</mi></mrow><mi>s</mi></mfrac></mstyle><mo>+</mo><mn>1</mn><mo>×</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><msub><mi>n</mi><mi>W</mi></msub><mo>−</mo><mi>f</mi></mrow><mi>s</mi></mfrac></mstyle><mo>+</mo><mn>1</mn><mo>×</mo><msub><mi>n</mi><mi>C</mi></msub></mrow><annotation encoding="application/x-tex">\dfrac{n_H-f}{s}+1\times \dfrac{n_W-f}{s}+1\times n_C</annotation></semantics></math></span> Down to the nearest integer.</li>
</ul>
<h4 id="4-1-3-CNN-example"><a href="#4-1-3-CNN-example" class="headerlink" title="4.1.3 CNN example"></a>4.1.3 CNN example</h4><ul>
<li>Fully Connected layer<ul>
<li>After several convolutional and pooling layers, the high-level reasoning in the neural network is done via FC layers. The output of the last pooling or convolutional layer, which is typically a multi-dimensional array, is flattened into a single vector of values. This vector is then fed into one or more FC layers.</li>
<li>Role:<ul>
<li><strong>Integration of Learned Features</strong>: FC layers combine all the features learned by previous convolutional layers across the entire image. While convolutional layers are good at identifying features in local areas of the input image, FC layers help in learning global patterns in the data.</li>
<li><strong>Dimensionality Reduction</strong>: FC layers can be seen as a form of dimensionality reduction, where the high-level, spatially hierarchical features extracted by the convolutional layers are compacted into a form where predictions can be made.</li>
<li><strong>Classification or Regression</strong>: In classification tasks, the final FC layer typically has as many neurons as the number of classes, with a softmax activation function being applied to the output. For regression tasks, the final FC layer’s output size and activation function are adjusted according to the specific requirements of the task.</li>
</ul>
</li>
<li>Operation is similar to neurons in a standard neural network.</li>
</ul>
</li>
<li>Example<ul>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/12/04/KuJxvBthlrEYD89.jpg" alt="deep-learning-notes_4-1-3" data-caption="deep-learning-notes_4-1-3" loading="lazy"></li>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/12/04/pl2oRWbte7XZ1i3.jpg" alt="deep-learning-notes_4-1-3-2" data-caption="deep-learning-notes_4-1-3-2" loading="lazy"></li>
</ul>
</li>
<li>Why convolutions?<ul>
<li><strong>Parameter sharing:</strong> A feature detector (such as a vertical edge detector) that’s useful in one part of the image is probably useful in another part of the image.</li>
<li><strong>Sparsity of connections:</strong> In each layer, each output value depends only on a small number of inputs.</li>
</ul>
</li>
<li>Training set <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x^{(1)},y^{(1)})...(x^{(m)},y^{(m)})</annotation></semantics></math></span></li>
<li>Cost <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>m</mi></mfrac></mstyle><mo>∑</mo><mi mathvariant="normal">_</mi><msup><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msup><mi>L</mi><mo stretchy="false">(</mo><msup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">J=\dfrac{1}{m}\sum\limits\_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)})</annotation></semantics></math></span></li>
</ul>
<h3 id="4-2-Deep-Convolutional-Models-Case-Studies"><a href="#4-2-Deep-Convolutional-Models-Case-Studies" class="headerlink" title="4.2 Deep Convolutional Models: Case Studies"></a>4.2 Deep Convolutional Models: Case Studies</h3><h4 id="4-2-1-Case-studies-LeNet-5-AlexNet-VGG-ResNets"><a href="#4-2-1-Case-studies-LeNet-5-AlexNet-VGG-ResNets" class="headerlink" title="4.2.1 Case studies (LeNet-5, AlexNet, VGG, ResNets)"></a>4.2.1 Case studies (LeNet-5, AlexNet, VGG, ResNets)</h4><ul>
<li>Red notations in the image below are what the network original designed but not suitable for nowadays.</li>
</ul>
<h5 id="4-2-1-1-LeNet-5"><a href="#4-2-1-1-LeNet-5" class="headerlink" title="4.2.1.1 LeNet-5"></a>4.2.1.1 LeNet-5</h5><ul>
<li><strong>Pioneer in CNNs</strong>: One of the earliest Convolutional Neural Networks, primarily used for digit recognition tasks.</li>
<li><strong>Architecture:</strong><ul>
<li>Consists of 7 layers (excluding input).</li>
<li>Includes convolutional layers, average pooling layers, and fully connected layers.</li>
</ul>
</li>
<li><strong>Activation Functions</strong>: Uses sigmoid and tanh activation functions in different layers. (Not using nowadays)</li>
<li><strong>Local Receptive Fields</strong>: Utilizes 5x5 convolution filters to capture spatial features.</li>
<li><strong>Subsampling Layers</strong>: Employs average pooling for subsampling. (Using max pool nowadays)</li>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/12/04/L8X2VgAwNvjPcMG.jpg" alt="deep-learning-notes_4-1-1-1" data-caption="deep-learning-notes_4-1-1-1" loading="lazy"></li>
</ul>
<h5 id="4-2-1-2-AlexNet"><a href="#4-2-1-2-AlexNet" class="headerlink" title="4.2.1.2 AlexNet"></a>4.2.1.2 AlexNet</h5><ul>
<li>Multiple GPUs in the paper is outdated for today. LRN is not useful after lots of other researches.</li>
<li><strong>Deeper Architecture</strong>: Contains 8 learned layers, 5 convolutional layers followed by 3 fully connected layers.</li>
<li><strong>ReLU Activation</strong>: One of the first CNNs to use ReLU (Rectified Linear Unit) activation function for faster training.</li>
<li><strong>Overlapping Pooling</strong>: Uses overlapping max pooling, reducing the network’s size and overfitting.</li>
<li><strong>Data Augmentation and Dropout</strong>: Employs data augmentation and dropout techniques for better generalization.</li>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/12/04/Wz7woIvTYcZO8qb.jpg" alt="deep-learning-notes_4-1-1-2" data-caption="deep-learning-notes_4-1-1-2" loading="lazy"></li>
</ul>
<h5 id="4-2-1-3-VGG-16"><a href="#4-2-1-3-VGG-16" class="headerlink" title="4.2.1.3 VGG-16"></a>4.2.1.3 VGG-16</h5><ul>
<li><strong>Simplicity and Depth</strong>: Known for its simplicity and depth, with 16 learned layers.</li>
<li><strong>Uniform Architecture</strong>: Features a very uniform architecture, using 3x3 convolution filters with stride and pad of 1, max pooling, and fully connected layers.</li>
<li><strong>Convolutional Layers</strong>: Stacks convolutional layers (2-4 layers) before each max pooling layer.</li>
<li><strong>Large Number of Parameters</strong>: Has a high number of parameters (around 138 million), making it computationally intensive.</li>
<li><strong>Transfer Learning</strong>: Proved to be an excellent model for transfer learning due to its performance and simplicity.</li>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/12/04/2VwkSjJBQUe87ZN.jpg" alt="deep-learning-notes_4-1-1-3" data-caption="deep-learning-notes_4-1-1-3" loading="lazy"></li>
</ul>
<h5 id="4-2-1-4-ResNets"><a href="#4-2-1-4-ResNets" class="headerlink" title="4.2.1.4 ResNets"></a>4.2.1.4 ResNets</h5><ul>
<li><p>Residual block</p>
<ul>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/12/04/Z1W9ofDxBlUyegz.jpg" alt="deep-learning-notes_4-2-1-4-1" data-caption="deep-learning-notes_4-2-1-4-1" loading="lazy"></li>
<li>Main Path: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">a^{[l]}</annotation></semantics></math></span> –&gt; Linear –&gt; ReLU –&gt; <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">a^{[l+1]}</annotation></semantics></math></span> –&gt; Linear –&gt; ReLU –&gt; <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">a^{[l+2]}</annotation></semantics></math></span><ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>W</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">z^{[l+1]}=W^{[l+1]}a^{[l]}+b^{[l+1]}</annotation></semantics></math></span> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a^{[l+1]}=g(z^{[l+1]})</annotation></semantics></math></span> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>W</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">z^{[l+2]}=W^{[l+2]}a^{[l+1]}+b^{[l+2]}</annotation></semantics></math></span> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a^{[l+2]}=g(z^{[l+2]})</annotation></semantics></math></span></li>
</ul>
</li>
<li>Short Cut &#x2F; Skip Connection: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">a^{[l]}</annotation></semantics></math></span> –&gt; ReLU –&gt; <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">a^{[l+2]}</annotation></semantics></math></span><ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>+</mo><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a^{[l+2]}=g(z^{[l+2]}+a^{[l]})</annotation></semantics></math></span></li>
</ul>
</li>
</ul>
</li>
<li><p>In normal plain network, the trainning error with increasing number of layers in theory will continuesly decrease. But in reality it will decrease but increase after a sweet point. What ResNet performs is decreasing training error with numbers of layers increase and the training error not increasing again.</p>
</li>
<li><p>Why do residual networks work?</p>
<ul>
<li><p><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/12/04/9FGXNdz8lqtmnAh.jpg" alt="deep-learning-notes_4-2-1-4-2" data-caption="deep-learning-notes_4-2-1-4-2" loading="lazy"></p>
</li>
<li><p>Residual networks introduce a shortcut or skip connection that allows the network to learn identity functions effectively.</p>
</li>
<li><p>This is crucial for training very deep networks by avoiding the vanishing gradient problem.</p>
</li>
<li><p>In a residual block:</p>
<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span> -> BigNN -> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">a^{[l]}</annotation></semantics></math></span> -> Residual block -> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">a^{[l+2]}</annotation></semantics></math></span></li>
<li>Input <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span> is passed through a standard neural network (BigNN) to obtain <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">a^{[l]}</annotation></semantics></math></span>, and then it goes through the residual block to produce <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">a^{[l+2]}</annotation></semantics></math></span>.</li>
<li>The formulation of a residual block can be represented as:<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>+</mo><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>+</mo><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a^{[l+2]} = g(z^{[l+2]} + a^{[l]}) = g(w^{[l+2]} a^{[l+1]} + b^{[l+2]} + a^{[l]}) </annotation></semantics></math></span>
<ul>
<li>Here, <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span> is the activation function.</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">z^{[l+2]}</annotation></semantics></math></span> is the output of the layer just before the activation function.</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">w^{[l+2]}</annotation></semantics></math></span> and <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">b^{[l+2]}</annotation></semantics></math></span> are the weight and bias of the layer, respectively.</li>
<li>If <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">w^{[l+2]} = 0</annotation></semantics></math></span> and <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>b</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">b^{[l+2]} = 0</annotation></semantics></math></span>, then <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo><mo>=</mo><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">a^{[l+2]} = g(a^{[l]}) = a^{[l]}</annotation></semantics></math></span>, effectively allowing the network to learn the identity function.</li>
</ul>
</li>
<li>In cases where the dimensions of <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">a^{[l+2]}</annotation></semantics></math></span> and <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">a^{[l]}</annotation></semantics></math></span> differ (e.g., <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mn>128</mn></msup></mrow><annotation encoding="application/x-tex">a^{[l]} \in \mathbb{R}^{128}</annotation></semantics></math></span> and <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>+</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mn>256</mn></msup></mrow><annotation encoding="application/x-tex">a^{[l+2]} \in \mathbb{R}^{256}</annotation></semantics></math></span>), a linear transformation <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">w_s</annotation></semantics></math></span> (e.g., <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>s</mi></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mn>256</mn><mo>×</mo><mn>128</mn></mrow></msup></mrow><annotation encoding="application/x-tex">w_s \in \mathbb{R}^{256 \times 128}</annotation></semantics></math></span>) is applied to <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false">]</mo></mrow></msup></mrow><annotation encoding="application/x-tex">a^{[l]}</annotation></semantics></math></span> to match the dimensions.</li>
</ul>
</li>
<li><p>This architecture enables training deeper models without performance degradation, which was a significant challenge in deep learning before the development of ResNet.</p>
</li>
<li><p>Understand through backdrop(<strong><u>personal notes not from the class content</u></strong>)</p>
<ul>
<li>Consider input as x, the residual block calculation as F(x), identity mapping just drag the x and add it to the residual block’s calculation which makes the final value <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">y=F(x)+x</annotation></semantics></math></span></li>
<li>Backprop for this will be as follow<ul>
<li>Gradient of the Residual Blokc’s Output: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>y</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\dfrac{\partial y}{\partial w}</annotation></semantics></math></span><ul>
<li>This represents the gradient of the output <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span> with respect to the weights <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span>.</li>
</ul>
</li>
<li>By chain rule: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>y</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac></mstyle><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>y</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac></mstyle><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">∂</mi><mi>x</mi></mrow></mfrac></mstyle><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>x</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac></mstyle><mo>+</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>y</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>x</mi></mrow></mfrac></mstyle><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>x</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\dfrac{\partial y}{\partial w} = \dfrac{\partial y}{\partial F(x)}\dfrac{\partial F(x)}{\partial x}\dfrac{\partial x}{\partial w} + \dfrac{\partial y}{\partial x}\dfrac{\partial x}{\partial w}</annotation></semantics></math></span></li>
<li>Since <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">y=F(x)+x</annotation></semantics></math></span>, <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>y</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\dfrac{\partial y}{\partial F(x)}</annotation></semantics></math></span> and <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>y</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>x</mi></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\dfrac{\partial y}{\partial x}</annotation></semantics></math></span> should be 1</li>
<li>So the formula become <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>y</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac></mstyle><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">∂</mi><mi>x</mi></mrow></mfrac></mstyle><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>x</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac></mstyle><mo>+</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>x</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\dfrac{\partial y}{\partial w} = \dfrac{\partial F(x)}{\partial x}\dfrac{\partial x}{\partial w} + \dfrac{\partial x}{\partial w}</annotation></semantics></math></span></li>
</ul>
</li>
<li>Compare to without the identity mapping <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span> added. <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>y</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac></mstyle><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">∂</mi><mi>x</mi></mrow></mfrac></mstyle><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>x</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\dfrac{\partial y}{\partial w} = \dfrac{\partial F(x)}{\partial x}\dfrac{\partial x}{\partial w}</annotation></semantics></math></span>, there is a <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>x</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\dfrac{\partial x}{\partial w}</annotation></semantics></math></span> less. Add this <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span> to <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">F(x)</annotation></semantics></math></span> makes the network will not get worse results compare to before.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="4-2-2-Network-in-Network-and-1-X-1-convolutions"><a href="#4-2-2-Network-in-Network-and-1-X-1-convolutions" class="headerlink" title="4.2.2 Network in Network and 1 X 1 convolutions"></a>4.2.2 Network in Network and 1 X 1 convolutions</h4><ul>
<li>1x1 convolutions<ul>
<li><strong>Functionality of 1x1 Convolutions</strong>: A 1x1 convolution, despite its simplicity, acts as a fully connected layer applied to each pixel separately across depth. It’s effectively used for channel-wise interactions and dimensionality reduction.</li>
<li><strong>Increasing Network Depth</strong>: 1x1 convolutions can increase the depth of the network without a significant increase in computational complexity.</li>
<li><strong>Dimensionality Reduction</strong>: They are often used for reducing the number of channels (depth) before applying expensive 3x3 or 5x5 convolutions, thus reducing the computational cost.</li>
<li><strong>Feature Re-calibration</strong>: 1x1 convolutions can recalibrate the feature maps channel-wise, enhancing the representational power of the network.</li>
</ul>
</li>
<li>Using 1x1 convolutions:<ul>
<li>Reduce dimension: Consider a 28x28x192 input with CONV 1x1 with 32 filters, the output will be 28x28x32.</li>
</ul>
</li>
</ul>
<h4 id="4-2-3-Inception-network"><a href="#4-2-3-Inception-network" class="headerlink" title="4.2.3 Inception network"></a>4.2.3 Inception network</h4><ul>
<li>Motivation for inception network<ul>
<li>Input 28x28x192<ul>
<li>Use 1x1x192 with 64 filters, output 28x28x64</li>
<li>Use same dimension 3x3x192, output 28x28x128</li>
<li>Use same dimension 5x5x192, output 28x28x32</li>
<li>use same dimension and s&#x3D;1 Max-Pool, output 28x28x32.</li>
</ul>
</li>
<li>Final output 28x28x256.</li>
<li>The problem of computational cost (Consider 5x5x192)<ul>
<li>5x5x192x28x28x32 is really big, 120M.</li>
<li>Bottleneck layer (Using 1x1 convolution): shrink 28x28x192 –&gt; CONV, 1x1, 16, 1x1x192 –&gt; 28x28x16 (Bottleneck layer) –&gt; CONV 5x5, 32, 5x5x16 –&gt; 28x28x32</li>
<li>In total only 28x28x16+28x28x32x5x5x16&#x3D;12.4M</li>
</ul>
</li>
</ul>
</li>
<li>Inception moule<ul>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/12/04/V9FCB8xG2HcOaze.jpg" alt="deep-learning-notes_4-2-1" data-caption="deep-learning-notes_4-2-1" loading="lazy"></li>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2023/12/04/Prp8IkdA2cKUtW3.jpg" alt="deep-learning-notes_4-2-1-2" data-caption="deep-learning-notes_4-2-1-2" loading="lazy"></li>
<li>The softmax in the itermediate position is used for regularization which is used avoid overfitting.</li>
</ul>
</li>
</ul>
<h4 id="4-2-4-MobileNet"><a href="#4-2-4-MobileNet" class="headerlink" title="4.2.4 MobileNet"></a>4.2.4 MobileNet</h4><ul>
<li><p>Depthwise Separable Convolution</p>
<ul>
<li>Depthwise Convolution<ul>
<li>Computational cost &#x3D; #filter params x #filter positions x #of filters</li>
</ul>
</li>
<li>Ppointwise Convolution<ul>
<li>Computational cost &#x3D; #filter params x #filter positions x # of filters<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>∗</mo><mi>c</mi><mo>∗</mo><mi>n</mi><mo>∗</mo><mi>c</mi><mo>∗</mo><mi>f</mi><mi>i</mi><mi>l</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">n*c * n*c * filters</annotation></semantics></math></span></li>
</ul>
</li>
</ul>
</li>
<li>Cost of depthwise seprable convolution &#x2F; normal convolution<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><msub><mi>n</mi><mi>c</mi></msub></mfrac></mstyle><mo>+</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><msup><mi>f</mi><mn>2</mn></msup></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\dfrac{1}{n_c} + \dfrac{1}{f^2}</annotation></semantics></math></span></li>
</ul>
</li>
</ul>
</li>
<li><p>MobileNet v2 Bottleneck</p>
<ul>
<li><p>Residual Connection</p>
<ul>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2024/01/10/LR3SMIsuQZmTbiH.png" alt="MobileNet v2 Bottleneck" data-caption="MobileNet v2 Bottleneck" loading="lazy"></li>
<li>Expansion</li>
<li>Depthwise</li>
<li>Pointwise (Projection)</li>
</ul>
</li>
<li><p>Similar computational cost as v1</p>
<ul>
<li>MobileNet V2 improves upon V1 by introducing an inverted residual structure with linear bottlenecks, which enhances the efficiency of feature extraction and information flow through the network. This architectural advancement allows V2 to achieve better performance than V1, despite having similar computational costs. Essentially, V2 optimizes the way features are processed and combined, providing more effective and complex feature representation within the same computational budget as V1.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="4-2-5-EfficientNet"><a href="#4-2-5-EfficientNet" class="headerlink" title="4.2.5 EfficientNet"></a>4.2.5 EfficientNet</h4><ul>
<li>EfficientNet is a series of deep learning models known for high efficiency and accuracy in image classification tasks.</li>
<li><strong>Compound Scaling</strong>:<ul>
<li>It introduces a novel compound scaling method, scaling network depth, width, and resolution uniformly with a set of fixed coefficients.</li>
</ul>
</li>
<li><strong>High Efficiency and Accuracy</strong>:<ul>
<li>EfficientNets provide state-of-the-art accuracy for image classification while being more computationally efficient compared to other models.</li>
</ul>
</li>
</ul>
<h4 id="4-2-6-Inception-network"><a href="#4-2-6-Inception-network" class="headerlink" title="4.2.6 Inception network"></a>4.2.6 Inception network</h4><ul>
<li>Transfer Learning<ul>
<li>Small training set: Freeze all hidden layers (save to disk), only train the softmax unit.</li>
<li>Big training set: Freeze less hidden layers, train some of the hidden layers (or use new hidden units), and also own softmax unit.</li>
<li>Lots of data: Use the already trained weights and bias as initalization, re-train based on it, as well as the softmax unit.</li>
</ul>
</li>
<li>Data augmentation<ul>
<li>Common augmentation method: Mirroring, Random Cropping, (Rotation, Shearing, Local warping, …)</li>
<li>Color shifting: add&#x2F;minus from RGB. Advanced: PCA &#x2F; PCA color augmentation.</li>
<li>Implementing distortions during training: One CPU thread doing augmentation, and other threads or GPU doing the training at same time.</li>
</ul>
</li>
<li>State of CV<ul>
<li>Data needed (little data to lots of data): Object detection &lt; Image recognition &lt; Speach recognition</li>
<li>Lots of data - Simpler algotithms (Less hand-engineering)</li>
<li>Little data - more hand-engineering (“hacks”) - Transfer learning</li>
<li>Two sources of knowledge<ul>
<li>Labeled data</li>
<li>Hand engineered features&#x2F;network architecture&#x2F;other components</li>
</ul>
</li>
<li>Tips for doing well on benchmarks&#x2F;winning competitions<ul>
<li>Ensembling: Train several networks independently and average their outputs (<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math></span>) 1-2% better. (3-15 networks)</li>
<li>Multi-crop at test time: Run classifier on multiple versions of test images and average results. (10-crop: center, four corner, also on mirror image the same 5 crops)</li>
</ul>
</li>
<li>Use open source code<ul>
<li>Use architectures of networks published in the literature.</li>
<li>Use open source implementations if possible.</li>
<li>Use pretrained models and fine-tune on your dataset.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="4-3-Object-Detection"><a href="#4-3-Object-Detection" class="headerlink" title="4.3 Object Detection"></a>4.3 Object Detection</h3><h4 id="4-3-1-Object-localization"><a href="#4-3-1-Object-localization" class="headerlink" title="4.3.1 Object localization"></a>4.3.1 Object localization</h4><ul>
<li>Want to detect 4 class: 1-pedestrian, 2-car, 3-mtorcycle, 4-background.</li>
<li>Defining the target label y: Need to out put <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mi>x</mi></msub><mo separator="true">,</mo><msub><mi>b</mi><mi>y</mi></msub><mo separator="true">,</mo><msub><mi>b</mi><mi>h</mi></msub><mo separator="true">,</mo><msub><mi>b</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">b_x, b_y, b_h, b_w</annotation></semantics></math></span>, class label (1-4). (In total 9 elements in the output vector).</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mo stretchy="false">[</mo><msub><mi>p</mi><mi>c</mi></msub><mo separator="true">,</mo><msub><mi>b</mi><mi>x</mi></msub><mo separator="true">,</mo><msub><mi>b</mi><mi>y</mi></msub><mo separator="true">,</mo><msub><mi>b</mi><mi>h</mi></msub><mo separator="true">,</mo><msub><mi>b</mi><mi>w</mi></msub><mo separator="true">,</mo><msub><mi>c</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>c</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>c</mi><mn>3</mn></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">y=[p_c, b_x, b_y, b_h, b_w, c_1, c_2, c_3]</annotation></semantics></math></span>
<ul>
<li>There is an object <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><msub><mi>b</mi><mi>x</mi></msub><mo separator="true">,</mo><msub><mi>b</mi><mi>y</mi></msub><mo separator="true">,</mo><msub><mi>b</mi><mi>h</mi></msub><mo separator="true">,</mo><msub><mi>b</mi><mi>w</mi></msub><mo separator="true">,</mo><msub><mi>c</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>c</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>c</mi><mn>3</mn></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">y=[1, b_x, b_y, b_h, b_w, c_1, c_2, c_3]</annotation></semantics></math></span></li>
<li>No object <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mo stretchy="false">?</mo><mo separator="true">,</mo><mo stretchy="false">?</mo><mo separator="true">,</mo><mo stretchy="false">?</mo><mo separator="true">,</mo><mo stretchy="false">?</mo><mo separator="true">,</mo><mo stretchy="false">?</mo><mo separator="true">,</mo><mo stretchy="false">?</mo><mo separator="true">,</mo><mo stretchy="false">?</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">y=[0, ?, ?, ?, ?, ?, ?, ?]</annotation></semantics></math></span> Don’t care for all of other</li>
</ul>
</li>
<li>Lost function:<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mover accent="true"><msub><mi>y</mi><mn>1</mn></msub><mo>^</mo></mover><mo>−</mo><msub><mi>y</mi><mn>1</mn></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>+</mo><mo stretchy="false">(</mo><mover accent="true"><msub><mi>y</mi><mn>2</mn></msub><mo>^</mo></mover><mo>−</mo><msub><mi>y</mi><mn>2</mn></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>+</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo>+</mo><mo stretchy="false">(</mo><mover accent="true"><msub><mi>y</mi><mn>8</mn></msub><mo>^</mo></mover><mo>−</mo><msub><mi>y</mi><mn>8</mn></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">L(\hat{y}, y)=(\hat{y_1} - y_1)^2 + (\hat{y_2} - y_2)^2 + ... + (\hat{y_8} - y_8)^2</annotation></semantics></math></span> if <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mn>1</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">y_1=1</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mover accent="true"><msub><mi>y</mi><mn>1</mn></msub><mo>^</mo></mover><mo>−</mo><msub><mi>y</mi><mn>1</mn></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">L(\hat{y}, y)=(\hat{y_1} - y_1)^2</annotation></semantics></math></span> if <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mn>1</mn></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">y_1=0</annotation></semantics></math></span></li>
</ul>
</li>
</ul>
<h4 id="4-3-2-Landmark-detection"><a href="#4-3-2-Landmark-detection" class="headerlink" title="4.3.2 Landmark detection"></a>4.3.2 Landmark detection</h4><ul>
<li>Annotate key positions (points-xy coordinate) as landmarks.</li>
</ul>
<h4 id="4-3-3-Object-detection"><a href="#4-3-3-Object-detection" class="headerlink" title="4.3.3 Object detection"></a>4.3.3 Object detection</h4><ul>
<li><p>Object detection</p>
<ul>
<li>Starts with closely crops images.</li>
<li>A window sliding from the top left to bottom right, once and once. If not find increase the window’s size and redo the sliding.</li>
<li>Run each individual image to the convnet.</li>
</ul>
</li>
<li><p>Turning FC layer into convolutional layers</p>
<ul>
<li>Instead directly to FC, use conv filter.</li>
</ul>
</li>
<li><p>Convolution implementation of sliding windows</p>
<ul>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2024/01/12/Ju5TjpoefSyzt3w.png" alt="Convolution implementation of sliding windows" data-caption="Convolution implementation of sliding windows" loading="lazy"></li>
<li>Instead of do 4 times 14x14x3, new conv fc share the computation, directly using the 2x2x4.</li>
</ul>
</li>
<li><p>Output accurate bounding boxes</p>
<ul>
<li><p>YOLO algorithm</p>
<ul>
<li>Find the medium point of target and working into the boundary box that contains that point.</li>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2024/01/13/7LoM6JaibGInBzr.png" alt="YOLO algorithm" data-caption="YOLO algorithm" loading="lazy"></li>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2024/01/13/VeFyJ18fQ7kMjKz.png" alt="YOLO algorithm-2" data-caption="YOLO algorithm-2" loading="lazy"></li>
</ul>
</li>
<li><p>Intersection over union (IoU)</p>
<ul>
<li>Use to check accuracy.</li>
<li>Size of intersection &#x2F; size of reunion (normally “Correct” if loU <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≥</mo></mrow><annotation encoding="application/x-tex">\geq</annotation></semantics></math></span> 0.5)</li>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2024/01/13/4GsDhIeF71TVgZv.png" alt="loU" data-caption="loU" loading="lazy"></li>
</ul>
</li>
<li><p>Non-max suppression</p>
<ul>
<li>Leave the maximum accuracy one, supprese all with high IoU.</li>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2024/01/13/GC6AuH4Rr3zE8if.png" alt="Non-max suppression-1" data-caption="Non-max suppression-1" loading="lazy"></li>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2024/01/13/VhZCJRq1im3GBFO.png" alt="Non-max suppression-2" data-caption="Non-max suppression-2" loading="lazy"></li>
</ul>
</li>
<li><p>Anchor Boxes</p>
<ul>
<li>Predefine anchor boxes, associate ojects with anchor boxes.</li>
<li>If objects more than assigned anchor boxes, not works. Not same shape, not works.</li>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2024/01/13/EVAghCZjYbF7wzQ.png" alt="Anchor Boxes-1" data-caption="Anchor Boxes-1" loading="lazy"></li>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2024/01/13/Kn5E2VFlbaAsTdM.png" alt="Anchor Boxe-2" data-caption="Anchor Boxe-2" loading="lazy"></li>
</ul>
</li>
<li><p>Training set</p>
<ul>
<li>y is 3x3x2x8 (which is # of grids x # of anchors x # classes(5(<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>c</mi></msub><mi mathvariant="normal">.</mi><msub><mi>b</mi><mi>x</mi></msub><mo separator="true">,</mo><msub><mi>b</mi><mi>y</mi></msub><mo separator="true">,</mo><msub><mi>b</mi><mi>h</mi></msub><mo separator="true">,</mo><msub><mi>b</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">p_c. b_x, b_y, b_h, b_w</annotation></semantics></math></span>) + classes))</li>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2024/01/13/XcFKuDMT5xJfpWC.png" alt="YOLO" data-caption="YOLO" loading="lazy"></li>
</ul>
</li>
</ul>
</li>
<li><p>Regision Proposals</p>
<ul>
<li>R-CNN: Propose regions. Classify proposed regions one at a time. Output label + bounding box.</li>
<li>Fast R-CNN: Propose regions. Use convolution implementation of sliding windows to classify all the proposed regions.</li>
<li>Faster R-CNN: Use convolutional network to propose regions.</li>
</ul>
</li>
<li><p>Semantic Segmentation with U-Net</p>
<ul>
<li><p>Per-pixel class labels</p>
<ul>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2024/01/13/UBQ4Xdpg7O9yZFz.png" alt="Per-pixel class labels" data-caption="Per-pixel class labels" loading="lazy"></li>
</ul>
</li>
<li><p>Deep Learning for Semantic Segmentation</p>
<ul>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2024/01/13/rt2LA5zhEg6Mx1V.png" alt="Deep Learning for Semantic Segmentation" data-caption="Deep Learning for Semantic Segmentation" loading="lazy"></li>
</ul>
</li>
<li><p>Transpose Convolution</p>
<ul>
<li>Increase the image size.</li>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2024/01/13/ULDT1WobnV4F3Pu.png" alt="Transpose Convolution - 1" data-caption="Transpose Convolution - 1" loading="lazy"></li>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2024/01/13/GdOkYNRx8VycA6K.png" alt="Transpose Convolution - 2" data-caption="Transpose Convolution - 2" loading="lazy"></li>
</ul>
</li>
<li><p>U-Net Architecture</p>
<ul>
<li>Skip Connections: Left one get more details in color or anything like that. Right one is more spatial information to figure out where is the object really is.</li>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2024/01/13/m4QoDnL1pJSyzN6.png" alt="U-Net Architecture - Skip Connections" data-caption="U-Net Architecture - Skip Connections" loading="lazy"></li>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2024/01/13/TdvisN1GKEZwuna.png" alt="U-Net Architecture" data-caption="U-Net Architecture" loading="lazy"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="4-4-Special-Applications-Face-Recognition-Neural-Style-Transfer"><a href="#4-4-Special-Applications-Face-Recognition-Neural-Style-Transfer" class="headerlink" title="4.4 Special Applications: Face Recognition &amp; Neural Style Transfer"></a>4.4 Special Applications: Face Recognition &amp; Neural Style Transfer</h3><h4 id="4-4-1-Face-recognition"><a href="#4-4-1-Face-recognition" class="headerlink" title="4.4.1 Face recognition"></a>4.4.1 Face recognition</h4><ul>
<li><p>Face verification vs. face recognition</p>
<ul>
<li>verification vs recognition —- 1:1 vs 1:K</li>
<li>Verification<ul>
<li>Input image, name&#x2F;ID.</li>
<li>Output whether the input image is that of the claimed person.</li>
</ul>
</li>
<li>Recognition<ul>
<li>Has a database of K persons</li>
<li>Get an input image</li>
<li>Output ID if the image is any of the K persons (or “not recognized”)</li>
</ul>
</li>
</ul>
</li>
<li><p>One-shot learning</p>
<ul>
<li>Learning from one example to recognize the person again.</li>
<li>Learning a “similarity” function<ul>
<li>d(img1, img2) &#x3D; degree of difference between images</li>
<li>If d(img1, img2) <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≤</mo><mi>τ</mi></mrow><annotation encoding="application/x-tex">\le \tau</annotation></semantics></math></span> “same” <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="#cc0000"><mtext>\textgreater</mtext></mstyle><mtext> </mtext><mi>τ</mi></mrow><annotation encoding="application/x-tex">\textgreater \space \tau</annotation></semantics></math></span> “Different”</li>
</ul>
</li>
</ul>
</li>
<li><p>Siamese network</p>
<ul>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2024/01/16/eT1uh8jkNFc3gAy.png" alt="Siamese network" data-caption="Siamese network" loading="lazy"></li>
<li>Input two differnet images into two CNN and ge the result of them.</li>
<li>Such as input <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">x^{(1)}, x^{(2)}</annotation></semantics></math></span> seperately into two differnt CNN, and the output will be the encoding of each of them <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x^{(1)}), f(x^{(2)})</annotation></semantics></math></span></li>
<li>Then compare the distance between them <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>f</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>−</mo><mi>f</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">_</mi><msup><mn>2</mn><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">d(x^{(1)}, x^{(2)}) = ||f(x^{(1)}) - f(x^{(2)})||\_2^2</annotation></semantics></math></span></li>
<li>Parameters of NN define an encoding <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x^{(i)})</annotation></semantics></math></span></li>
<li>Learn parameters so that:<ul>
<li>If <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">x^{(i)}, x^{(j)}</annotation></semantics></math></span> are the smae person, <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>f</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>−</mo><mi>f</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><msup><mi mathvariant="normal">∣</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">||f(x^{(i)}) - f(x^{(j)})||^2</annotation></semantics></math></span> is small.</li>
<li>If <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">x^{(i)}, x^{(j)}</annotation></semantics></math></span> are the different person, <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>f</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>−</mo><mi>f</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><msup><mi mathvariant="normal">∣</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">||f(x^{(i)}) - f(x^{(j)})||^2</annotation></semantics></math></span> is large..</li>
</ul>
</li>
</ul>
</li>
<li><p>Triplet Loss</p>
<ul>
<li><p>Learning objective: (Anchor, Positive), (Anchor, Negative)</p>
<ul>
<li>Want: <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>f</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo><mo>−</mo><mi>f</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><msup><mi mathvariant="normal">∣</mi><mn>2</mn></msup><mo>+</mo><mi>α</mi><mo>≤</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>f</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo><mo>−</mo><mi>f</mi><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><msup><mi mathvariant="normal">∣</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">||f(A) - f(P)||^2 + \alpha \le ||f(A) - f(N)||^2</annotation></semantics></math></span> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span> is the margin (similar to SVM)</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>f</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo><mo>−</mo><mi>f</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><msup><mi mathvariant="normal">∣</mi><mn>2</mn></msup><mo>−</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>f</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo><mo>−</mo><mi>f</mi><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><msup><mi mathvariant="normal">∣</mi><mn>2</mn></msup><mo>+</mo><mi>α</mi><mo>≤</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + \alpha \le 0</annotation></semantics></math></span></li>
</ul>
</li>
<li><p>Loss function</p>
<ul>
<li>Given 3 images A, P, N:</li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>A</mi><mo separator="true">,</mo><mi>P</mi><mo separator="true">,</mo><mi>N</mi><mo stretchy="false">)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>f</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo><mo>−</mo><mi>f</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><msup><mi mathvariant="normal">∣</mi><mn>2</mn></msup><mo>−</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>f</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo><mo>−</mo><mi>f</mi><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><msup><mi mathvariant="normal">∣</mi><mn>2</mn></msup><mo>+</mo><mi>α</mi><mo separator="true">,</mo><mn>0</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L(A, P, N) = max(||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + \alpha, 0)</annotation></semantics></math></span></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo>=</mo><mo>∑</mo><mi mathvariant="normal">_</mi><msup><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msup><mi>L</mi><mo stretchy="false">(</mo><msup><mi>A</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>P</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>N</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">J = \sum\limits\_{i=1}^m L(A^{(i)},P^{(i)},N^{(i)})</annotation></semantics></math></span></li>
</ul>
</li>
<li><p>If have a training set of 10K pictures of 1k persons. Put those 10K into triplet A, P, N, then put into the loss function.</p>
</li>
<li><p>Choosing the triplets A, P, N</p>
<ul>
<li>During training, if A, P, N are chosen randomly, <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo stretchy="false">(</mo><mi>A</mi><mo separator="true">,</mo><mi>P</mi><mo stretchy="false">)</mo><mo>+</mo><mi>α</mi><mo>≤</mo><mi>d</mi><mo stretchy="false">(</mo><mi>A</mi><mo separator="true">,</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">d(A,P) +\alpha \le d(A, N)</annotation></semantics></math></span> is easily satisfied.</li>
<li>Choose triplets that’re “hard” to train on. (such as choose <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo stretchy="false">(</mo><mi>A</mi><mo separator="true">,</mo><mi>P</mi><mo stretchy="false">)</mo><mo>≈</mo><mi>d</mi><mo stretchy="false">(</mo><mi>A</mi><mo separator="true">,</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">d(A,P) \approx d(A,N)</annotation></semantics></math></span>)</li>
</ul>
</li>
<li><p>Training set using triplet loss to make J smaller. And make distance of d for same person small and different large.</p>
</li>
</ul>
</li>
<li><p>Face Verification and Binary Classification</p>
<ul>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2024/01/16/CPgRhtsSjOQkbLG.png" alt="Learning the similarity function" data-caption="Learning the similarity function" loading="lazy"></li>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><mo>∑</mo><mi mathvariant="normal">_</mi><msup><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mn>128</mn></msup><msub><mi>w</mi><mi>k</mi></msub><mi mathvariant="normal">∣</mi><mi>f</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mi mathvariant="normal">_</mi><mi>k</mi><mo>−</mo><mi>f</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mi mathvariant="normal">_</mi><mi>k</mi><mi mathvariant="normal">∣</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\hat{y} = \sigma (\sum\limits\_{k=1}^{128}w_k|f(x^{(i)})\_k-f(x^{(j)})\_k| + b)</annotation></semantics></math></span></li>
<li>Only store the <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x^{(j)})</annotation></semantics></math></span> as pre-compute, save storage and computational resources.</li>
<li>Face verification supervised learning.</li>
</ul>
</li>
</ul>
<h4 id="4-4-2-Neural-style-transfer"><a href="#4-4-2-Neural-style-transfer" class="headerlink" title="4.4.2 Neural style transfer"></a>4.4.2 Neural style transfer</h4><ul>
<li>What is it?<ul>
<li><img onerror="imgOnError(this);" data-fancybox="gallery" src="https://s2.loli.net/2024/01/16/mAw43Z9OPqcR8Bk.png" alt="Neural style transfer" data-caption="Neural style transfer" loading="lazy"></li>
</ul>
</li>
<li>Cost Function<ul>
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>G</mi><mo stretchy="false">)</mo><mo>=</mo><mi>α</mi><mi>J</mi><mo>∗</mo><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi></mrow><mo stretchy="false">(</mo><mi>C</mi><mo separator="true">,</mo><mi>G</mi><mo stretchy="false">)</mo><mo>+</mo><mi>β</mi><mi>J</mi><mo>∗</mo><mrow><mi>S</mi><mi>t</mi><mi>y</mi><mi>l</mi><mi>e</mi></mrow><mo stretchy="false">(</mo><mi>S</mi><mo separator="true">,</mo><mi>G</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">J(G) = \alpha J*{content}(C, G) + \beta J*{Style}(S, G)</annotation></semantics></math></span></li>
<li>Find the generated image G</li>
<li><ol>
<li>Initiate G randomly G: 100x100x3</li>
<li>Use gradient descent to minimize J(G) <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mo>:</mo><mo>=</mo><mi>G</mi><mo>−</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mi mathvariant="normal">∂</mi><mrow><mi mathvariant="normal">∂</mi><mi>G</mi></mrow></mfrac></mstyle><mi>J</mi><mo stretchy="false">(</mo><mi>G</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">G:=G-\dfrac{\partial}{\partial G}J(G)</annotation></semantics></math></span></li>
</ol>
</li>
</ul>
</li>
</ul>
<h2 id="V-Sequence-Models"><a href="#V-Sequence-Models" class="headerlink" title="V. Sequence Models"></a>V. Sequence Models</h2><h3 id="5-1-Recurrent-Neural-Networks"><a href="#5-1-Recurrent-Neural-Networks" class="headerlink" title="5.1 Recurrent Neural Networks"></a>5.1 Recurrent Neural Networks</h3><h4 id="5-1-1-RNN-model"><a href="#5-1-1-RNN-model" class="headerlink" title="5.1.1 RNN model"></a>5.1.1 RNN model</h4><h4 id="5-1-2-Backpropagation-through-time"><a href="#5-1-2-Backpropagation-through-time" class="headerlink" title="5.1.2 Backpropagation through time"></a>5.1.2 Backpropagation through time</h4><h4 id="5-1-3-Different-types-of-RNNs"><a href="#5-1-3-Different-types-of-RNNs" class="headerlink" title="5.1.3 Different types of RNNs"></a>5.1.3 Different types of RNNs</h4><h3 id="5-2-Natural-Language-Processing-Word-Embeddings"><a href="#5-2-Natural-Language-Processing-Word-Embeddings" class="headerlink" title="5.2 Natural Language Processing &amp; Word Embeddings"></a>5.2 Natural Language Processing &amp; Word Embeddings</h3><h4 id="5-2-1-Word-Representation"><a href="#5-2-1-Word-Representation" class="headerlink" title="5.2.1 Word Representation"></a>5.2.1 Word Representation</h4><h4 id="5-2-2-Embedding-matrix"><a href="#5-2-2-Embedding-matrix" class="headerlink" title="5.2.2 Embedding matrix"></a>5.2.2 Embedding matrix</h4><h4 id="5-2-3-Word-embeddings-in-TensorFlow"><a href="#5-2-3-Word-embeddings-in-TensorFlow" class="headerlink" title="5.2.3 Word embeddings in TensorFlow"></a>5.2.3 Word embeddings in TensorFlow</h4><h3 id="5-3-Sequence-Models-Attention-Mechanism"><a href="#5-3-Sequence-Models-Attention-Mechanism" class="headerlink" title="5.3 Sequence Models &amp; Attention Mechanism"></a>5.3 Sequence Models &amp; Attention Mechanism</h3><h4 id="5-3-1-Sequence-to-sequence-model"><a href="#5-3-1-Sequence-to-sequence-model" class="headerlink" title="5.3.1 Sequence to sequence model"></a>5.3.1 Sequence to sequence model</h4><h4 id="5-3-2-Beam-search"><a href="#5-3-2-Beam-search" class="headerlink" title="5.3.2 Beam search"></a>5.3.2 Beam search</h4><h4 id="5-3-3-Attention-model"><a href="#5-3-3-Attention-model" class="headerlink" title="5.3.3 Attention model"></a>5.3.3 Attention model</h4>
    
  </article>

  
      
    <div class="nexmoe-post-copyright">
        <strong>本文作者：</strong>ZL Asica<br>
        <strong>本文链接：</strong><a href="https://www.zl-asica.com/2023/deep-learning-notes/" title="https:&#x2F;&#x2F;www.zl-asica.com&#x2F;2023&#x2F;deep-learning-notes&#x2F;" target="_blank" rel="noopener">https:&#x2F;&#x2F;www.zl-asica.com&#x2F;2023&#x2F;deep-learning-notes&#x2F;</a><br>
        
            <strong>版权声明：</strong>本文采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh-hans" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0 DEED</a> 协议进行许可

        
    </div>


  
  
  <div class="nexmoe-post-meta nexmoe-rainbow">
   
    
        <a class="nexmoefont icon-tag-fill -none-link" href="/tags/DL/" rel="tag">DL</a>
    
</div>
  
  
    <script async src="/js/copy-codeblock.js?v=1729053201399"></script>
  

  
      <div class="nexmoe-post-footer">
          <div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://zl-asica.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener external nofollow noreferrer" href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

      </div>
  
</div></div><div class="nexmoe-post-right">    <div class="nexmoe-fixed">
        <div class="nexmoe-tool">

            

            
            
            <button class="mdui-fab catalog" style="overflow:unset;">
                <i class="nexmoefont icon-i-catalog"></i>
                <div class="nexmoe-toc">
                    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Neural-Networks-and-Deep-Learning"><span class="toc-number">1.</span> <span class="toc-text">1. Neural Networks and Deep Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-Introduction-to-Deep-Learning"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 Introduction to Deep Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-1-Supervised-Learning-with-Deep-Learning"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1.1 Supervised Learning with Deep Learning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-2-Scale-drives-deep-learning-progress"><span class="toc-number">1.1.2.</span> <span class="toc-text">1.1.2 Scale drives deep learning progress</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-Basics-of-Neural-Network-Programming"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 Basics of Neural Network Programming</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-1-Binary-Classification"><span class="toc-number">1.2.1.</span> <span class="toc-text">1.2.1 Binary Classification</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-2-Logistic-Regression"><span class="toc-number">1.2.2.</span> <span class="toc-text">1.2.2 Logistic Regression</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-3-Gradient-Descent"><span class="toc-number">1.2.3.</span> <span class="toc-text">1.2.3 Gradient Descent</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-4-Computational-Graph"><span class="toc-number">1.2.4.</span> <span class="toc-text">1.2.4 Computational Graph</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-5-Vectorization"><span class="toc-number">1.2.5.</span> <span class="toc-text">1.2.5 Vectorization</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-Shallow-Neural-Networks"><span class="toc-number">1.3.</span> <span class="toc-text">1.3 Shallow Neural Networks</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-1-Neural-Network-Representation"><span class="toc-number">1.3.1.</span> <span class="toc-text">1.3.1 Neural Network Representation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-2-Activation-Functions"><span class="toc-number">1.3.2.</span> <span class="toc-text">1.3.2 Activation Functions</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-3-Forward-and-Backward-Propogation"><span class="toc-number">1.3.3.</span> <span class="toc-text">1.3.3 Forward and Backward Propogation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-Deep-Neural-Networks"><span class="toc-number">1.4.</span> <span class="toc-text">1.4 Deep Neural Networks</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-1-Deep-L-Layer-Neural-Network"><span class="toc-number">1.4.1.</span> <span class="toc-text">1.4.1 Deep L-Layer Neural Network</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-2-Forward-Propagation-in-a-Deep-Network"><span class="toc-number">1.4.2.</span> <span class="toc-text">1.4.2 Forward Propagation in a Deep Network</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-3-Building-Blocks-of-Deep-Neural-Networks"><span class="toc-number">1.4.3.</span> <span class="toc-text">1.4.3 Building Blocks of Deep Neural Networks</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-4-Parameters-vs-Hyperparameters"><span class="toc-number">1.4.4.</span> <span class="toc-text">1.4.4 Parameters vs. Hyperparameters</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#II-Improving-Deep-Neural-Networks-Hyperparameter-Tuning-Regularization-and-Optimization"><span class="toc-number">2.</span> <span class="toc-text">II. Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Practical-Aspects-of-Deep-Learning"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 Practical Aspects of Deep Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-1-Train-Dev-Test-sets"><span class="toc-number">2.1.1.</span> <span class="toc-text">2.1.1 Train &#x2F; Dev &#x2F; Test sets</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-2-Bias-Variance"><span class="toc-number">2.1.2.</span> <span class="toc-text">2.1.2 Bias &#x2F; Variance</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-3-Basic-Recipe-for-Machine-Learning"><span class="toc-number">2.1.3.</span> <span class="toc-text">2.1.3 Basic Recipe for Machine Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#2-1-3-1-Basic-Recipe"><span class="toc-number">2.1.3.1.</span> <span class="toc-text">2.1.3.1 Basic Recipe</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-1-3-2-Regularization"><span class="toc-number">2.1.3.2.</span> <span class="toc-text">2.1.3.2 Regularization</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-1-3-3-Setting-up-your-optimization-problem"><span class="toc-number">2.1.3.3.</span> <span class="toc-text">2.1.3.3 Setting up your optimization problem</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Optimization-Algorithms"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 Optimization Algorithms</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-1-Mini-batch-gradient-descent"><span class="toc-number">2.2.1.</span> <span class="toc-text">2.2.1 Mini-batch gradient descent</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-2-Exponentially-weighted-averages"><span class="toc-number">2.2.2.</span> <span class="toc-text">2.2.2 Exponentially weighted averages</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-3-RMSprop-and-Adam-optimization"><span class="toc-number">2.2.3.</span> <span class="toc-text">2.2.3 RMSprop and Adam optimization</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">2.3.</span> <span class="toc-text"></span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-1-Tuning-process"><span class="toc-number">2.3.1.</span> <span class="toc-text">2.3.1 Tuning process</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-2-Using-an-appropriate-scale-to-pick-hyperparameters"><span class="toc-number">2.3.2.</span> <span class="toc-text">2.3.2 Using an appropriate scale to pick hyperparameters</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-3-Batch-Normalization"><span class="toc-number">2.3.3.</span> <span class="toc-text">2.3.3 Batch Normalization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-4-Multi-class-classification"><span class="toc-number">2.3.4.</span> <span class="toc-text">2.3.4 Multi-class classification</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#III-Structuring-Machine-Learning-Projects"><span class="toc-number">3.</span> <span class="toc-text">III. Structuring Machine Learning Projects</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-ML-Strategy-1"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 ML Strategy (1)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-1-Setting-up-your-goal"><span class="toc-number">3.1.1.</span> <span class="toc-text">3.1.1 Setting up your goal</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-2-Comparing-to-human-level-performance"><span class="toc-number">3.1.2.</span> <span class="toc-text">3.1.2 Comparing to human-level performance</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-3-Analyzing-bias-and-variance"><span class="toc-number">3.1.3.</span> <span class="toc-text">3.1.3 Analyzing bias and variance</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-ML-Strategy-2"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 ML Strategy (2)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-Error-analysis"><span class="toc-number">3.2.1.</span> <span class="toc-text">3.2.1 Error analysis</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2-Mismatched-training-and-dev-test-set"><span class="toc-number">3.2.2.</span> <span class="toc-text">3.2.2 Mismatched training and dev&#x2F;test set</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-3-Learning-from-multiple-tasks"><span class="toc-number">3.2.3.</span> <span class="toc-text">3.2.3 Learning from multiple tasks</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-4-End-to-end-deep-learning"><span class="toc-number">3.2.4.</span> <span class="toc-text">3.2.4 End-to-end deep learning</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#IV-Convolutional-Neural-Networks"><span class="toc-number">4.</span> <span class="toc-text">IV. Convolutional Neural Networks</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Foundations-of-Convolutional-Neural-Networks"><span class="toc-number">4.1.</span> <span class="toc-text">4.1 Foundations of Convolutional Neural Networks</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-1-Convolutional-operatin"><span class="toc-number">4.1.1.</span> <span class="toc-text">4.1.1 Convolutional operatin</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-2-Pooling-layers"><span class="toc-number">4.1.2.</span> <span class="toc-text">4.1.2 Pooling layers</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-3-CNN-example"><span class="toc-number">4.1.3.</span> <span class="toc-text">4.1.3 CNN example</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Deep-Convolutional-Models-Case-Studies"><span class="toc-number">4.2.</span> <span class="toc-text">4.2 Deep Convolutional Models: Case Studies</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-1-Case-studies-LeNet-5-AlexNet-VGG-ResNets"><span class="toc-number">4.2.1.</span> <span class="toc-text">4.2.1 Case studies (LeNet-5, AlexNet, VGG, ResNets)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#4-2-1-1-LeNet-5"><span class="toc-number">4.2.1.1.</span> <span class="toc-text">4.2.1.1 LeNet-5</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-2-1-2-AlexNet"><span class="toc-number">4.2.1.2.</span> <span class="toc-text">4.2.1.2 AlexNet</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-2-1-3-VGG-16"><span class="toc-number">4.2.1.3.</span> <span class="toc-text">4.2.1.3 VGG-16</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-2-1-4-ResNets"><span class="toc-number">4.2.1.4.</span> <span class="toc-text">4.2.1.4 ResNets</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-2-Network-in-Network-and-1-X-1-convolutions"><span class="toc-number">4.2.2.</span> <span class="toc-text">4.2.2 Network in Network and 1 X 1 convolutions</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-3-Inception-network"><span class="toc-number">4.2.3.</span> <span class="toc-text">4.2.3 Inception network</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-4-MobileNet"><span class="toc-number">4.2.4.</span> <span class="toc-text">4.2.4 MobileNet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-5-EfficientNet"><span class="toc-number">4.2.5.</span> <span class="toc-text">4.2.5 EfficientNet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-6-Inception-network"><span class="toc-number">4.2.6.</span> <span class="toc-text">4.2.6 Inception network</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-Object-Detection"><span class="toc-number">4.3.</span> <span class="toc-text">4.3 Object Detection</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-1-Object-localization"><span class="toc-number">4.3.1.</span> <span class="toc-text">4.3.1 Object localization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-2-Landmark-detection"><span class="toc-number">4.3.2.</span> <span class="toc-text">4.3.2 Landmark detection</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-3-Object-detection"><span class="toc-number">4.3.3.</span> <span class="toc-text">4.3.3 Object detection</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-Special-Applications-Face-Recognition-Neural-Style-Transfer"><span class="toc-number">4.4.</span> <span class="toc-text">4.4 Special Applications: Face Recognition &amp; Neural Style Transfer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-1-Face-recognition"><span class="toc-number">4.4.1.</span> <span class="toc-text">4.4.1 Face recognition</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-2-Neural-style-transfer"><span class="toc-number">4.4.2.</span> <span class="toc-text">4.4.2 Neural style transfer</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#V-Sequence-Models"><span class="toc-number">5.</span> <span class="toc-text">V. Sequence Models</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-Recurrent-Neural-Networks"><span class="toc-number">5.1.</span> <span class="toc-text">5.1 Recurrent Neural Networks</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-1-RNN-model"><span class="toc-number">5.1.1.</span> <span class="toc-text">5.1.1 RNN model</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-2-Backpropagation-through-time"><span class="toc-number">5.1.2.</span> <span class="toc-text">5.1.2 Backpropagation through time</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-3-Different-types-of-RNNs"><span class="toc-number">5.1.3.</span> <span class="toc-text">5.1.3 Different types of RNNs</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-Natural-Language-Processing-Word-Embeddings"><span class="toc-number">5.2.</span> <span class="toc-text">5.2 Natural Language Processing &amp; Word Embeddings</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-1-Word-Representation"><span class="toc-number">5.2.1.</span> <span class="toc-text">5.2.1 Word Representation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-2-Embedding-matrix"><span class="toc-number">5.2.2.</span> <span class="toc-text">5.2.2 Embedding matrix</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-3-Word-embeddings-in-TensorFlow"><span class="toc-number">5.2.3.</span> <span class="toc-text">5.2.3 Word embeddings in TensorFlow</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-Sequence-Models-Attention-Mechanism"><span class="toc-number">5.3.</span> <span class="toc-text">5.3 Sequence Models &amp; Attention Mechanism</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-3-1-Sequence-to-sequence-model"><span class="toc-number">5.3.1.</span> <span class="toc-text">5.3.1 Sequence to sequence model</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-3-2-Beam-search"><span class="toc-number">5.3.2.</span> <span class="toc-text">5.3.2 Beam search</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-3-3-Attention-model"><span class="toc-number">5.3.3.</span> <span class="toc-text">5.3.3 Attention model</span></a></li></ol></li></ol></li></ol>
                </div>
            </button>
            

            

            <a href="#nexmoe-content" class="backtop toc-link" aria-label="Back To Top" title="top"><button class="mdui-fab mdui-ripple"><i class="nexmoefont icon-caret-top"></i></button></a>
        </div>
    </div>
</div></div><div id="nexmoe-footer"><!--!--></div><div id="nexmoe-search-space"><div class="search-container"><div class="search-header"><div class="search-input-container"><input class="search-input" type="text" placeholder="搜索" onInput="sinput();"></div><a class="search-close" onclick="sclose();">×</a></div><div class="search-body"></div></div></div><div></div></body></html>